(this["webpackJsonpzine-blog"]=this["webpackJsonpzine-blog"]||[]).push([[0],{17:function(e,t,a){e.exports=a(29)},22:function(e,t,a){},23:function(e,t,a){},29:function(e,t,a){"use strict";a.r(t);var n=a(0),i=a.n(n),o=a(14),r=a.n(o),s=(a(22),a(23),a(4)),h=a(6),l=function(){return i.a.createElement("div",{className:"container"},i.a.createElement("div",{className:"row",style:{margin:"5vh"}},i.a.createElement("div",{className:"col"},i.a.createElement("h1",null,"Everybody loves ... convolutions (Work in progress)"),i.a.createElement("div",{style:{backgroundColor:"#f2f2f2",padding:"10px",margin:"10px",borderRadius:"5px"}},"In 3D, the following mental image may prove useful. Imagine two sheets of colored paper: one red and one blue. Put one on top of the other. Now crumple them together into a small ball. That crumpled paper ball is your input data, and each sheet of paper is a class of data in a classification problem. What a neural network (or any other machine-learning model) is meant to do is figure out a transformation of the paper ball that would uncrumple it, so as to make the two classes cleanly separable again. With deep learning, this would be implemented as a series of simple transformations of the 3D space, such as those you could apply on the paper ball with your fingers, one movement at a time.\xa0 Uncrumpling paper balls is what machine learning is about: finding neat representations for complex, highly folded data manifolds. At this point, you should have a pretty good intuition as to why deep learning excels at this: it takes the approach of incrementally decomposing a complicated geometric transformation into a long chain of elementary ones, which is pretty much the strategy a human would follow to uncrumple a paper ball. Each layer in a deep network applies a transformation that disentangles the data a little\u200a-\u200aand a deep stack of layers makes tractable an extremely complicated disentanglement process. Conv layers look at spatially local patterns by applying the same geometric transformation to different spatial locations (patches) in an input tensor. This results in a representation that are translation invariant (does not matter where in the image the object occurs). Convnets consist of stacks of convolution and max-pooling layers. The pooling layers let you spatially downsample the data, which is required to keep feature maps to a reasonable size as the number of features grows, and to allow subsequent convolution layers to see a greater spatial extent of the inputs. Convnets are often ended with a `Flatten` operation or global pooling layer, turning spatial feature maps into vectors, followed by `Dense` layers to achieve classification or regression.",i.a.createElement("br",null),i.a.createElement("br",null),i.a.createElement("b",null,"Deep Learning with Python page 44\u20135.")),i.a.createElement("img",{src:"https://i.stack.imgur.com/iY5n5.png",style:{width:"50%",height:"30%"}}),i.a.createElement("h2",null,"What is a convolution?"),i.a.createElement("p",null,"It is a sliding window applied to a matrix, whether that be an image or feature map. At each element we multiply the filter/kernel with the corresponding input pixel and then add them up. The same filter/kernel is slide across the input which is called weight tying. Filters of the convolutions are learned, so the model learns which filter is best to apply in each layer. An early layer of a convolution is likely, at best, to only detect very low level features such as edges. This is because the filters are limited at earlier layers, whereas as we go deeper, the amount of output channels (which we set) increases and later layers have several channels with which to predict collections of edges and orientations that make up a face or eye ball."),i.a.createElement("h2",null,"Properties"),i.a.createElement("p",null,"Location Invariance and Compositionality. Say you want to spot a cat in an image. Because of this sliding window approach you don't care where the cat occurs. The second aspect is (local) compositionality. Each filter composes a local patch of lower-level features into higher-level representation. You build edges from pixels, shapes from edges, eyes from circles and so on. You can derive more complex shapes and objects from lower level representations such as edges, pixels, gradients etc and from several channels of these features. Convolutions are translationally invariant because the filters slide over the image horizontally and vertically. But they are not rotationally invariant because the filters don't rotate. Thus, the net seems to need several similar filters in different orientations to detect objects and patterns that are differently oriented."),i.a.createElement("img",{src:"https://cdn-images-1.medium.com/max/1200/1*B41mvbzpZ7ythn5AlJWh-A.gif",style:{width:"90%"}}))))},c=function(){return i.a.createElement("div",null,i.a.createElement("h1",null,"Limitations (Work in progress)"),i.a.createElement("p",null,i.a.createElement("h2",null,i.a.createElement("a",{href:"http://faculty.neu.edu.cn/yury/AAI/Textbook/Deep%20Learning%20with%20Python.pdf"},"Heavily Inspired by this brilliant book")),'Deep learning works by doing a series of geometric transformations on an input. And then, given some error signal, improves the geometric transformations iteratively to map some input x to a correct output y. "In deep learning, everything is a vector: everything is a point in a geometric space. Model inputs and targets are first vectorized: turned into an initial input vector space and target vector space. Each layer operates one simple geometric transformation on the data that goes through it. The chain of layers forms one complex geometric trasnformation, broken down into a series of simpler ones. This complex transformation attempts to map the input space to the target space, one point at a time. This transformation is parameterized by weights of the layers, which are iteratively updated based on how well the model is currently performing. A key characteristic is that the geometric trasnformation must be differentiable, to be able to perform gradient descent. Thi means the geometric morphing from inputs to outputs must be smooth and continuous - a significant constraint." "The entire process of applying this complex geometric transformation to the input data can be visualized in 3D by imagining a person trying to uncrumple a paper ball: the crumpled paper ball is the manifold of the input data that the model starts with. Each movement operated by the person on the paper ball is similar to a simple geometric transformation operated by one layer. The full uncrumpling gesture sequence is the complex transformation of the entire model. Deep-learning models are mathematical machines for uncrumpling complicated manifolds of high-dimensional data. That\u2019s the magic of deep learning: turning meaning into vectors, into geometric spaces, and then incrementally learning complex geometric transformations that map one space to another. All you need are spaces of sufficiently high dimensionality in order to capture the full scope of the relationships found in the original data." "The whole thing hinges on a single core idea: that meaning is derived from the pairwise relationship between things (between words in a language, between pixels in an image) and that these relationships can be captured by a distance function. If the brain works in geometric spaces is a different question. Vector spaces are efficient to work with from a computational standpoint, but different data structures can be envisioned- like graphs. Neural networks initially emerged from using graphs (connectionism) but nowadays they have an incorrect meaning since they are neither neural nor networks. A more appropriate name is layered representations learning or hierarchical representations learning, or deep differentiable models or chained geometric transforms." "The space of applications is nearly infinite but many are completely out of reach for current deep learning techniques. You cannot train a model to read a product description and generate the appropriate codebase. Anything that requires reasoning, long-term planning, and algorithmic data manipulation is out of reach for deep-learning models no matter how much data is given. This is because a deep-learning model is just a chain of simple, continuous geometric transformation mapping one vector space into another. All it can do is map one data manifold X into another manifold Y, assuming the existence of a learnable continuous transform from X to Y." "One real risk is overestimating/ anthropomorphizing deep-learning models and their abilities. A fundamental feature of humans is our theory of mind: our tendency to project intentions, beliefs, and knowledge on the things around us. Drawing a smiley face on a rock suddenly makes it happy-in our minds. Applied to deep learning, this means that, for instance, when we\'re able to train a model to generate captions to describe pictures and the captions it generates. Then we\'re surprised when any slight departure from the sort of images present in the training data causes the model to generate completely absurd captions. This is highlighted by adversarial examples, which are samples ged to a deep-learning network that are designed to trick the model into misclassifying them. It\'s possible to do gradient ascent in input space to generate inputs that maximize the activation of some convnet filter-this is the basis of the filter-visualization technique. Similarly, through gradient ascent you can slightly modify an image to maximize the class prediction for a given class. By taking a picture of a panda and adding to it a gibbon gradient, we can get a neural network to classify the panda as a gibbon. This evidences both the brittleness of these models and the deep difference between their input-to-output mapping and our human perception."',i.a.createElement("img",{src:"https://blog.keras.io/img/limitations-of-dl/adversarial_example.png"}),'"In short, deepl-learning models don\'t have any understanding of their input-not in the human sense. Our understanding of images, sounds etc is grounded in our sensimotor experience as humans. ML models have no access to such experiences and thus can\'t understand their inputs in a human relatable way. By annotating large numbers of training examples to feed to our models, we get them to learn a geometric transform that maps data to human concepts on a specific set of examples, but this mapping is a simplistic sketch of the original model in our minds-the one developed from our experience as embodied agents. Never fall into the trap of believing that neural networks understand the task they perform. They were trained on a different, far narrower task than the one we wanted to teach them: that of mapping training inputs to training targets, point by point. Show them anything that deviates from their training data, and they will break." "Humans are capable of far more than mapping immediate stimuli to immediate responses, as a deep network, or maybe an insect, would. We maintain complex, abstract models of our current situation, of ourselves, and of other people, and can use these models to anticipate different possible futures and perform long-term planning. We can merge together known concepts to represent something we\u2019ve never experienced before-like picturing a horse wearing jeans, for instance. This ability to hypothecize, to expand our mental model beyond what we can experience directly-to perform abstraction and reasoning-is arguably the defining characterstic of human cognition. Extreme generalization: an ability to adapt to novel, never-before experienced situations using little data or no new data. This stands in sharp contrast with deep nets, local generalization. The mapping from inputs to outputs performed by deep net quickly stops making sense if new inputs deviate from what was seen during training. Consider, for instance, the problem of learning the appropriate launch parameters to get a rocket to land on the moon. If you used a deep net for this task and trained it using supervised learning or reinforcement learning, you\u2019d have to feed it thousands or even millions of launch trials: you\u2019d need to expose it to a dense sampling of the input space, in order for it to learn a reliable mapping from input space to output space. In contrast, as humans we can use our power of abstraction to come up with physical models\u2014rocket science\u2014and derive an exact solution that will land the rocket on the moon in one or a few trials. Similarly, if you developed a deep net controlling a human body, and you wanted it to learn to safely navigate a city without getting hit by cars, the net would have to die many thousands of times in various situations until it could infer that cars are dangerous, and develop appropriate avoidance behaviors. Dropped into a new city, the net would have to relearn most of what it knows. On the other hand, humans are able to learn safe behaviors without having to die even once\u2014again, thanks to our power of abstract modeling of hypothetical situations." "In short, despite our progress on machine perception, we\u2019re still far from humanlevel AI. Our models can only perform local generalization, adapting to new situations that must be similar to past data, whereas human cognition is capable of extreme generalization, quickly adapting to radically novel situations and planning for long-term future situations."'))},m=function(){return i.a.createElement("nav",{class:"navbar navbar-expand-lg navbar-light bg-light"},i.a.createElement("a",{class:"navbar-brand",href:"/zine-blog"},"DL Zine's"),i.a.createElement("button",{class:"navbar-toggler",type:"button","data-toggle":"collapse","data-target":"#navbarNav","aria-controls":"navbarNav","aria-expanded":"false","aria-label":"Toggle navigation"},i.a.createElement("span",{class:"navbar-toggler-icon"})),i.a.createElement("div",{class:"collapse navbar-collapse",id:"navbarNav"},i.a.createElement("ul",{class:"navbar-nav"},i.a.createElement("li",{class:"nav-item active"},i.a.createElement("a",{class:"nav-link",href:"/zine-blog"},"Home ",i.a.createElement("span",{class:"sr-only"},"(current)"))),i.a.createElement("li",{class:"nav-item"},i.a.createElement(s.b,{class:"nav-link",to:"/zine-blog/topics"},"Topics")))))},d=function(e){e.match;return i.a.createElement("div",{style:{margin:"5vh"}},i.a.createElement("h1",null,"Topics"),i.a.createElement("div",{class:"row",style:{margin:"5vh"}},i.a.createElement("div",{class:"col-sm-4"},i.a.createElement("div",{class:"card"},i.a.createElement("div",{class:"card-body"},i.a.createElement("h5",{class:"card-title"},"Everybody loves Convolutions..."),i.a.createElement("p",{class:"card-text"},"The focus here is on convolutional neural networks, the convolution operation and how it relates to neural nets, seeing convolution as just matrix multiplication and more!"),i.a.createElement(s.b,{to:"/cnns",class:"btn btn-primary"},"Read")))),i.a.createElement("div",{class:"col-sm-4"},i.a.createElement("div",{class:"card"},i.a.createElement("div",{class:"card-body"},i.a.createElement("h5",{class:"card-title"},"Limitations"),i.a.createElement("p",{class:"card-text"},"The aim of this post is to learn about the Limitations of neural networks before you embark on some new project so you can avoid wasting time."),i.a.createElement(s.b,{to:"/limitations",class:"btn btn-primary"},"Read")))),i.a.createElement("div",{class:"col-sm-4"},i.a.createElement("div",{class:"card"},i.a.createElement("div",{class:"card-body"},i.a.createElement("h5",{class:"card-title"},"SGD"),i.a.createElement("p",{class:"card-text"}),i.a.createElement(s.b,{to:"/sgd",class:"btn btn-primary"},"Read"))))),i.a.createElement("div",{class:"row",style:{margin:"5vh"}},i.a.createElement("div",{class:"col-sm-4"},i.a.createElement("div",{class:"card"},i.a.createElement("div",{class:"card-body"},i.a.createElement("h5",{class:"card-title"},"Batch Norm"),i.a.createElement("p",{class:"card-text"}),i.a.createElement(s.b,{to:"/batch_norm",class:"btn btn-primary"},"Read")))),i.a.createElement("div",{class:"col-sm-4"},i.a.createElement("div",{class:"card"},i.a.createElement("div",{class:"card-body"},i.a.createElement("h5",{class:"card-title"},"Probability and Information Theory"),i.a.createElement("p",{class:"card-text"}),i.a.createElement(s.b,{to:"/#",class:"btn btn-primary"},"Read")))),i.a.createElement("div",{class:"col-sm-4"},i.a.createElement("div",{class:"card"},i.a.createElement("div",{class:"card-body"},i.a.createElement("h5",{class:"card-title"},"Linear Algebra for ML"),i.a.createElement("p",{class:"card-text"}),i.a.createElement(s.b,{to:"/linear_algebra",class:"btn btn-primary"},"Read"))))))},u=function(){return i.a.createElement("div",null,i.a.createElement("h1",null,"Home"),i.a.createElement("p",null,"This site is designed to produce helpful content for people trying to learn deep learning and for me to formalize my understanding. I try and take work from brilliant educators and practioners and condense and describe them in a simplistic way to enable new comers to learn without being intimidated."))},f=function(){return i.a.createElement("div",{style:{margin:"5vh"}},i.a.createElement("h1",null,"Stochastic Gradient Descent"),i.a.createElement("h2",null,"How do you create a predictor function as a function of pixel values?"),i.a.createElement("p",null,"What does this mean? Well the broader question here is how does a simple series of matrix multiplications result in a good prediction based on its input.",i.a.createElement("br",null),i.a.createElement("img",{src:"https://latex.codecogs.com/gif.latex?%24y%3Dmx&plus;b%24"}),i.a.createElement("br",null),i.a.createElement("img",{src:"https://latex.codecogs.com/gif.latex?%24y%3Da_1x_1&plus;a_2*x_2%24"})))},p=function(){return i.a.createElement("div",{style:{margin:"5vh"}},i.a.createElement("h1",null,"Batch Norm"),i.a.createElement("h3",null,"What is Batch Normalization? It accelerates training by reducing internal covariate shift. ... Or does it?"),i.a.createElement("h3",null,"How does it work?"),i.a.createElement("p",null,"Batch norm removes the mean and and divides by the standard deviation of a channel of activations.",i.a.createElement("br",null),"Insert Channel Activation picture",i.a.createElement("br",null),i.a.createElement("img",{src:"https://miro.medium.com/max/810/1*Hiq-rLFGDpESpr8QNsJ1jg.png"}),i.a.createElement("br",null),'"How does batch normalization help optimization? Not by reducing internal covariate shift. What it does do, is allow you to increase your learning rate. The standard training without batch norm has a much more bumpier loss landscape but with batch norm there are less big bumps. So there is less risk of divergence. It is simple, it takes a mini-batch and it\u2019s activations. Firstly, we find the mean of the activativations, then the variance. Then we normalize, so element-wise activation values minus the mean divided by the standard deviation. The really important bit is: we take those normalized activation values and we add a vector of biases. Then we use something that\u2019s like a bias but instead we multiply by it. We have a gamma which we multiply by those normalized activations and add a bias beta. These two parameters are learnable parameters. The value of our predictions y_hat = f(w_1, w_2,...,w_n, X) is some function of our weights (there could be millions) and our input. This is our neural net function. Our loss: L = Sum(y_hat-y)^2. We want to predict outcomes between 1 and 5. Our activations and our final layer are between -1 and 1, and they\u2019re way off. The scale is off the mean is off. One thing we can do is try to come up with new weights to cause the spread and mean to increase. But this is hard since all these weights interact in specific ways. So that will take ages. But what if we amended that neural net function: y_hat = f(w_1, w_2, \u2026, w_n, X) * gamma + beta. These are two more parameter vectors, so this is really easy. To increase the scale or change the mean we have these two parameters and we have their gradient to increase the scale/mean. That is what batch norm does. It makes it easier to shift the outputs up and down etc. In practice, we don\u2019t use a different mean and standard deviation for each mini-batch since it would vary so greatly. So instead we take the exponentially weighted moving average of the mean and standard deviation. They use the same technique as they did for adam to calculate this. You can vary the amount of momentum in the batch norm layer. The smaller the momentum the less the standard deviation and mean will vary less from mini-batch to mini-batch and will have less of a regularising effect.""'),i.a.createElement("p",null,"The beta is a learnable mean and gamma the standard deviation. This reparameterises the network. Batch Norm stabilises optimsation allowing much higher learning rates (by reducing the peaks of the loss landscape), it injects noise through basic statistics to improve generalisation, it reduces sensitivity to weight initialisation and it interacts with weight decay to control learning rate dynamics.",i.a.createElement("img",{src:"https://296830-909578-raikfcquaxqncofqfm.stackpathdns.com/wp-content/uploads/2019/06/Artboard-1-7-1-1.svg"}),"It is evident that the network with batch norm is stable over a much larger range of learning rates (in log scale). This accelerates training as claimed in the original paper as we can safely use much higher learning rates.",i.a.createElement("h3",null,"Drawbacks"),"* Batch norm is slow, it's different at training and test time, it's ineffective for small batches and various layer types, it has multiple interacting effects which are hard to separate. Monitoring histograms of channel activations is rarely done but essential. Each channel has a different colour.",i.a.createElement("img",{src:"https://296830-909578-raikfcquaxqncofqfm.stackpathdns.com/wp-content/uploads/2019/06/before_after_0.svg"}),i.a.createElement("h3",null,"Resources:"),i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("a",{href:"https://myrtle.ai/how-to-train-your-resnet-7-batch-norm/"},"Myrtle.ai")),i.a.createElement("li",null,i.a.createElement("a",{href:"https://arxiv.org/pdf/1805.11604.pdf"},"How Does Batch Normalization Help Optimization?")))),i.a.createElement("p",null,"Batch Normalization (Szegedy): Batch Norm doesn\u2019t reduce Covariate Shift nor is it why how it works? What it does is, it allows you to increase your learning rate and the loss is less bumpy. The weights used to jump off an awful part of the weight space it can\u2019t recover from. The algorithm takes a mini-batch and takes in some activations from a layer. Find the mean of those activations and variance. Then normalize, the values minus mean divided by standard deviation. Important bit: take those values and add a vector of bias\u2019 (beta) and times a gamma (both learnable params). The value of our predictions is a function of our weights and inputs to our layer i.e. our NN function. Our loss is our actuals minus predicted squared. E.g. lets say our actuals are between 1-5 but our predictinos are -1-1. But our weights are intertwined in this landscape. Why not: add a *gamma + b to our NN function. We now have a direct gradient to change the scale and the mean. Makes it easier to shift the outputs up and down and in and out. So y_hat = F(w1, w2, w3, \u2026. Wm, X) * gamma + b. We take the exponentially weighted moving averages of the mean and standard deviation."))},g=function(){return i.a.createElement("div",null,i.a.createElement("h1",null,"Linear Algebra for ML"),i.a.createElement("div",null,i.a.createElement("h3",null,"Linear Systems of Equations"),i.a.createElement("p",null,"How do we solve for a system of linear equations? If I saw this title or if I saw this in a maths book without much clarification what would this mean to me? When we have two or more linear equations, this is a system of linear equations. Put simply, it's when we have two or more equations that define a line. If we graph the relationship between those lines then we can define their relationship to one another. Say those two lines intersect? Well then there is exactly one solution to this system of linear equations. If they do no intersect then there is no solutions to the system of equations. If they lie on the same line there are an infinite amount of solutions.",i.a.createElement("br",null),i.a.createElement("br",null),i.a.createElement("img",{src:"https://www.mathsisfun.com/algebra/images/system-linear-types.svg"}),i.a.createElement("br",null),i.a.createElement("br",null))),'We need to find the values of the variables x1, .., xn that satisfies all equations, that is to say the values taken by the unknowns. Name the 3 possible cases a system of linear of equations Ax = b must be? No solution, One solution or infinite number of solutions. Why cant there be more than one solution and less than infinite? Because if you visualize it in R2, a solution corresponds to the intersection of two lines so two lines will either never meet \u2013 no solution (parallel), they intersect i.e. 1 solution or are linearly dependent \u2013 infinite number of solutions (superimposed). Write a proof for it? Say Ax = b and Ay = b and Az = b with z being the solution. So Az = A(alphax + ( 1 \u2013 alpha)y) which = Axalpha + Ay( 1 \u2013 alpha), And since Ax = Ay = b. This leads to Az = balpha + b(1-alpha) which = balpha + b \u2013 balpha which = b. So z is the solution. Can Ax = b be represented by a matrix A? Yes matrix A contains all the weights of each variable in the vector x which equals = b. This corresponds to a set of linear equations. The number of columns of A is the number of dimensions of our vector space and the number n of directions we can travel in. The number of solutions of our linear system corresponds to the number of ways we can reach b by travelling through our n dimensions. What is an overdetermined system? It is when we have more equations m than unknowns n. E.g. say we have three lines that do not intersect all together. What is an undetermined system? If there is more unknowns than equations. E.g. there is only 1 equation i.e. 1 line and 2 dimensions, each point on that line is a solution therefore there is an infinite number of solutions. What is a linear combination? The linear combination of 2 vectors is their weighted sum. E.g. u = [1, 3] and v = [2, 1]. The linear combination of them is au + bu = a [1, 3] + b[2, 1] with a and b being the weights of the vectors. E.g. if a = 2 and b = 1 we get [4, 7]. What is Span? Well it is all the points that the linear combination of the 2 vectors above can reach by changing the combinations of a and b. What is a space? The space of a vector is all the values that can be taken by this vector denoted as R. Rn is the n amount of dimensions. What is a subspace? If you take a 2-d plane in R3, this plane is a subspace of your original R3 space. So a line in your R2 space, this line is a subspace of your original space. The linear combination of vectors gives vectors in the original space. What is the column form? You can represent a set of equations by considering the solution of b as corresponding to a linear combination of each column multiplied by their weights. So weights times the variables. We travel from the origin to the point of the coordinates of b. The columns of AA give us the directions we can travel by and their weights are the length of the way in that direction. What is linear dependence? Columns are linearly dependent if one of them is a linear combination of others. The direction of two linearly dependent vectors is the same. The row picture shows the lines are parallel, and the column picture shows them on the same line. What is a hyperplane? A hyper plane is n-1 dimensional. What is a Norm? Any function with the following properties: Non-negative (it is a length thus can\u2019t be negative), Norms are 0 if and only if vectors are zero vectors, Norms respect the triangle inequality and the norm of a vector multiplied by a scalar is equal to the absolute value of this scalar multiplied by the norm of the vector. ||k . u|| = |k| . ||u||. It is usually written with two horizontal bars ||u||. Absolute value is a magnitude, can be thought of as the distance from the origin. What is the triangle inequality? The norm of the sum of some vectors is less than or equal to the sum of the norms of these vectors. \u2225u+v\u2225\u2264\u2225u\u2225+\u2225v\u2225 Geometrically this means the shortest path between two points is a line. How do you get the p-norm of a vector? Calculate the absolute value of each element, take the power p of these absolute values, sum all these powered absolute value, take the power 1/p of this result. \u2225x\u2225p=(\u2211i|xi|p)1/p So the P in p norm is the power of e.g. L0, L1, L2. What is the L0 norm? All positive values give you a 1 if you calculate its power as 0 except 0 which gives you 0. This norm tells you the number of non-zero elements in the vector. Not really a norm as if you multiply a vector by alpha, you get the same number back. What is the L1 norm? P=1 so this norm is simply the sum of the absolute values. \u2225x\u22251=\u2211i|xi| What is the L2 norm? The Euclidean norm is p=2. \u2225x\u22252=(\u2211ix2i)1/2\u21d4\u221a\u2211ix2i The Euclidean norm corresponds to the length of the vector from origin to point. E.g. u=[3,4] So ||u||2 = sqrt |3|2 + |4|2 which = sqrt 25 which = 5 so the L2 norm is 5. What is the squared Euclidean norm? It is convenient since it removes negatives and removes the square root and we end up with the simple sum of every squared values of the vector. It is widely used because it can be calculated with xTx. What is the derivative of the squared L2 norm? Its partial derivatives are easy to calculate. The derivative of the L2 is complicated as it takes every element of the vector into account. However, the squared L2 norm barely discriminates between 0 and small values because the increase of the function is slow. What is the max norm? It is the Linfinity norm and corresponds to the absolute value of the greatest element of the vector. \u2225x\u2225\u221e=maxi|xi| What is the frobenius norm? It is the same as the L2 norm of the matrix after flattening. It\u2019s the square root of the L2 norm of a matrix. What are dot products with norms? I believe they give you the cosine angle of vectors. xTy=\u2225x\u22252\u22c5\u2225y\u22252cos\u03b8 E.g. x = [0,2] and y=[2,2]. We see that \u03b8 is equal to 45 degrees. Xty = [0, 2] . [2, 2] = 0 x 2 + 2 x 2 = 4 and \u2225x\u22252=\u221a02+22 = \u221a4 = 2 \u2225y\u22252=\u221a22+22 = 8 2 \xd7 \u221a 8 \xd7 cos(45) = 4 What are diagonal matrices? A matrix Ai,j is diagonal if its entries are all zeros except on the diagonal when I = j. A diagonal matrix is denoted diag(v) where v is the vector containing the diagonal values. The multiplication of a diagonal matrix and a vector is as you\u2019d expect. The invert of a square diagonal matrix exists if all entries of the diagonal are non-zero. What is a symmetric matrix? A matrix A is symmetric if it is equal to its transpose i.e. A = AT . What is a unit vector? A unit vector is a vector of length 1 denoted u_hat. What is an orthogonal vector? Two orthogonal vectors are separated by a 90 degree angle. Their dot product is 0. What is orthonormal? When the norm of orthogonal vectors is the unit norm. It is impossible to have more than n vectors mutually orthogonal in Rn. They won\u2019t be mutually orthogonal i.e. wont share the 90 degree angle. What is unit norm? The unit vector obtained by normalizing the normal vector (i.e., dividing a nonzero normal vector by its vector norm) is the unit normal vector, often known simply as the "unit normal." What is an orthogonal matrix? A matrix is orthogonal if columns are mutually orthogonal and have a unit norm (orthonormal) and rows are mutually orthonormal and have unit norm. E.g. A1,1 , A2,1 and A1,2 , A2,2 are orthogonal vectors and the rows A1,1 A1,2 and A2,1 A2,2 are orthogonal vectors. The rows and columns are orthogonal vectors. Name some properties of an orthogonal matrix? ATA = AAT = I. E.g. A = a,b,c,d and thus A transpose is a,c,b,d and their product is: The norm of a vector [a,c] is a2 + c2. The rows of A have unit norm because A is orthogonal. This means that a2 + c2 = 1 and b2 + d2 = 1. So we have: Name another property? AT = A-1. We can show that if ATA = I then AT = A-1. If we multiply each side of the equation by A inverse we have. (ATA)A-1=IA-1. When a matrix or vector is multiplied by the identity matrix it doesn\u2019t change. So we have (ATA)A-1=IA-1 and that matrix multiplication is associate so we can remove the paranthesis: ATAA-1=IA-1 , we also know AA-1 = I so we can replace ATI=A-1. This shows that AT=A-1. What is eigen decomposition? It is one form of matrix decomposition. Decomposing a matrix means we want to find the product of matrices that is equal to the initial matrix. We decompose the initial matrix into the product of its eigenvectors and eigenvalues. We can think of matrices as linear transformations. Some matrices will rotate your space, others rescale. So when we apply a matrix to a vector we end up with a transformed version of the vector. By apply we mean we calculate the dot product of the matrix with the vector. What are Eigenvectors and Eigenvalues? Imagine that the transformation of the initial vector gives a new vector that has the same direction. The scale can be different but in the same direction. Applying the matrix didn\u2019t change the direction. This special vector is called an eigenvector of the matrix. Finding these eigenvectors can be useful. This means that v is an eigenvector of A if v and Av are in the same direction. That is to say that the vectors Av and v are parallel. The output vector is just a scaled version of the input vector. The scaling factor \u03bb is called the eigenvalue of A. Av=\u03bbv. The eigenvector of matrix A is equivalent to the scaling factor of the initial vector v. E.g. A = [5,1][3,3] and we know that one eigenvector of A is v = [1,1] we can check that Av=\u03bbv: by A dot v = [6,6]. We can see that 6 x [1,1] = [6,6] which means v is an eigenvector of A and the eigenvalue is 6.  If v is an eigenvector of A then any rescaled vector sv is also an eigenvector of A. How do we decompose a matrix using eigenvectors and eigenvalues? All eigenvectors of a matrix A can be concatenated in a matrix with each column corresponding to each eigenvector. The vector of eigenvalues can be created from this. The eigen decomposition is given by: A=V\u22c5diag(\u03bb)\u22c5V\u22121. Diag(v) is a diagonal matrix containing all the eigenvalues. E.g. V = [1,1][1,-3] and the diagonal matrix is all zeros except the diagonal that being the vector of our eigenvalues: diag(v)=[6,0][0,2]. The inverse of V = [0.75, 0.25][0.25, -0.25]. Lets plug it into our equation on the right: A is the original matrix shown above. What is a real symmetric matrix? A=Q\u039bQT where Q is the matrix with eigenvectors as columns and \u039b is the diag(\u03bb). Symmetric matrices are useful for this! How can eigendecomposition be used for quadratic forms? They can be used to optimize quadratic functions. When x takes the values of an eigenvector, f(x) takes the values of its eigenvalue. With x = x1, x2 and A = a,b,c,d we call them matrix forms. This form is used to do various things on the quadratic equation like constrained optimization. E.g a gives you the number x21, (b+c) the number x1x2 and d the number x22. This means that the same quadratic form can be obtained from infinite number of matrices A by changing b and c while preserving their sum.  See : What is linear substitution? A change of variable means we replace a variable with another. It can be used to remove cross terms (a quadratic form without a cross term is called diagonal form since it comes from a diagonal matrix) in our equation. Without the cross term its easier to characterize and optimize the function (finding max or minimum). The change of variable will end up with a new equation but finding a specific substitute can simplify our statement (equation), and can be used to get rid of the cross term. The correct substitution is given by the eigenvectors of the matrix used to generate the quadratic form. E.g. x = x1, x2 and A = 6,2,2,3 and the eigenvectors of A are 0.89442719, 0.4472136, 0.4472136, 0.89442719, which can be replaced with 1/sqrt(5)[2,-1][1,2] (https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.7-Eigendecomposition/). The first eigen vector is 1/sqrt(5)[2,1] and second is 1/sqrt(5)[-1,2]. Thus see below: What is principal axes theorem? A simpler way to do the change of variable. Recall f(x)=xTAx and the linear substitution can be written in these terms. We want to replace the variables x by y that relates by: x=Py and we want to find P such as our new equation after changing the variable doesn\u2019t contain the cross terms. Start by replacing this in the first equation xTAx=(Py)TA(Py)=yT(PTAP)y. We also know A is symmetric, so the diagonal matrix D contains the eigen vectors of A such that D = PTAP. We end up with xTAx = yTDy. This implies we can use D to simplify our quadratic and remove the cross terms. To summarise, the principal axes form can be found with: xTAx=\u03bb1y21+\u03bb2y22 where \u03bb1\u03bb1 is the eigenvalue corresponding to the first eigenvector and \u03bb2\u03bb2 the eigenvalue corresponding to the second eigenvector (second column of x). How do you find f(x) with eigendecomposition? You can with the eigenvectors/values when x is a unit vector: so f(x) = xTAx we know that if x is an eigenvector of A and \u03bb the eigenvalue then Ax = \u03bbx and replace the term we get xT\u03bbx = xTx \u03bb. Since x is a unit vector (||x||2 = 1 or xTx =1) we end up with f(x) = \u03bb. If x is an eigenvector of A, f(x) = xTAx will take the value of the corresponding eigenvalue. Thus, this only works if the Euclidean norm of x is 1 a unit vector. What is quadratic form optimisation? Optimisation is finding the max or min of a function. Used to minimize the cost function. Eigendecomposition can be used to optimize quadratic functions and can be done without cross terms. The difficulty is we want a constrained optimization, that is to find the min or max of the function for f(x) being a unit vector. Thus, we want to optimize f(x) = xTAx subject to ||x||2 =1. The minimum of f(x) is the minimum eigenvalue of the corresponding matrix A. This value is obtained when x takes the value of the corresponding eigenvector. We saw that the quadratic function can be represented by the symmetric matrix A. These functions can take one of three general shapes. With the constraint that x is a unit vector, the minimum of the function f(x) corresponds to smallest eigenvalue and its eigenvector. The max is the biggest eigenvalue and its eigenvector. Eigendecomposition can\u2019t be used for non-square matrices this is where Singular Value Decomposition comes in. Please summarize? Linear transformations transform an vector with operations like rotation. Every transformations can be seen as applying a matrix on the input vector. What is Singular Value Decomposition? Eigendecomposition cant decompose square matrices so we need SVD for that. So we need to decompose matrix A into 3 matrices instead of 2 with ED. The matrices U, D and V have the following properties: U and V are orthogonal matrices, and D is a diagonal matrix but not necessarily square. The columns of U are the left-singular vectors of A while the columns of V are the right-singular vectors of A. The values along the diagonal of D are the singular values of A. Right is the dimensions of the factorization. A is a matrix that can be seen as a linear transformation, this can be decomposed in three sub-transformations: 1. Rotation, 2. Re-scaling, 3. Rotation. These three steps correspond to the three matrices U, D, and V. Every matrix is a linear transformation of a vector or another matrix. If we apply a matrix to a unit circle it will expand/contract/shift this circle. The general rule is that the transformation associated with diagonal matrices imply only a rescaling of each coordinate without rotation. Matrices that are not diagonal can produce a rotation. Vectors/matrices can be transformed by rotating or scaling matrices. The SVD can be seen as the decomposition of one complex transformation in 3 simpler transformations(a rotation, a scaling and another rotation). With non-square matrices we e.g. may take a 3 by 2 matrix will map a 2D space to 3D. What are the three transformations? We use the three transformations of the SVD as equivalent to the transformation done with the original matrix. What are the singular values? In descending order, they are a new set of features (that are a linear combination of the original features) with the first feature explaining most of the variance. The major axis of the ellipse will be the first left singular value (u1) and its norm will be the first singular value (sigma 1). They are the major (sigma 1 u1) and minor (sigma 2 u2) axes of the ellipse and the major axis is associated with more variance. How are the sub-transformations found? U, D and V can be found by transforming A in a square matrix and computing the eigenvectors of this square matrix. The square matrix can be obtained by multiplying the matrix A by its transpose. U=eigenvectors of AAT, V = eigenvectors of ATA and D = eigenvalues of AAT and ATA. E.g. A is non-square matrix = [7,2][3,4][5,3]. The left singular values of A = eigenvectors of AAT. The right singular values of A correspond to the eigenvectors of ATA. The nonzero singular values of A are the square roots of the eigenvalues of ATA and AAT. What is the Moore-Penrose Pseudoinverse? This is the direct application of the SVD. The case where the set of equations have 0 or many solutions the inverse cannot be found and the equation cannot be solved. The pseudoinverse is A+ such as AA+ approaches In minimizing the ||AA+ - In||2. The following formula is used to find the pseudoinverse: ||AA+ - In||2. A+ is the pseudoinverse of A and D+ the pseudoinverse of D. D is the diagonal matrix thus D+ can be calculated by taking the reciprocal (the quantity obtained by dividing the number one by a given quantity) of the non-zero values of D. E.g. A is non-square matrix = [7,2][3,4][5,3], lets calculate its SVD and then pseudoinverse. Another way to compute the pseudo is to use this formula: (ATA)-1AT. How can you use the pseudo to solve an overdetermined system of linear equations? In general, there is no solution to overdetermined systems, no point of intersection of the three lines. The pseudo solves the system in the least square error perspective: finds the solution that minimizes the error. This gives you the point at the intersection of the three symmedians of the triangle. This same method can be used to fit a line to a set of points. We have a set of x and y co-ordinates and we want to find a line y = mx+b that minimizes the error. The error is the sum of the differences between the fit and the actual data points. The matrix A are the values of the coefficients. What is the trace operator? The trace is the sum of all values in the diagonal of a square matrix. The trace can be used to specify the Frobenius norm of a matrix, that norm being equivalent to the L2 norm for matrices (take the square of all the elements then sum them and then square root the result).  Since the transpose of a matrix doesn\u2019t change the diagonal, the trace of the matrix is equal to the trace of its transpose. What is the trace of a product? Tr(ABC) = Tr(CAB) = Tr(BCA) What is the determinant? The determinant of a matrix A is a number corresponding to the multiplicative change you get when you transform your space with this matrix. A negative determinant means there is a change in orientation. A change in orientation means for instance in 2D we take a plane out of these 2 dimensions, do some transformations and get back to the initial 2D space. The second transformation cant be obtained through rotation and rescaling. Thus the sign can tell you the nature of the transformation associated with the matrix. The determinant also gives you the amount of transformation. The absolute value of the determinant corresponds to the area of the transformed figure. We have two vectors 0,1 and 1,0 and apply matrix a 2,0 0,2 which is a diagonal matrix. This will rescale our space, without any rotation. It will rescale each dimension because the diagonal values are the same. The determinant of A is 4. The transformation has multiplied the area of the unit square by 4, the lengths of the newi and newj are 2 thus 2*2=4. What is Principal components analysis? Dimensions are crucial in data science, the dimensions are all the features (columns) of the dataset. Dimensions = columns, two dimensions easy to plot, >3 harder to visualize. With n amount of dimensions, some might be correlated. One way to reduce dimensionality is to simply keep only some of them, but you loose good information. It would be nice to reduce these dimensions while keeping all the information present in the dataset. The aim of PCA is reduce the number of dimensions of a dataset where the dimensions are not completely decorrelated. PCA provides a new set of dimensions the principal components. They are ordered: the PC is the dimension having the largest variance. Each PC is orthogonal to the preceding one. Remember that orthogonal vectors means that their dot product is equal to 0. This means each PC is decorrelated to the preceding one and is better than feature selection.  Unit vectors are an example of orthogonal vectors: The problem can be expressed as finding a function that converts a set of data points from Rn to Rl. This means we change the number of dimensions of the dataset. We also need a function that can decode back from the transformed dataset to the initial one: The encoding function f(x) transforms x into c and the decoding function transforms back c into an approximation of x. PCA will respect some constraints: 1. The decoding function has to be a simple matrix multiplication g(c) = Dc. By applying the matrix D to the dataset from the new coordinate system we should get back to the initial coordinate system. 2. The columns of D must be orthogonal. 3. Columns of D must have unit norm. We want a decoding function which is a simple matmul. We will then find the encoding function from the decoding function. We want to minimize the error between the decoded data point and actual data point i.e. reducing the distance between x and g(c) using squared L2 norm: ||x-g(c)||22. Lets call c* the optimal c. C* = argminc ||x-g(c)||22. L2 norm can be expressed as ||y||22 = yTy, we have named y to avoid confusion with x, here y=x-g(c). Thus the equation that we want to minimize becomes: (x-g(c))T(x-g(c)) and transpose respects addition we have (x T-g(c) T) (x-g(c)). The distributive property: xTx\u2212xTg(c)\u2212g(c)Tx+g(c)Tg(c), the commutative property tells us xTy=yTx. So the equation becomes: xTx\u22122xTg(c)+g(c)Tg(c). DON\u2019T UNDERSTAND BEFORE MINIMIZING THE FUNCTION BIT (https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.12-Example-Principal-Components-Analysis/). But essentially we remove the xTx from the c*=argminc -2xTg(c)+g(c)Tg(c) and since g(c ) = Dc we replace g(c ) with Dc. With (Dc)T = cTDT we have c*=argminc - 2xTDc+cTDTDc. DTD=It because D is orthogonal and their columns have unit norm. We replace the equation: -2xTDc+cTIlc with -2xTDc+cTc. How do we find the minimum of -2xTDc+cTc? Use gradient descent. The main idea of the derivative of the function at a specific value of x tells you if you need to increase or decrease x to reach the min. The gradient is a vector containing the partial derivatives of all dimensions. Its notation is \u2207xf(x). How do we calculate the gradient of a function? Here we want to min through each dim of c. We want a slope of 0. \u2207c(\u22122xTDc+cTc)=0. Lets take these terms separately to calculate the derivative according to c. d(\u22122xTDc)/dc = -2xTD. Great we found the encoding function the dimensions are below: And to go back from c to x we use g(c ) = Dc: r(x)=g(f(x)=DDTx. The next step is to find matrix D. Recall the purpose of PCA is to change the coordinate system in order to maximize the variance along the first dimensions of the projected space. This is equivalent to minimizing the error between data points and their reconstruction. Maximizing the variance is equal to minimizing the error of the reconstruction. Since we have to take all points into account we use the Frobenius norm of the errors which is equal to L2 norm for matrices. It is like unrolling the matrix to a one dimensional vector and take the L2 norm of that vector. D* is the optimal D. D* = argminD sqrt sum over x(i)j \u2013 r(x(i)j)2. With the constraint DTD=It and columns of D orthogonal. We start to find only the first PC, we will have l =1. So matrix D will have shape (n x 1): it\u2019s a simple column vector. We can thus remove the sum over j and the square root since we take the squared L2 norm instead of frobenius norm as it\u2019s a vector not matrix. r(x) = DDTx and we are looking for the first PC r(x) = ddTx. We can plug r(x) into the equation: d*=argmind\u2211I ||x(i)\u2212ddTx(i)||22. Because of the constrain the columns of D have unit norms we have ||d||2 = 1, d is one column of D. d*=argmind||X\u2212XddT||2F. with the constraint ddT=1. We then use the trace operator to simplify the equation to minimize. Recall ||A||F= sqrt Tr(AAT) So here A = X \u2013 XddT. (DON\u2019T UNDERSTAND ABOVE EIGENDECOMPOSITION). We will see that we can find the maximum of the function by calculating the eigenvectors of XTX. What is the covariance matrix? The optimization problem of maximizing the variance of components and minimizing the error between reconstructed and actual data are equivalent. If we centre data around 0, XTX is the covariance matrix. The Covariance matrix is n by n matrix (n being dims).  Its diagonal is the variance of the dimensions and the other cells are the covariance between the two corresponding dimensions (amount of redundancy). The largest covariance between two dims the more redundancy exists between these dims. To max the variance and min covariance (to deccorelate) means the ideal covariance is a diagonal matrix. Thus, the diagonalization of the covariance matrix will give us the optimal solution. Highly correlated data means the dims are redundant, possible to predict one from the other w/out losing much info. The first processing we do is center the data around 0. PCA is a regression model without intercept so the first PC crosses the origin.  We can now look for PCs and they are the values taken by d that max d*, to find d we calculate the eigenvectors of XTX. These are the vectors maximizing our function, the vector associated with the larger eigenvalue tells us the direction associated with the larger variance in our data \u2013 the blue vector.  The blue vector direction corresponds to the oblique shape of the data. The idea is that if you project the data points on the line corresponding to the blue vector direction you will end up with the largest variance. This vector has the direction that max\u2019s variance of the projected data. The second eigenvector is orthogonal to the orange (blue), representing the biggest eigenvalue. Now we have found d we will use the encoding function to rotate the data, the goal being to end up with a new coordinate system where data is uncorrelated and thus the basis axes gather all the variance. Thus only a few axes are kept. The rotation then transforms the dataset so we have more variance on one of the basis axis. What is a Hessian? We need to first understand what a gradient is. What is a gradient? The rate of change of some function in various directions. The derivative of some function (usually the loss) is basically how much the loss changes if you change the input by a very small amount. In 1-D, the gradient is simply the slope of the function at that point. The diagram to the right is for one variable (x-axis), but many functions have multiple inputs. Geometrically, each input can be seen as having a different direction. For such functions, there is a different rate of change for each direction. The gradient is simply a collection (vector) of the derivatives of the function for each direction. Each element of the gradient is simply the slope of the function in each direction. Next we need to understand second-order derivatives. What is a second-order derivative? The second-order derivative is simply the derivative of the derivative. It is the rate of change of the slope. See the two lines to the left. The red arrows represent the derivative/slope at each point. In the left plot the slope changes a lot while the right one is more stable. The rate of change of the slope corresponds to how curved each loss function is. The sharper the curve the more rapidly the slope changes. In high-dimensional space, there are different rates of change = slope for each direction. What\u2019s more, there is a different rate of change of the slope in each direction as well, meaning that the number of second-order derivatives in d2, where d is the dimensionality of the input. This is why second-order derivatives are expressed by the Hessian matrix is defined by: Each row represents the change of the gradient in a certain direction. Each column can be thought of as the gradient of one element of the gradient. The hessian is represented by H. It is important to remember that regardless of what dimensional space we are in: the second-order derivative represents the curvature of a function. The curvature will differ according to direction. NN\u2019s are too complex to analyse so we will build our intuition using the following, simple problem: minimizing a quadratic form. Quadratic forms: a 2-D quadratic form is simply any function that can be expressed like:  When we write this in matrix form, we get the following equation . Where A is symmetric(ith row and ith column are the same). For this function, the gradient is Ax-b and the Hessian is A. The optimal solution is where the gradient Ax-b becomes 0. What does a quadratic for look like? It depends on the Hessian: The function curves in different ways according to the Hessian. But how is the curvature determined by the Hessian? The relationship between the shape of the quadratic form and the Hessian\u2019s eigenvalues. The eigenvectors of matrix M do not change direction when multiplied with M and the eigenvalues represent the change in length of the eigenvector when multiplied with M.  where vi is an eigenvector and  is an eigenvalue. Each eigenvector has a single eigenvalue as a pair. Eigenvectors and values have important properties in a Hessian: 1. Each eigenvector represents a direction where the curvature is independent of other directions. 2. The curvature in the direction of the eigenvector is determined by the eigenvalue. If the eigenvalue is larger, there is a larger curvature, and if it is positive, the curvature will be positive and vice-versa. Lets have a look at the first element in second row of the H:  , . This is the rate of change in direction x2 of the gradient in direction x1. How confusing right? Wouldn\u2019t it be great if the gradient only changed in the direction it pointed in? This is where the eigenvectors come to our rescue. For the eigenvectors, the gradient only changes in the direction it points in. In other words, all the non-diagonal elements of the Hessian are 0 for the eigenvectors. This is clearer when we plot the contours of the loss function:    A contour is a set of points where the loss function is equal. Here the eigenvectors are the axes of the ellipses. The length of the axis is determined by the eigenvalue in its direction. Larger eigenvalue=larger curvature=larger rate of change, so the axes in the direction of the large eigenvalues are actually shorter. Looking back up at the loss functions above, we can see (a) the loss is curving upwards in two directions so has two positive eigenvalues, whereas the eigenvalues are both negative for (b). (d) has one negative and one positive eigenvalue. (c) is special since one of its eigenvalues is 0 hence why the loss is flat across one axis. So what\u2019s the relationship between the Hessian and Optimisation? The key intuition is the relation between the eigenvalue, the eigenvector and the speed and direction of convergence when using gradient descent. The larger the eigenvalue, the faster the convergence from the direction of its corresponding eigenvector. Can be derived as follows: 1. A gradient descent step with step size  can be expressed as:  2. If  (the ith eigenvector),  where  is the initial vector. 3. Any vector  can be expressed as a weighted sum of eigenvectors. Thus, for each component, the convergence rate is . This is because the curve is steeper in directions with larger eigenvalues. The stronger the curve the quicker the ball can reach the minima where the gradient is zero. In the left diagram the gradient changes quickly so crosses 0 quickly as well. In the right, the gradient slowly changes so does not reach 0 within the diagram. This means you have to pick the right step size in case you diverge. What concerns us is whether the sign of the gradient is the same, if we take a step that is too large, we could end up actually increasing the loss. The ideal step size for each eigenvector component is when x converges to 0 in one step. Therefore, it is . If all the eigenvalues are the same, choosing the step size is trivial. The problem is when eigenvalues are very different. We have to be careful not to allow any of the components to diverge: this means the step-size is effectively bounded by the largest curvature = largest eigenvalue. The speed of convergence is determined by the smallest eigenvalue. This ratio between the largest and smallest eigenvalue in the Hessian is very important and is called the condition number. The problem of the condition number being very large is called ill-conditioning. NN\u2019s are far more complex but we can look at small, local areas of the loss function, we can approximate the loss function as a quadratic form. This is done by using Taylor expansion: All its saying is that we are approximating a function using a quadratic form. Large portion of problems in DL can be attributed to ill-conditioning, some parameters have huge curvature while some have smaller curvature. To mitigate we can normalize the inputs (mean of the inputs makes the largest eigenvalue even larger, thus normalizing reduces the largest curvature) and use batch norm. One other way is to instead of changing the Hessian itself, momentum is a way of reducing step size in directions with larger curvature and increasing it in directions of smaller curvature. When the parameters go back and forth, the curvature is high, so momentum automatically reduces the learning rate for that direction.')};var v=function(){return i.a.createElement("div",{className:"container"},i.a.createElement(s.a,null,i.a.createElement(m,null),i.a.createElement("div",{className:"App"},i.a.createElement("div",null,i.a.createElement(h.a,{exact:!0,path:"/zine-blog",component:u}),i.a.createElement(h.a,{path:"/zine-blog/topics",component:d}),i.a.createElement(h.a,{path:"/cnns",component:l}),i.a.createElement(h.a,{path:"/limitations",component:c}),i.a.createElement(h.a,{path:"/sgd",component:f}),i.a.createElement(h.a,{path:"/batch_norm",component:p}),i.a.createElement(h.a,{path:"/linear_algebra",component:g})))))};Boolean("localhost"===window.location.hostname||"[::1]"===window.location.hostname||window.location.hostname.match(/^127(?:\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/));r.a.render(i.a.createElement(v,null),document.getElementById("root")),"serviceWorker"in navigator&&navigator.serviceWorker.ready.then((function(e){e.unregister()}))}},[[17,1,2]]]);
//# sourceMappingURL=main.fd758582.chunk.js.map