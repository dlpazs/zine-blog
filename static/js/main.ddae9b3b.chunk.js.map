{"version":3,"sources":["components/common/Header.js","components/articles/ArticleData.js","components/articles/react/react_components/Components_Props.js","components/articles/react/react_components/State_Lifecycle.js","components/articles/react/react.js","components/topics/Topic.js","components/topics/Topics.js","components/home/Home.js","components/about/About.js","components/common/PageNotFound.js","components/courses/CoursesPage.js","components/reducers/counter.js","components/reducers/index.js","App.js","serviceWorker.js","index.js"],"names":["Header","activeStyle","color","class","to","exact","Articles","tag","name","description","content","React","Component","Topic","location","state","parse","Topics","match","className","map","item","id","key","pathname","url","path","component","Home","About","PageNotFound","CoursesPage","counterReducer","action","type","rootReducer","combineReducers","counter","store","createStore","App","Boolean","window","hostname","ReactDOM","render","document","getElementById","navigator","serviceWorker","ready","then","registration","unregister"],"mappings":"mQA2BeA,EAxBA,WACb,IAAMC,EAAc,CAAEC,MAAO,WAE7B,OACE,yBAAKC,MAAM,iDACT,kBAAC,IAAD,CAASC,GAAG,aAAaH,YAAaA,EAAaI,OAAK,GAAxD,QAGC,MACD,kBAAC,IAAD,CAASD,GAAG,mBAAmBH,YAAaA,GAA5C,SAGC,MACD,kBAAC,IAAD,CAASG,GAAG,oBAAoBH,YAAaA,GAA7C,YAGC,MACD,kBAAC,IAAD,CAASG,GAAG,qBAAqBH,YAAaA,GAA9C,aCpBOK,EAAW,CACtB,CACEC,IAAK,MACLC,KAAM,kCACNC,YAAY,yLAGZC,QAAQ,ikKA2FV,CACEH,IAAK,cACLC,KAAM,oBACNC,YAAa,wDACbC,QAAQ,y0RAyHV,CACEH,IAAK,WACLC,KAAM,gBACNC,YAAa,iDACbC,QAAQ,4yXA4KV,CACEH,IAAK,GACLC,KAAM,GACNC,YAAa,GACbC,QAAQ,IAEV,CACEH,IAAK,GACLC,KAAM,GACNC,YAAa,GACbC,QAAQ,IAEV,CACEH,IAAK,GACLC,KAAM,GACNC,YAAa,GACbC,QAAQ,K,4CC1XUC,IAAMC,UCsGRD,IAAMC,UCrI1B,I,iBCOeC,EAVD,SAAC,GAAD,IAAGC,EAAH,EAAGA,SAAH,OACZ,6BACE,4BAAKA,EAASC,MAAMP,MACpB,4BAAKM,EAASC,MAAMN,aACpB,6BACE,2BAAIO,IAAMF,EAASC,MAAML,aCwBhBO,EAzBA,SAAC,GAAD,IAAGC,EAAH,EAAGA,MAAH,OACb,6BACE,yBAAKC,UAAU,aACb,uCAGF,6BACGb,EAASc,KAAI,SAACC,EAAMC,GAAP,OACZ,yBAAKC,IAAKD,GACR,kBAAC,IAAD,CACElB,GAAI,CACFoB,SAAS,GAAD,OAAKN,EAAMO,IAAX,YAAkBJ,EAAKd,KAC/BQ,MAAOM,IAGRA,EAAKb,WAKd,6BACA,kBAAC,IAAD,CAAOkB,KAAI,yBAA4BC,UAAWd,MCXvCe,EAdF,kBACX,6BACE,oCACA,2UAOA,gCCXW,SAASC,IACtB,OACE,6BACE,sCCHN,IACeC,EADM,kBAAM,sDCANC,E,iLAEjB,OACE,6BACE,4C,GAJiCnB,a,gBCS1BoB,EAXQ,WAAwB,IAAvBjB,EAAsB,uDAAd,EAAGkB,EAAW,uCAC5C,OAAQA,EAAOC,MACb,IAAK,YACH,OAAOnB,EAAQ,EACjB,IAAK,YACH,OAAOA,EAAQ,EACjB,QACE,OAAOA,ICDEoB,EAHKC,YAAgB,CAClCC,QAASL,ICYLM,EAAQC,YAAYJ,GAuBXK,MArBf,WACE,OACE,kBAAC,IAAD,CAAUF,MAAOA,GACf,yBAAKnB,UAAU,mBACb,kBAAC,EAAD,MACA,yBAAKA,UAAU,OACb,6BACE,kBAAC,IAAD,KACE,kBAAC,IAAD,CAAOd,OAAK,EAACqB,KAAK,aAAaC,UAAWC,IAC1C,kBAAC,IAAD,CAAOvB,OAAK,EAACqB,KAAK,mBAAmBC,UAAWE,IAChD,kBAAC,IAAD,CAAOH,KAAK,oBAAoBC,UAAWV,IAC3C,kBAAC,IAAD,CAAOS,KAAK,qBAAqBC,UAAWI,IAC5C,kBAAC,IAAD,CAAOJ,UAAWG,UClBZW,QACW,cAA7BC,OAAO5B,SAAS6B,UAEe,UAA7BD,OAAO5B,SAAS6B,UAEhBD,OAAO5B,SAAS6B,SAASzB,MACvB,2DCXN0B,IAASC,OACP,kBAAC,IAAD,KACE,kBAAC,EAAD,OAEFC,SAASC,eAAe,SDsHpB,kBAAmBC,WACrBA,UAAUC,cAAcC,MAAMC,MAAK,SAAAC,GACjCA,EAAaC,kB","file":"static/js/main.ddae9b3b.chunk.js","sourcesContent":["import React from \"react\";\r\nimport { NavLink } from \"react-router-dom\";\r\n\r\nconst Header = () => {\r\n  const activeStyle = { color: \"#F15B2A\" };\r\n\r\n  return (\r\n    <nav class=\"navbar navbar-expand-lg navbar-light bg-light\">\r\n      <NavLink to=\"/zine-blog\" activeStyle={activeStyle} exact>\r\n        Home\r\n      </NavLink>\r\n      {\" | \"}\r\n      <NavLink to=\"/zine-blog/about\" activeStyle={activeStyle}>\r\n        About\r\n      </NavLink>\r\n      {\" | \"}\r\n      <NavLink to=\"/zine-blog/topics\" activeStyle={activeStyle}>\r\n        Articles\r\n      </NavLink>\r\n      {\" | \"}\r\n      <NavLink to=\"/zine-blog/courses\" activeStyle={activeStyle}>\r\n        Courses\r\n      </NavLink>\r\n    </nav>\r\n  );\r\n};\r\n\r\nexport default Header;\r\n","export const Articles = [\r\n  {\r\n    tag: \"cnn\",\r\n    name: \"Everybody loves Convolutions...\",\r\n    description: `The focus here is on convolutional neural networks,\r\n    the convolution operation and how it relates to neural nets, \r\n    seeing convolution as just matrix multiplication and more!`,\r\n    content: `<div className=\"container\">\r\n    <div\r\n      className=\"row\"\r\n      style=\"margin: 5vh\"\r\n    >\r\n      <div className=\"col\">\r\n        <h1>Everybody loves ... convolutions (Work in progress)</h1>\r\n        <div\r\n        style=\"background-color: #f2f2f2;\r\n            padding: 10px;\r\n            margin: 10px;\r\n            border-radius: 5px\"\r\n        >\r\n          In 3D, the following mental image may prove useful. Imagine two sheets\r\n          of colored paper: one red and one blue. Put one on top of the other.\r\n          Now crumple them together into a small ball. That crumpled paper ball\r\n          is your input data, and each sheet of paper is a class of data in a\r\n          classification problem. What a neural network (or any other\r\n          machine-learning model) is meant to do is figure out a transformation\r\n          of the paper ball that would uncrumple it, so as to make the two\r\n          classes cleanly separable again. With deep learning, this would be\r\n          implemented as a series of simple transformations of the 3D space,\r\n          such as those you could apply on the paper ball with your fingers, one\r\n          movement at a time.  Uncrumpling paper balls is what machine learning\r\n          is about: finding neat representations for complex, highly folded data\r\n          manifolds. At this point, you should have a pretty good intuition as\r\n          to why deep learning excels at this: it takes the approach of\r\n          incrementally decomposing a complicated geometric transformation into\r\n          a long chain of elementary ones, which is pretty much the strategy a\r\n          human would follow to uncrumple a paper ball. Each layer in a deep\r\n          network applies a transformation that disentangles the data a\r\n          little - and a deep stack of layers makes tractable an extremely\r\n          complicated disentanglement process. Conv layers look at spatially\r\n          local patterns by applying the same geometric transformation to\r\n          different spatial locations (patches) in an input tensor. This results\r\n          in a representation that are translation invariant (does not matter\r\n          where in the image the object occurs). Convnets consist of stacks of\r\n          convolution and max-pooling layers. The pooling layers let you\r\n          spatially downsample the data, which is required to keep feature maps\r\n          to a reasonable size as the number of features grows, and to allow\r\n          subsequent convolution layers to see a greater spatial extent of the\r\n          inputs. Convnets are often ended with a Flatten operation or global\r\n          pooling layer, turning spatial feature maps into vectors, followed by\r\n          Dense layers to achieve classification or regression.\r\n          <br></br>\r\n          <br></br>\r\n          <b>Deep Learning with Python page 44–5.</b>\r\n        </div>\r\n\r\n        <img\r\n          src=\"https://i.stack.imgur.com/iY5n5.png\"\r\n          style=\"width: 90%\">\r\n\r\n        <h2>What is a convolution?</h2>\r\n        <p>\r\n          It is a sliding window applied to a matrix, whether that be an image\r\n          or feature map. At each element we multiply the filter/kernel with the\r\n          corresponding input pixel and then add them up. The same filter/kernel\r\n          is slide across the input which is called weight tying. Filters of the\r\n          convolutions are learned, so the model learns which filter is best to\r\n          apply in each layer. An early layer of a convolution is likely, at\r\n          best, to only detect very low level features such as edges. This is\r\n          because the filters are limited at earlier layers, whereas as we go\r\n          deeper, the amount of output channels (which we set) increases and\r\n          later layers have several channels with which to predict collections\r\n          of edges and orientations that make up a face or eye ball.\r\n        </p>\r\n\r\n        <h2>Properties</h2>\r\n        <p>\r\n          Location Invariance and Compositionality. Say you want to spot a cat\r\n          in an image. Because of this sliding window approach you don't care\r\n          where the cat occurs. The second aspect is (local) compositionality.\r\n          Each filter composes a local patch of lower-level features into\r\n          higher-level representation. You build edges from pixels, shapes from\r\n          edges, eyes from circles and so on. You can derive more complex shapes\r\n          and objects from lower level representations such as edges, pixels,\r\n          gradients etc and from several channels of these features.\r\n          Convolutions are translationally invariant because the filters slide\r\n          over the image horizontally and vertically. But they are not\r\n          rotationally invariant because the filters don't rotate. Thus, the net\r\n          seems to need several similar filters in different orientations to\r\n          detect objects and patterns that are differently oriented.\r\n        </p>\r\n        <img\r\n          src=\"https://cdn-images-1.medium.com/max/1200/1*B41mvbzpZ7ythn5AlJWh-A.gif\"\r\n          style=\"width: 90%\">\r\n      </div>\r\n    </div>\r\n  </div>`\r\n  },\r\n  {\r\n    tag: \"limitations\",\r\n    name: \"Limitations of DL\",\r\n    description: \"Focus is on the limitations of current deep learning.\",\r\n    content: `\r\n    <div>\r\n      <p>\r\n      <h4>\r\n      <a href=\"http://faculty.neu.edu.cn/yury/AAI/Textbook/Deep%20Learning%20with%20Python.pdf\">\r\n          Heavily Inspired by this brilliant book</a>\r\n      </h4>\r\n  \r\n  Deep learning works by doing a series of geometric transformations on an input. And then, \r\n  given some error signal, improves the geometric transformations iteratively to map some \r\n  input x to a correct output y. \r\n  \r\n  \"In deep learning, everything is a vector: everything is a point in a geometric space. \r\n  Model inputs and targets are first vectorized: turned into an initial input vector space \r\n  and target vector space. Each layer operates one simple geometric transformation on the \r\n  data that goes through it. The chain of layers forms one complex geometric trasnformation, \r\n  broken down into a series of simpler ones. This complex transformation attempts to map the \r\n  input space to the target space, one point at a time. This transformation is parameterized \r\n  by weights of the layers, which are iteratively updated based on how well the model is \r\n  currently performing. A key characteristic is that the geometric trasnformation must be \r\n  differentiable, to be able to perform gradient descent. Thi means the geometric morphing \r\n  from inputs to outputs must be smooth and continuous - a significant constraint.\"\r\n  \r\n  \"The entire process of applying this complex geometric transformation to the input data can be \r\n  visualized in 3D by imagining a person trying to uncrumple a paper ball: the crumpled paper ball \r\n  is the manifold of the input data that the model starts with.\r\n  Each movement operated by the person on the paper ball is similar to a simple geometric \r\n  transformation operated by one layer. The full uncrumpling gesture sequence is the complex \r\n  transformation of the entire model. Deep-learning models are mathematical machines for uncrumpling \r\n  complicated manifolds of high-dimensional data.\r\n  \r\n  That’s the magic of deep learning: turning meaning into vectors, into geometric spaces, and \r\n  then incrementally learning complex geometric transformations that map one space to another. \r\n  All you need are spaces of sufficiently high dimensionality in order to capture the full scope \r\n  of the relationships found in the original data.\"\r\n  \r\n  \"The whole thing hinges on a single core idea: that meaning is derived from the pairwise \r\n  relationship between things (between words in a language, between pixels in an image) and \r\n  that these relationships can be captured by a distance function. If the brain works in \r\n  geometric spaces is a different question. Vector spaces are efficient to work with from a \r\n  computational standpoint, but different data structures can be envisioned- like graphs. \r\n  Neural networks initially emerged from using graphs (connectionism) but nowadays they have \r\n  an incorrect meaning since they are neither neural nor networks. A more appropriate name is \r\n  layered representations learning or hierarchical representations learning, or deep differentiable \r\n  models or chained geometric transforms.\"\r\n  \r\n  \"The space of applications is nearly infinite but many are completely out of reach for current \r\n  deep learning techniques. You cannot train a model to read a product description and generate \r\n  the appropriate codebase. Anything that requires reasoning, long-term planning, and algorithmic \r\n  data manipulation is out of reach for deep-learning models no matter how much data is given. \r\n  \r\n  This is because a deep-learning model is just a chain of simple, continuous geometric \r\n  transformation mapping one vector space into another. All it can do is map one data manifold X \r\n  into another manifold Y, assuming the existence of a learnable continuous transform from X to Y.\"\r\n  \r\n  \"One real risk is overestimating/ anthropomorphizing deep-learning models and their abilities. \r\n  A fundamental feature of humans is our theory of mind: our tendency to project intentions, beliefs, \r\n  and knowledge on the things around us. Drawing a smiley face on a rock suddenly makes it happy-in \r\n  our minds. Applied to deep learning, this means that, for instance, when we're able to train a \r\n  model to generate captions to describe pictures and the captions it generates. Then we're \r\n  surprised when any slight departure from the sort of images present in the training data \r\n  causes the model to generate completely absurd captions. \r\n  \r\n  This is highlighted by adversarial examples, which are samples ged to a deep-learning network \r\n  that are designed to trick the model into misclassifying them. It's possible to do gradient \r\n  ascent in input space to generate inputs that maximize the activation of some convnet filter-this \r\n  is the basis of the filter-visualization technique. Similarly, through gradient ascent you can \r\n  slightly modify an image to maximize the class prediction for a given class. By taking a \r\n  picture of a panda and adding to it a gibbon gradient, we can get a neural network to classify \r\n  the panda as a gibbon. This evidences both the brittleness of these models and the deep \r\n  difference between their input-to-output mapping and our human perception.\"\r\n  \r\n  <img src=\"https://blog.keras.io/img/limitations-of-dl/adversarial_example.png\" />\r\n  \"In short, deepl-learning models don't have any understanding of their input-not in the human \r\n  sense. Our understanding of images, sounds etc is grounded in our sensimotor experience as humans. \r\n  ML models have no access to such experiences and thus can't understand their inputs in a human \r\n  relatable way. By annotating large numbers of training examples to feed to our models, we get \r\n  them to learn a geometric transform that maps data to human concepts on a specific set of examples, \r\n  but this mapping is a simplistic sketch of the original model in our minds-the one developed from \r\n  our experience as embodied agents.\r\n  \r\n  Never fall into the trap of believing that neural networks understand the task they perform. \r\n  They were trained on a different, far narrower task than the one we wanted to teach them: that of \r\n  mapping training inputs to training targets, point by point. Show them anything that deviates from \r\n  their training data, and they will break.\"\r\n  \r\n  \"Humans are capable of far more than mapping immediate stimuli to immediate\r\n  responses, as a deep network, or maybe an insect, would. We maintain complex, abstract\r\n  models of our current situation, of ourselves, and of other people, and can use these\r\n  models to anticipate different possible futures and perform long-term planning. We\r\n  can merge together known concepts to represent something we’ve never experienced \r\n  before-like picturing a horse wearing jeans, for instance. This ability to hypothecize, \r\n  to expand our mental model beyond what we can experience directly-to perform abstraction \r\n  and reasoning-is arguably the defining characterstic of human cognition. \r\n  Extreme generalization: an ability to adapt to novel, never-before experienced \r\n  situations using little data or no new data. This stands in sharp contrast with deep nets, \r\n  local generalization. The mapping from inputs to outputs performed by deep net quickly \r\n  stops making sense if new inputs deviate from what was seen during training. Consider, \r\n  for instance, the problem of learning the appropriate launch parameters to get a rocket \r\n  to land on the moon. If you used a deep net for this task and trained\r\n  it using supervised learning or reinforcement learning, you’d have to feed it thousands\r\n  or even millions of launch trials: you’d need to expose it to a dense sampling of the input\r\n  space, in order for it to learn a reliable mapping from input space to output space. In\r\n  contrast, as humans we can use our power of abstraction to come up with physical \r\n  models—rocket science—and derive an exact solution that will land the rocket on the moon\r\n  in one or a few trials. Similarly, if you developed a deep net controlling a human body,\r\n  and you wanted it to learn to safely navigate a city without getting hit by cars, the net\r\n  would have to die many thousands of times in various situations until it could infer that\r\n  cars are dangerous, and develop appropriate avoidance behaviors. Dropped into a new\r\n  city, the net would have to relearn most of what it knows. On the other hand, humans\r\n  are able to learn safe behaviors without having to die even once—again, thanks to our\r\n  power of abstract modeling of hypothetical situations.\"\r\n  \r\n  \"In short, despite our progress on machine perception, we’re still far from humanlevel AI. \r\n  Our models can only perform local generalization, adapting to new situations that must be \r\n  similar to past data, whereas human cognition is capable of extreme generalization, quickly \r\n  adapting to radically novel situations and planning\r\n  for long-term future situations.\"\r\n  </p>\r\n    </div>`\r\n  },\r\n  {\r\n    tag: \"research\",\r\n    name: \"Research Idea\",\r\n    description: \"Here I will talk on some of my research ideas.\",\r\n    content: `\r\n    <div>\r\n        <p>\r\n            The area I am most interested in is giving object detectors the ability to incorporate context\r\n            and relationships to other objects. \r\n\r\n            The challenge here is encoding meaning and semantics into inference of an object detector. \r\n            The use of object detectors becomes very limited in practical applications when un-equipped \r\n            with scene-level semantics. \r\n\r\n            <a href=\"https://arxiv.org/pdf/1807.00119.pdf\">Structure Inference Net [1]</a> aims to incorporate\r\n            two kinds of context from scene and from <b>object relationships.</b> \r\n\r\n            This proposal tries to infer some level of context from object relationships and scene-level context.\r\n            \"\r\n            Detection aims to answer what is where. From a structure perspective, it can be formulated as a \r\n            reasoning problem of a graph involving the mutually complementary information of scene, objects and\r\n            relationships.\r\n            \"\r\n            <img src=\"https://d3i71xaburhd42.cloudfront.net/8633ebfe1bd7f6a95233025c872e4a2cf660f54c/2-Figure2-1.png\"/>\r\n            \"\r\n            Sequentially object detection is regarded as not only a cognition problem, but also an inference\r\n            problem which is based on contextual information with object fine-grained details. Objects are nodes\r\n            of the graph and object relationships are edges. These objects interact with each other via the \r\n            graph under the guidance of scene context. \r\n\r\n            Structure Inference network (SIN) to reason object state in a graph, where memory cell is the key\r\n            module to encode different kinds of messages (e.g. from scene and other objects) into object state,\r\n            and a novel way of using GRUs as the memory cell is presented in this work. Specifically, we fix object\r\n            representation as the inital state of GRU and then input each kind of message to achieve the goal of\r\n            updating object state. Since SIN can accomplish inference as long as the inputs to it covers the \r\n            representations of object, scene-level context and instance-level relationship, our structure inference\r\n            method is not constrained to specific detection framework.\r\n\r\n\r\n            \"\r\n\r\n            Two-stage detectors: (F-RCNN, Faster-RCNN) first stage produces numbers of condidate boxes, and then \r\n            the second stage classifies these boxes into foreground classes or background. R-CNN extracts CNN features\r\n            from candidate regions and applies linear SVMs as classifier. Faster-RCNN proposes novel ROI-pooling\r\n            operation to extract feature vectors for each candidate box from shared convolutional feature map. \r\n\r\n            \"\r\n            For example, Mottaghi et al. [29] propose a deformable part-based\r\n            model, which exploits both local context around each candidate detection and \r\n            global context at the level of the scene.\r\n            The presence of objects in irrelevant scenes is penalized in\r\n            [41]. Recently, some works [3, 43, 37] based on deep ConvNet have made some \r\n            attempts to incorporate contextual information to object detection. Contextual information outside the region of interest is integrated using spatial recurrent neural network in ION [3]. GBD-Net [43] proposes\r\n            a novel gated bi-directional CNN to pass message between\r\n            features of different support regions around objects. Shrivastava et al. [37] \r\n            use segmentation to provide top-down\r\n            context to guide region proposal generation and object detection. While context \r\n            around object or scene-level context\r\n            has been addressed in such works [3, 43, 37] under the deep\r\n            learning-based pipeline, they make less progress in exploring object-object \r\n            relationships. On the contrary, a much recent work [4] proposes a new sequential \r\n            reasoning architecture that mainly exploits object-object relationships to \r\n            sequentially detect objects in an image, however, with only\r\n            implicit yet weak consideration of scene-level context. Different from these \r\n            existing works, our proposed structure inference network has the capability \r\n            of jointly modeling both\r\n            scene-level context and object-object relationships and inferring different \r\n            object instances within an image from a\r\n            structural and global perspective.\r\n            Structure inference. Several interesting works [28, 34,\r\n            23, 39, 21, 9, 2, 22, 42] have been proposed to combine\r\n            deep networks with graphical models for structured prediction tasks that are solved by structure inference techniques.\r\n            A generic structured model is designed to leverage diverse\r\n            label relations including scene, object and attributes to improve image classification performance in [21]. Deng et al.\r\n            [9] propose structure inference machines for analyzing relations in group activity recognition. Structural-RNN [22]\r\n            combines the power of high-level spatio-temporal graphs\r\n            and sequence learning, and evaluates the model ranging\r\n            from motion to object interactions. In [42], a graph inference model is proposed to tackle the task of generating\r\n            structured scene graph from an image. While our work\r\n            shares similar spirit as [42] to formulate the object detection task as a graph structure inference problem, the two\r\n            works have essential differences in their technical sides,\r\n            such as the graph instantiation manners, inference mechanisms, message passing schemes, etc, which highly depend\r\n            on the specific task domains.\r\n\r\n            \r\n\r\n            <img src=\"https://d3i71xaburhd42.cloudfront.net/8633ebfe1bd7f6a95233025c872e4a2cf660f54c/3-Figure3-1.png\" />\r\n            Figure 3. SIN: The Framework of Our Method. Firstly we get a fixed number of ROIs from an input image. Each ROI is pooled into a\r\n            fixed-size feature map and then mapped to a feature vector by a fully connected layer as node. We extract the whole image feature as scene\r\n            in the same way, and then we concatenate the descriptors of every two ROIs into edges. To iteratively update the node state, an elaborately\r\n            designed structure inference method is triggered, and the final state of each node is used to predict the category and refine the location of\r\n            the corresponding ROI. The whole framework is trained end-to-end with the original multi-task loss (this study exploits Faster R-CNN as\r\n            the base detection framework).\r\n            \"\r\n\r\n            <a href=\"https://arxiv.org/pdf/1807.05857.pdf\">Object Relation Detection Based on One-shot Learning</a>\r\n            Object relation is abstract representation of the visually\r\n            observable interactions between a pair of a subject and an\r\n            object, such as “person play piano”. Detecting object relations in images is one \r\n            crucial task in image understanding. Each object relation involves a pair of \r\n            localized objects\r\n            (subject and object) which are connected via a predicate. A\r\n            predicate can be action (e.g. “play”), a spatial preposition\r\n            (e.g. “on”) or some comparative expression (e.g. “taller\r\n            than”). While objects are the basic constituent elements of\r\n            an image, it is often the relations between objects that provide the holistic \r\n            interpretation of a scene.\r\n            <img src=\"https://www.researchgate.net/profile/Jian_Zhao68/publication/326437189/figure/fig1/AS:649228653191178@1531799581405/Here-are-two-examples-from-the-Visual-Relationship-Dataset-15-Object-relations-widely_Q320.jpg\"/>\r\n            Meanwhile, each relation involves two parts, namely subject and object, resulting in a greater skew of rare relations especially when\r\n            co-occurrence of some pairs of objects is infrequent in the\r\n            dataset. Some types of relations contain very limited examples. \r\n            We call this phenomenon the <b>long-tail problem</b>. The\r\n            second reason is the large intra-class divergence. Relations that \r\n            have the same predicate but different subjects or\r\n            objects are essentially different. \r\n\r\n            The third reason is the semantic dependency that the predicate in a \r\n            relation is not only determined\r\n            by semantic information but also the categories of the subject and the object.\r\n\r\n            In essence, object relation detection can be regarded\r\n            as a classification task. Then the fundamental problem is\r\n            how to formulate the relation triplet (subject, predicate, object) to \r\n            define a reasonable classification task.\r\n\r\n            Without considering semantics dependencies, learning an object detection\r\n            model would be challenged by two difficulties. The first\r\n            difficulty is due to the intra-class divergence problem we\r\n            discuss above. A relation may contain different semantics\r\n            that would confuse the model. Second, the ignorance of\r\n            semantics dependencies consequently results in the lack of\r\n            attention of the model to visual feature regions at the high\r\n            level. However, in the human visual system, one can concentrate his attention volitionally due to top-down signals\r\n            guided by the current task (e.g., attempting to determine the\r\n            relation between objects), and automatically due to bottomup signals caused by salient or eye-catching stimuli.\r\n            \r\n            According to the above observations, we propose to learn\r\n            models for objects and predicates respectively and develop a\r\n            framework which combines both bottom-up and top-down\r\n            attention mechanisms. We refer to the mechanism related\r\n            to semantics dependencies as the top-down mechanism\r\n            and the one purely related to visual representations as the\r\n            bottom-up mechanism. The bottom-up mechanism generates a set of object proposals with category information,\r\n            then visual features are extracted from these proposals by\r\n            a Convolution Neural Network [19]. Practically, we implement bottom-up attention utilizing Faster R-CNN [16].\r\n            The top-down mechanism exploits semantics dependencies\r\n            (the categories of subjects and objects) to predict an attention distribution over the visual features from the bottom-up\r\n            mechanism.\r\n\r\n            Besides the dual attention mechanisms, the learning to\r\n            learn [5] module—Semantics Induced Learner (SIL)—is\r\n            the core component of our framework. SIL is able to\r\n            learn to fast adapt the predicate classification model conditioned on the semantics dependencies inferred from the\r\n            categories of subjects and objects, and therefore effectively\r\n            improves performance of the predicate classification model.\r\n            The proposed SIL incorporates semantics dependencies into\r\n            the predicate classification model in a novel and effective\r\n            way, then dynamically determines, in one-shot manner, the\r\n            weightings of visual features generated by the bottom-up\r\n            mechanism, which is challenging to accomplish for a purely\r\n            static predicate classification model.\r\n\r\n            \r\n\r\n            <a href=\"\"></a>\r\n            <a href=\"\"></a>\r\n            <a href=\"\"></a>\r\n            <a href=\"\"></a>\r\n            <a href=\"\"></a>\r\n            <a href=\"\"></a>\r\n            <a href=\"\"></a>\r\n\r\n        </p>\r\n    </div>\r\n)`\r\n  },\r\n  {\r\n    tag: \"\",\r\n    name: \"\",\r\n    description: \"\",\r\n    content: ``\r\n  },\r\n  {\r\n    tag: \"\",\r\n    name: \"\",\r\n    description: \"\",\r\n    content: ``\r\n  },\r\n  {\r\n    tag: \"\",\r\n    name: \"\",\r\n    description: \"\",\r\n    content: ``\r\n  }\r\n];\r\n","import React from 'react'\r\nimport ScriptSection from \"./Script_Section\"\r\n\r\nconst scriptArray = [\r\n    `\r\n    function Welcomer(props){\r\n        return <h1> Hello, {props.name}</h1>\r\n    }\r\n    `,\r\n    `\r\n    class Welcome extends React.Component{\r\n        render(){\r\n            return <h1>Hello, {this.props.name}\r\n        }\r\n    }\r\n    `,\r\n    `\r\n    function App(){\r\n        <div>\r\n            <Welcome name={\"Joe Bloggs\"}/>\r\n            <Welcome name={\"Jim Joe\"}/>\r\n            <Welcome name={\"Eric Jeff\"}/>\r\n        </div>\r\n    }\r\n    `,\r\n    `\r\n    function sum(a,b){ return a + b }\r\n    `,\r\n    `\r\n    function withdraw(account, amount){ account.total -= amount }\r\n    `\r\n]\r\n\r\nfunction Welcomer(props){\r\n    return <h3>Hello, {props.name}</h3>\r\n}\r\n\r\nclass Welcome extends React.Component{\r\n    \r\n    render(){\r\n        return (\r\n            <h3>Hello, {this.props.name}</h3>\r\n        )\r\n    }\r\n}\r\n\r\nfunction pApp(){\r\n    return (\r\n        <div>\r\n            <Welcome name={\"Joe Bloggs\"}/>\r\n            <Welcome name={\"Jim Joe\"}/>\r\n            <Welcome name={\"Eric Jeff\"}/>\r\n        </div>\r\n    )\r\n}\r\n\r\nconst data = {\r\n    name: \"You\"\r\n}\r\n\r\nexport default function Components_Props() {\r\n    return (\r\n        <div>\r\n            <h2>Components and Props</h2>\r\n            <p>\r\n                Components allow you to split the UI into independent, reusable pieces in isolation. \r\n                Components are like JS functions, they accept inputs called props and return React \r\n                elements describing what should appear on screen.\r\n            </p>\r\n            <p>\r\n                The simplest way to define a component is to write a JS function:\r\n                {ScriptSection(0, scriptArray)}\r\n                {Welcomer(data)}\r\n                This is a valid React component since it accepts a single props object argument with data \r\n                and returns a React element. This is a function component. \r\n                You can also use ES6 class to define a component:\r\n                {ScriptSection(1, scriptArray)}\r\n                <Welcome name={data.name}/>\r\n                React elements can also represent user-defined components. When React sees an element representing\r\n                a user-defined component, it passes JSX attributes to this component as a single object.\r\n                we call this object props.\r\n            </p>\r\n            <p>\r\n                Components can refer to other components in their output. This lets us use the same component\r\n                abstraction for any level of detail. A button, a form, a dialog: in React apps, all those are \r\n                commonly expressed as components. For instance we can create an App component that renders \r\n                Welcome many times:\r\n                {ScriptSection(2, scriptArray)}\r\n                {pApp()}\r\n                Typically React apps have a single App component at the very top. \r\n            </p>\r\n\r\n            <p>\r\n                It is good to split components into smaller components. This allows you to have \r\n                a lot of reusable components which pays off in larger apps. A good mantra to have is \r\n                that if a part of your UI is used several times (Button, Panel, Avatar), or is complex enough\r\n                on its own (App, FeedStory, Comment) it is a good candidate to be a reusable component.\r\n            </p>\r\n\r\n            <p>\r\n                A component must never modify its own props. \r\n                {ScriptSection(3, scriptArray)}\r\n                Such functions are called `pure` functions because they do not attempt to change their \r\n                inputs and always return the same result for the same inputs. In contrast:\r\n                {ScriptSection(4, scriptArray)}\r\n                React is flexible but it has a single strict rule:\r\n                <br/>\r\n                <b>All React components must act like pure functions with respect to their props.</b>\r\n                <br/>\r\n                React apps do change over time but we use state which allows React components to change their\r\n                output over time in response to user actions, etc. without violating this rule.\r\n            </p>\r\n\r\n            <p>\r\n                <a href=\"https://www.robinwieruch.de/react-pass-props-to-component\">Pass props to components</a>\r\n            </p>\r\n        </div>\r\n    )\r\n}\r\n","import React from 'react'\r\nimport ScriptSection from \"./Script_Section\";\r\n\r\nconst scriptArray = [\r\n    `\r\n    function Clock(props){\r\n        return (\r\n            <div>\r\n                <h4>Hello</h4>\r\n                <h4>It is {props.date.toLocaleTimeString()}</h4>\r\n            </div>\r\n        )\r\n    }\r\n    `,\r\n    `\r\n    function tick(){\r\n        ReactDOM.render(\r\n            <Clock date={new Date()}/>,\r\n            document.getElementById('root')\r\n        )\r\n    }\r\n    `,\r\n    `\r\n    class Clock extends React.Component {\r\n        render() {\r\n            return (\r\n                <div>\r\n                    <h4>Hello</h4>\r\n                    <h4>It is {this.props.date.toLocaleTimeString()}</h4>\r\n                </div>\r\n            )\r\n        }\r\n    }\r\n    `,\r\n    `\r\n    <h4>It is {this.state.date.toLocaleTimeString()}</h4>\r\n    `,\r\n    `\r\n    this.state = {date: new Date()}\r\n    `,\r\n    `\r\n    constructor(props){\r\n        super(props)\r\n    }\r\n    `,\r\n    `\r\n    <Clock />\r\n    `,\r\n    `\r\n    class Clock extends React.Component {\r\n        constructor(props){\r\n            super(props)\r\n            this.state = {date: new Date()}\r\n        }\r\n        \r\n        render() {\r\n            return (\r\n                <div>\r\n                    <h4>Hello</h4>\r\n                    <h4>It is {this.state.date.toLocaleTimeString()}</h4>\r\n                </div>\r\n            )\r\n        }\r\n    }\r\n    `,\r\n    `\r\n    componentDidMount(){\r\n\r\n    }\r\n    `,\r\n    `\r\n    componentWillUnmount(){\r\n\r\n    }\r\n    `,\r\n    `\r\n    componentDidMount(){\r\n        this.timerID = setInterval(\r\n            () => this.tick(),\r\n            1000\r\n        )\r\n    }\r\n    `,\r\n    `\r\n    componentWillUnmount(){\r\n        clearInterval(this.timerID)\r\n    }\r\n    `,\r\n    `\r\n    tick(){\r\n        this.setState({\r\n            date: new Date()\r\n        })\r\n    }\r\n    `,\r\n    `\r\n    this.state.comment = \"Hi\"\r\n    `,\r\n    `\r\n    this.setState({comment: \"Hi\"})\r\n    `,\r\n    `\r\n    this.setState({\r\n        counter: this.state.counter + this.props.increment\r\n    })\r\n    `,\r\n    `\r\n    this.setState(function(state, props){\r\n        return {\r\n            counter: state.counter + props.increment\r\n        }\r\n    })\r\n    `,\r\n    `\r\n    componentDidMount(){\r\n        doSomething().then(res => {\r\n            this.setState({\r\n                blogs: res.blogs\r\n            })\r\n        })\r\n\r\n        fetchSomething().then(res => {\r\n            this.setState({\r\n                comments: res.comments\r\n            })\r\n        })\r\n    }\r\n    `,\r\n    `\r\n    <FormattedDate date={this.state.date} />\r\n    `,\r\n    `\r\n    function FormattedDate(props){\r\n        return <h2>It is {props.date.toLocaleTimeString()}</h2>\r\n    }\r\n    `\r\n]\r\n\r\n\r\nclass Clock extends React.Component {\r\n    constructor(props){\r\n        super(props)\r\n        this.state = {date: new Date()}\r\n    }\r\n\r\n    componentDidMount(){\r\n        this.timerID = setInterval(\r\n            () => this.tick(),\r\n            1000\r\n        )\r\n    }\r\n\r\n    componentWillUnmount(){\r\n        clearInterval(this.timerID)\r\n    }\r\n\r\n    tick(){\r\n        this.setState({\r\n            date: new Date()\r\n        })\r\n    }\r\n\r\n    render() {\r\n        return (\r\n            <div>\r\n                <h4>Hello</h4>\r\n                <h4>It is {this.state.date.toLocaleTimeString()}</h4>\r\n            </div>\r\n        )\r\n    }\r\n}\r\n\r\n\r\nexport default function StateLifecycle() {\r\n    return (\r\n        <div>\r\n            <h2>State and Lifecycle</h2>\r\n            <p>\r\n                Consider the ticking clock example. Here we will learn how to make it reusable and encapsulated.\r\n                We firstly encapsulate how the clock looks:\r\n                {ScriptSection(0, scriptArray)}\r\n                {ScriptSection(1, scriptArray)}\r\n                The fact that the clock sets up a timer and updates the UI every second should be an implementation\r\n                details of the clock. Ideally we write this once and have the clock update itself:\r\n                To do this we add state.\r\n            </p>\r\n            <p>\r\n                You can convert a function component like Clock to a class in five steps:\r\n                <ol>\r\n                    <li> Create an ES6 class, with the same name that extends React.Component</li>\r\n                    <li> Add a single empty method to it called render()</li>\r\n                    <li> Move the body of the function into the render() method</li>\r\n                    <li> Replace props with this.props in the render() body</li>\r\n                    <li> Delete remaining empty function declaration.</li>\r\n                </ol>\r\n                {ScriptSection(2, scriptArray)}\r\n                Now Clock is a class instead of a function. \r\n            </p>\r\n            <p>\r\n                We will now move the date from props to state in three steps:\r\n                <ol>\r\n                    <li>Replace this.props.date with this.state.date in render() method:</li>\r\n                    {ScriptSection(3, scriptArray)}\r\n                    <li>Add a class constructor that assigns the inital this.state:</li>\r\n                    {ScriptSection(4, scriptArray)}\r\n                    Note how we pass props to base constructor\r\n                    {ScriptSection(5, scriptArray)}\r\n                    <li>Class components should always call the base constructor with props. Remove\r\n                        the data prop from Clock element:\r\n                    </li>\r\n                    {ScriptSection(6, scriptArray)}\r\n                    {ScriptSection(7, scriptArray)}\r\n                    <Clock />\r\n                    Now we will make the Clock setup its own timer and update itself every second.\r\n                </ol>\r\n            </p>\r\n            <p>\r\n                In apps with many components its important to free up resources taken by the components\r\n                when they are destroyed. We want to set up a timer whenever the Clock is rendered to the \r\n                DOM for the first time. This is called <b>mounting</b> in React. We also want to clear\r\n                that timer whenver the DOM produced by the Clock is removed. This is called <b>unmounting</b>.\r\n                {ScriptSection(8, scriptArray)}\r\n                {ScriptSection(9, scriptArray)}\r\n                These methods are called <b>lifecycle methods</b>. The componentDidMount method runs\r\n                after the component output has been rendered to the DOM. \r\n                {ScriptSection(10, scriptArray)}\r\n                You can add this.anything you won't if you need to store something.\r\n                {ScriptSection(11, scriptArray)}\r\n                Finally, lets implement the tick method which uses this.setState() to schedule updates\r\n                to the components local state:\r\n                {ScriptSection(12, scriptArray)}\r\n                Now the clock ticks every second.\r\n                <Clock />\r\n            </p>\r\n            <p>\r\n                There are 3 things to know about setState()\r\n                <br/>\r\n                Do Not Modify State Directly, for example this will not re-render a component:\r\n                {ScriptSection(13, scriptArray)}\r\n                Instead use setState():\r\n                {ScriptSection(14, scriptArray)}\r\n            </p>\r\n            <p>\r\n                State Updates may be asynchronous. React may batch multiple setState() calls into a single\r\n                update for performance. Because this.props and this.state may be updated asynchronously \r\n                you shouldn't rely on their values for calculating the next state.\r\n                {ScriptSection(15, scriptArray)}\r\n                That code may fail. To fix this we use a second form of setState() that accepts a function\r\n                rather than an object. That function takes the previous state as the first argument and \r\n                the props at the time the update is applied as the second argument:\r\n                {ScriptSection(16, scriptArray)}\r\n            </p>\r\n            <p>\r\n                State Updates Are Merged. When you call setState(), React merges the object you provide\r\n                into the current state. You may have several variables which you update separately:\r\n                {ScriptSection(17, scriptArray)}\r\n                The merging is shallow, so this.setState of the comments leaves this.state.posts intact,\r\n                but completely replaces this.state.comments.                \r\n            </p>\r\n            <p>\r\n                The Data Flows Down. Neither parent nor child can know if a certain component is stateful\r\n                or stateless, and they shouldn't care whether it is defined as a function or class.\r\n                This is why state is often called local or encapsulated. It is not accessible to any component\r\n                other than the one that owns and sets it. A component may choose to pass its state down as \r\n                props to its child components:\r\n                {ScriptSection(18, scriptArray)}\r\n                The FormattedDate receives the date in its props and wouldn't know whether it came from \r\n                the clock's state, from the clock's props, or was typed by hand:\r\n                {ScriptSection(19, scriptArray)}\r\n                This is a top-down unidrectional data flow. Any state is always owned by some specific \r\n                component, and any data or UI from that state can only affect components below them\r\n                in the tree. A component tree is like a waterfall of props each component's state is like\r\n                an additional water source that joins it at an arbitrary point but flows downwards.\r\n            </p>\r\n        </div>\r\n    )\r\n}\r\n","import React from \"react\"\r\nimport JSX_Blog from \"./react_components/JSX\"\r\nimport Rendering_Elements from \"./react_components/rendering_elements\"\r\nimport ComponentsandProps from \"./react_components/Components_Props\"\r\nimport StateLifecycle from \"./react_components/State_Lifecycle\";\r\n\r\nconst React_Blog = () => (\r\n    <div style={{\r\n        margin:\"5vh\"\r\n    }}>\r\n        <h1>React</h1>\r\n\r\n        <JSX_Blog />\r\n        <Rendering_Elements/>\r\n        <ComponentsandProps/>\r\n        <StateLifecycle/>\r\n\r\n    </div>\r\n)\r\n\r\nexport default React_Blog\r\n\r\n\r\n","import React from \"react\";\r\nimport parse from \"html-react-parser\";\r\n\r\nconst Topic = ({ location }) => (\r\n  <div>\r\n    <h1>{location.state.name}</h1>\r\n    <h3>{location.state.description}</h3>\r\n    <div>\r\n      <p>{parse(location.state.content)}</p>\r\n    </div>\r\n  </div>\r\n);\r\n\r\nexport default Topic;\r\n","import React from \"react\";\r\nimport { Link, Route } from \"react-router-dom\";\r\nimport { Articles } from \"../articles/ArticleData\";\r\nimport ReactArticles from \"../articles/react/react\";\r\n\r\nimport Topic from \"./Topic\";\r\n\r\nconst Topics = ({ match }) => (\r\n  <div>\r\n    <div className=\"jumbotron\">\r\n      <h1>Topics</h1>\r\n    </div>\r\n\r\n    <div>\r\n      {Articles.map((item, id) => (\r\n        <div key={id}>\r\n          <Link\r\n            to={{\r\n              pathname: `${match.url}/${item.tag}`,\r\n              state: item\r\n            }}\r\n          >\r\n            {item.name}\r\n          </Link>\r\n        </div>\r\n      ))}\r\n    </div>\r\n    <hr />\r\n    <Route path={`/zine-blog/topics/:tag`} component={Topic} />\r\n  </div>\r\n);\r\n\r\nexport default Topics;\r\n","import React from \"react\";\r\nimport { Link } from \"react-router-dom\";\r\n\r\nconst Home = () => (\r\n  <div>\r\n    <h1>Home</h1>\r\n    <p>\r\n      This site is designed to produce helpful content for people trying to\r\n      learn deep learning and for me to formalize my understanding. I try and\r\n      take work from brilliant educators and practioners and condense and\r\n      describe them in a simplistic way to enable new comers to learn without\r\n      being intimidated.\r\n    </p>\r\n    <div>{/* <Link to=\"/zine-blog/topics\">Topics</Link> */}</div>\r\n  </div>\r\n);\r\n\r\nexport default Home;\r\n","import React from \"react\";\r\n\r\nexport default function About() {\r\n  return (\r\n    <div>\r\n      <h1>About</h1>\r\n    </div>\r\n  );\r\n}\r\n","import React from \"react\";\r\n\r\nconst PageNotFound = () => <h1>Oops! Page not found.</h1>;\r\nexport default PageNotFound;\r\n","import React, { Component } from \"react\";\r\n\r\nexport default class CoursesPage extends Component {\r\n  render() {\r\n    return (\r\n      <div>\r\n        <h2>Courses</h2>\r\n      </div>\r\n    );\r\n  }\r\n}\r\n","const counterReducer = (state = 0, action) => {\r\n  switch (action.type) {\r\n    case \"INCREMENT\":\r\n      return state + 1;\r\n    case \"DECREMENT\":\r\n      return state - 1;\r\n    default:\r\n      return state;\r\n  }\r\n};\r\n\r\nexport default counterReducer;\r\n","import { combineReducers } from \"redux\";\r\nimport counterReducer from \"./counter\";\r\n\r\nconst rootReducer = combineReducers({\r\n  counter: counterReducer\r\n});\r\nexport default rootReducer;\r\n","import React from \"react\";\nimport \"./App.css\";\nimport { Route, Switch } from \"react-router-dom\";\n\nimport Header from \"./components/common/Header\";\n//ROUTES\nimport Topics from \"./components/topics/Topics\";\nimport Home from \"./components/home/Home\";\nimport About from \"./components/about/About\";\nimport PageNotFound from \"./components/common/PageNotFound\";\nimport CoursesPage from \"./components/courses/CoursesPage\";\n//REDUX\nimport { createStore } from \"redux\";\nimport { Provider } from \"react-redux\";\nimport rootReducer from \"./components/reducers\";\n\nconst store = createStore(rootReducer);\n\nfunction App() {\n  return (\n    <Provider store={store}>\n      <div className=\"container-fluid\">\n        <Header />\n        <div className=\"App\">\n          <div>\n            <Switch>\n              <Route exact path=\"/zine-blog\" component={Home} />\n              <Route exact path=\"/zine-blog/about\" component={About} />\n              <Route path=\"/zine-blog/topics\" component={Topics} />\n              <Route path=\"/zine-blog/courses\" component={CoursesPage} />\n              <Route component={PageNotFound} />\n            </Switch>\n          </div>\n        </div>\n      </div>\n    </Provider>\n  );\n}\n\nexport default App;\n","// This optional code is used to register a service worker.\n// register() is not called by default.\n\n// This lets the app load faster on subsequent visits in production, and gives\n// it offline capabilities. However, it also means that developers (and users)\n// will only see deployed updates on subsequent visits to a page, after all the\n// existing tabs open on the page have been closed, since previously cached\n// resources are updated in the background.\n\n// To learn more about the benefits of this model and instructions on how to\n// opt-in, read https://bit.ly/CRA-PWA\n\nconst isLocalhost = Boolean(\n  window.location.hostname === 'localhost' ||\n    // [::1] is the IPv6 localhost address.\n    window.location.hostname === '[::1]' ||\n    // 127.0.0.1/8 is considered localhost for IPv4.\n    window.location.hostname.match(\n      /^127(?:\\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/\n    )\n);\n\nexport function register(config) {\n  if (process.env.NODE_ENV === 'production' && 'serviceWorker' in navigator) {\n    // The URL constructor is available in all browsers that support SW.\n    const publicUrl = new URL(process.env.PUBLIC_URL, window.location.href);\n    if (publicUrl.origin !== window.location.origin) {\n      // Our service worker won't work if PUBLIC_URL is on a different origin\n      // from what our page is served on. This might happen if a CDN is used to\n      // serve assets; see https://github.com/facebook/create-react-app/issues/2374\n      return;\n    }\n\n    window.addEventListener('load', () => {\n      const swUrl = `${process.env.PUBLIC_URL}/service-worker.js`;\n\n      if (isLocalhost) {\n        // This is running on localhost. Let's check if a service worker still exists or not.\n        checkValidServiceWorker(swUrl, config);\n\n        // Add some additional logging to localhost, pointing developers to the\n        // service worker/PWA documentation.\n        navigator.serviceWorker.ready.then(() => {\n          console.log(\n            'This web app is being served cache-first by a service ' +\n              'worker. To learn more, visit https://bit.ly/CRA-PWA'\n          );\n        });\n      } else {\n        // Is not localhost. Just register service worker\n        registerValidSW(swUrl, config);\n      }\n    });\n  }\n}\n\nfunction registerValidSW(swUrl, config) {\n  navigator.serviceWorker\n    .register(swUrl)\n    .then(registration => {\n      registration.onupdatefound = () => {\n        const installingWorker = registration.installing;\n        if (installingWorker == null) {\n          return;\n        }\n        installingWorker.onstatechange = () => {\n          if (installingWorker.state === 'installed') {\n            if (navigator.serviceWorker.controller) {\n              // At this point, the updated precached content has been fetched,\n              // but the previous service worker will still serve the older\n              // content until all client tabs are closed.\n              console.log(\n                'New content is available and will be used when all ' +\n                  'tabs for this page are closed. See https://bit.ly/CRA-PWA.'\n              );\n\n              // Execute callback\n              if (config && config.onUpdate) {\n                config.onUpdate(registration);\n              }\n            } else {\n              // At this point, everything has been precached.\n              // It's the perfect time to display a\n              // \"Content is cached for offline use.\" message.\n              console.log('Content is cached for offline use.');\n\n              // Execute callback\n              if (config && config.onSuccess) {\n                config.onSuccess(registration);\n              }\n            }\n          }\n        };\n      };\n    })\n    .catch(error => {\n      console.error('Error during service worker registration:', error);\n    });\n}\n\nfunction checkValidServiceWorker(swUrl, config) {\n  // Check if the service worker can be found. If it can't reload the page.\n  fetch(swUrl)\n    .then(response => {\n      // Ensure service worker exists, and that we really are getting a JS file.\n      const contentType = response.headers.get('content-type');\n      if (\n        response.status === 404 ||\n        (contentType != null && contentType.indexOf('javascript') === -1)\n      ) {\n        // No service worker found. Probably a different app. Reload the page.\n        navigator.serviceWorker.ready.then(registration => {\n          registration.unregister().then(() => {\n            window.location.reload();\n          });\n        });\n      } else {\n        // Service worker found. Proceed as normal.\n        registerValidSW(swUrl, config);\n      }\n    })\n    .catch(() => {\n      console.log(\n        'No internet connection found. App is running in offline mode.'\n      );\n    });\n}\n\nexport function unregister() {\n  if ('serviceWorker' in navigator) {\n    navigator.serviceWorker.ready.then(registration => {\n      registration.unregister();\n    });\n  }\n}\n","import React from \"react\";\nimport ReactDOM from \"react-dom\";\nimport \"./index.css\";\nimport App from \"./App\";\nimport * as serviceWorker from \"./serviceWorker\";\nimport { BrowserRouter as Router } from \"react-router-dom\";\n\nReactDOM.render(\n  <Router>\n    <App />\n  </Router>,\n  document.getElementById(\"root\")\n);\n\n// If you want your app to work offline and load faster, you can change\n// unregister() to register() below. Note this comes with some pitfalls.\n// Learn more about service workers: https://bit.ly/CRA-PWA\nserviceWorker.unregister();\n"],"sourceRoot":""}