(this["webpackJsonpzine-blog"]=this["webpackJsonpzine-blog"]||[]).push([[0],{31:function(e,n,t){e.exports=t(54)},36:function(e,n,t){},37:function(e,n,t){},54:function(e,n,t){"use strict";t.r(n);var a=t(0),o=t.n(a),i=t(17),r=t.n(i),s=(t(36),t(37),t(9)),l=t(6),c=function(){var e={color:"#F15B2A"};return o.a.createElement("nav",{class:"navbar navbar-expand-lg navbar-light bg-light"},o.a.createElement(l.c,{to:"/zine-blog",activeStyle:e,exact:!0},"Home")," | ",o.a.createElement(l.c,{to:"/zine-blog/about",activeStyle:e},"About")," | ",o.a.createElement(l.c,{to:"/zine-blog/topics",activeStyle:e},"Articles")," | ",o.a.createElement(l.c,{to:"/zine-blog/courses",activeStyle:e},"Courses"))},h=[{tag:"cnn",name:"Everybody loves Convolutions...",description:"The focus here is on convolutional neural networks,\n    the convolution operation and how it relates to neural nets, \n    seeing convolution as just matrix multiplication and more!",content:'<div className="container">\n    <div\n      className="row"\n      style="margin: 5vh"\n    >\n      <div className="col">\n        <h1>Everybody loves ... convolutions (Work in progress)</h1>\n        <div\n        style="background-color: #f2f2f2;\n            padding: 10px;\n            margin: 10px;\n            border-radius: 5px"\n        >\n          In 3D, the following mental image may prove useful. Imagine two sheets\n          of colored paper: one red and one blue. Put one on top of the other.\n          Now crumple them together into a small ball. That crumpled paper ball\n          is your input data, and each sheet of paper is a class of data in a\n          classification problem. What a neural network (or any other\n          machine-learning model) is meant to do is figure out a transformation\n          of the paper ball that would uncrumple it, so as to make the two\n          classes cleanly separable again. With deep learning, this would be\n          implemented as a series of simple transformations of the 3D space,\n          such as those you could apply on the paper ball with your fingers, one\n          movement at a time.\xa0 Uncrumpling paper balls is what machine learning\n          is about: finding neat representations for complex, highly folded data\n          manifolds. At this point, you should have a pretty good intuition as\n          to why deep learning excels at this: it takes the approach of\n          incrementally decomposing a complicated geometric transformation into\n          a long chain of elementary ones, which is pretty much the strategy a\n          human would follow to uncrumple a paper ball. Each layer in a deep\n          network applies a transformation that disentangles the data a\n          little\u200a-\u200aand a deep stack of layers makes tractable an extremely\n          complicated disentanglement process. Conv layers look at spatially\n          local patterns by applying the same geometric transformation to\n          different spatial locations (patches) in an input tensor. This results\n          in a representation that are translation invariant (does not matter\n          where in the image the object occurs). Convnets consist of stacks of\n          convolution and max-pooling layers. The pooling layers let you\n          spatially downsample the data, which is required to keep feature maps\n          to a reasonable size as the number of features grows, and to allow\n          subsequent convolution layers to see a greater spatial extent of the\n          inputs. Convnets are often ended with a Flatten operation or global\n          pooling layer, turning spatial feature maps into vectors, followed by\n          Dense layers to achieve classification or regression.\n          <br></br>\n          <br></br>\n          <b>Deep Learning with Python page 44\u20135.</b>\n        </div>\n\n        <img\n          src="https://i.stack.imgur.com/iY5n5.png"\n          style="width: 90%">\n\n        <h2>What is a convolution?</h2>\n        <p>\n          It is a sliding window applied to a matrix, whether that be an image\n          or feature map. At each element we multiply the filter/kernel with the\n          corresponding input pixel and then add them up. The same filter/kernel\n          is slide across the input which is called weight tying. Filters of the\n          convolutions are learned, so the model learns which filter is best to\n          apply in each layer. An early layer of a convolution is likely, at\n          best, to only detect very low level features such as edges. This is\n          because the filters are limited at earlier layers, whereas as we go\n          deeper, the amount of output channels (which we set) increases and\n          later layers have several channels with which to predict collections\n          of edges and orientations that make up a face or eye ball.\n        </p>\n\n        <h2>Properties</h2>\n        <p>\n          Location Invariance and Compositionality. Say you want to spot a cat\n          in an image. Because of this sliding window approach you don\'t care\n          where the cat occurs. The second aspect is (local) compositionality.\n          Each filter composes a local patch of lower-level features into\n          higher-level representation. You build edges from pixels, shapes from\n          edges, eyes from circles and so on. You can derive more complex shapes\n          and objects from lower level representations such as edges, pixels,\n          gradients etc and from several channels of these features.\n          Convolutions are translationally invariant because the filters slide\n          over the image horizontally and vertically. But they are not\n          rotationally invariant because the filters don\'t rotate. Thus, the net\n          seems to need several similar filters in different orientations to\n          detect objects and patterns that are differently oriented.\n        </p>\n        <img\n          src="https://cdn-images-1.medium.com/max/1200/1*B41mvbzpZ7ythn5AlJWh-A.gif"\n          style="width: 90%">\n      </div>\n    </div>\n  </div>'},{tag:"limitations",name:"Limitations of DL",description:"Focus is on the limitations of current deep learning.",content:'\n    <div>\n      <p>\n      <h4>\n      <a href="http://faculty.neu.edu.cn/yury/AAI/Textbook/Deep%20Learning%20with%20Python.pdf">\n          Heavily Inspired by this brilliant book</a>\n      </h4>\n  \n  Deep learning works by doing a series of geometric transformations on an input. And then, \n  given some error signal, improves the geometric transformations iteratively to map some \n  input x to a correct output y. \n  \n  "In deep learning, everything is a vector: everything is a point in a geometric space. \n  Model inputs and targets are first vectorized: turned into an initial input vector space \n  and target vector space. Each layer operates one simple geometric transformation on the \n  data that goes through it. The chain of layers forms one complex geometric trasnformation, \n  broken down into a series of simpler ones. This complex transformation attempts to map the \n  input space to the target space, one point at a time. This transformation is parameterized \n  by weights of the layers, which are iteratively updated based on how well the model is \n  currently performing. A key characteristic is that the geometric trasnformation must be \n  differentiable, to be able to perform gradient descent. Thi means the geometric morphing \n  from inputs to outputs must be smooth and continuous - a significant constraint."\n  \n  "The entire process of applying this complex geometric transformation to the input data can be \n  visualized in 3D by imagining a person trying to uncrumple a paper ball: the crumpled paper ball \n  is the manifold of the input data that the model starts with.\n  Each movement operated by the person on the paper ball is similar to a simple geometric \n  transformation operated by one layer. The full uncrumpling gesture sequence is the complex \n  transformation of the entire model. Deep-learning models are mathematical machines for uncrumpling \n  complicated manifolds of high-dimensional data.\n  \n  That\u2019s the magic of deep learning: turning meaning into vectors, into geometric spaces, and \n  then incrementally learning complex geometric transformations that map one space to another. \n  All you need are spaces of sufficiently high dimensionality in order to capture the full scope \n  of the relationships found in the original data."\n  \n  "The whole thing hinges on a single core idea: that meaning is derived from the pairwise \n  relationship between things (between words in a language, between pixels in an image) and \n  that these relationships can be captured by a distance function. If the brain works in \n  geometric spaces is a different question. Vector spaces are efficient to work with from a \n  computational standpoint, but different data structures can be envisioned- like graphs. \n  Neural networks initially emerged from using graphs (connectionism) but nowadays they have \n  an incorrect meaning since they are neither neural nor networks. A more appropriate name is \n  layered representations learning or hierarchical representations learning, or deep differentiable \n  models or chained geometric transforms."\n  \n  "The space of applications is nearly infinite but many are completely out of reach for current \n  deep learning techniques. You cannot train a model to read a product description and generate \n  the appropriate codebase. Anything that requires reasoning, long-term planning, and algorithmic \n  data manipulation is out of reach for deep-learning models no matter how much data is given. \n  \n  This is because a deep-learning model is just a chain of simple, continuous geometric \n  transformation mapping one vector space into another. All it can do is map one data manifold X \n  into another manifold Y, assuming the existence of a learnable continuous transform from X to Y."\n  \n  "One real risk is overestimating/ anthropomorphizing deep-learning models and their abilities. \n  A fundamental feature of humans is our theory of mind: our tendency to project intentions, beliefs, \n  and knowledge on the things around us. Drawing a smiley face on a rock suddenly makes it happy-in \n  our minds. Applied to deep learning, this means that, for instance, when we\'re able to train a \n  model to generate captions to describe pictures and the captions it generates. Then we\'re \n  surprised when any slight departure from the sort of images present in the training data \n  causes the model to generate completely absurd captions. \n  \n  This is highlighted by adversarial examples, which are samples ged to a deep-learning network \n  that are designed to trick the model into misclassifying them. It\'s possible to do gradient \n  ascent in input space to generate inputs that maximize the activation of some convnet filter-this \n  is the basis of the filter-visualization technique. Similarly, through gradient ascent you can \n  slightly modify an image to maximize the class prediction for a given class. By taking a \n  picture of a panda and adding to it a gibbon gradient, we can get a neural network to classify \n  the panda as a gibbon. This evidences both the brittleness of these models and the deep \n  difference between their input-to-output mapping and our human perception."\n  \n  <img src="https://blog.keras.io/img/limitations-of-dl/adversarial_example.png" />\n  "In short, deepl-learning models don\'t have any understanding of their input-not in the human \n  sense. Our understanding of images, sounds etc is grounded in our sensimotor experience as humans. \n  ML models have no access to such experiences and thus can\'t understand their inputs in a human \n  relatable way. By annotating large numbers of training examples to feed to our models, we get \n  them to learn a geometric transform that maps data to human concepts on a specific set of examples, \n  but this mapping is a simplistic sketch of the original model in our minds-the one developed from \n  our experience as embodied agents.\n  \n  Never fall into the trap of believing that neural networks understand the task they perform. \n  They were trained on a different, far narrower task than the one we wanted to teach them: that of \n  mapping training inputs to training targets, point by point. Show them anything that deviates from \n  their training data, and they will break."\n  \n  "Humans are capable of far more than mapping immediate stimuli to immediate\n  responses, as a deep network, or maybe an insect, would. We maintain complex, abstract\n  models of our current situation, of ourselves, and of other people, and can use these\n  models to anticipate different possible futures and perform long-term planning. We\n  can merge together known concepts to represent something we\u2019ve never experienced \n  before-like picturing a horse wearing jeans, for instance. This ability to hypothecize, \n  to expand our mental model beyond what we can experience directly-to perform abstraction \n  and reasoning-is arguably the defining characterstic of human cognition. \n  Extreme generalization: an ability to adapt to novel, never-before experienced \n  situations using little data or no new data. This stands in sharp contrast with deep nets, \n  local generalization. The mapping from inputs to outputs performed by deep net quickly \n  stops making sense if new inputs deviate from what was seen during training. Consider, \n  for instance, the problem of learning the appropriate launch parameters to get a rocket \n  to land on the moon. If you used a deep net for this task and trained\n  it using supervised learning or reinforcement learning, you\u2019d have to feed it thousands\n  or even millions of launch trials: you\u2019d need to expose it to a dense sampling of the input\n  space, in order for it to learn a reliable mapping from input space to output space. In\n  contrast, as humans we can use our power of abstraction to come up with physical \n  models\u2014rocket science\u2014and derive an exact solution that will land the rocket on the moon\n  in one or a few trials. Similarly, if you developed a deep net controlling a human body,\n  and you wanted it to learn to safely navigate a city without getting hit by cars, the net\n  would have to die many thousands of times in various situations until it could infer that\n  cars are dangerous, and develop appropriate avoidance behaviors. Dropped into a new\n  city, the net would have to relearn most of what it knows. On the other hand, humans\n  are able to learn safe behaviors without having to die even once\u2014again, thanks to our\n  power of abstract modeling of hypothetical situations."\n  \n  "In short, despite our progress on machine perception, we\u2019re still far from humanlevel AI. \n  Our models can only perform local generalization, adapting to new situations that must be \n  similar to past data, whereas human cognition is capable of extreme generalization, quickly \n  adapting to radically novel situations and planning\n  for long-term future situations."\n  </p>\n    </div>'},{tag:"research",name:"Research Idea",description:"Here I will talk on some of my research ideas.",content:'\n    <div>\n        <p>\n            The area I am most interested in is giving object detectors the ability to incorporate context\n            and relationships to other objects. \n\n            The challenge here is encoding meaning and semantics into inference of an object detector. \n            The use of object detectors becomes very limited in practical applications when un-equipped \n            with scene-level semantics. \n\n            <a href="https://arxiv.org/pdf/1807.00119.pdf">Structure Inference Net [1]</a> aims to incorporate\n            two kinds of context from scene and from <b>object relationships.</b> \n\n            This proposal tries to infer some level of context from object relationships and scene-level context.\n            "\n            Detection aims to answer what is where. From a structure perspective, it can be formulated as a \n            reasoning problem of a graph involving the mutually complementary information of scene, objects and\n            relationships.\n            "\n            <img src="https://d3i71xaburhd42.cloudfront.net/8633ebfe1bd7f6a95233025c872e4a2cf660f54c/2-Figure2-1.png"/>\n            "\n            Sequentially object detection is regarded as not only a cognition problem, but also an inference\n            problem which is based on contextual information with object fine-grained details. Objects are nodes\n            of the graph and object relationships are edges. These objects interact with each other via the \n            graph under the guidance of scene context. \n\n            Structure Inference network (SIN) to reason object state in a graph, where memory cell is the key\n            module to encode different kinds of messages (e.g. from scene and other objects) into object state,\n            and a novel way of using GRUs as the memory cell is presented in this work. Specifically, we fix object\n            representation as the inital state of GRU and then input each kind of message to achieve the goal of\n            updating object state. Since SIN can accomplish inference as long as the inputs to it covers the \n            representations of object, scene-level context and instance-level relationship, our structure inference\n            method is not constrained to specific detection framework.\n\n\n            "\n\n            Two-stage detectors: (F-RCNN, Faster-RCNN) first stage produces numbers of condidate boxes, and then \n            the second stage classifies these boxes into foreground classes or background. R-CNN extracts CNN features\n            from candidate regions and applies linear SVMs as classifier. Faster-RCNN proposes novel ROI-pooling\n            operation to extract feature vectors for each candidate box from shared convolutional feature map. \n\n            "\n            For example, Mottaghi et al. [29] propose a deformable part-based\n            model, which exploits both local context around each candidate detection and \n            global context at the level of the scene.\n            The presence of objects in irrelevant scenes is penalized in\n            [41]. Recently, some works [3, 43, 37] based on deep ConvNet have made some \n            attempts to incorporate contextual information to object detection. Contextual information outside the region of interest is integrated using spatial recurrent neural network in ION [3]. GBD-Net [43] proposes\n            a novel gated bi-directional CNN to pass message between\n            features of different support regions around objects. Shrivastava et al. [37] \n            use segmentation to provide top-down\n            context to guide region proposal generation and object detection. While context \n            around object or scene-level context\n            has been addressed in such works [3, 43, 37] under the deep\n            learning-based pipeline, they make less progress in exploring object-object \n            relationships. On the contrary, a much recent work [4] proposes a new sequential \n            reasoning architecture that mainly exploits object-object relationships to \n            sequentially detect objects in an image, however, with only\n            implicit yet weak consideration of scene-level context. Different from these \n            existing works, our proposed structure inference network has the capability \n            of jointly modeling both\n            scene-level context and object-object relationships and inferring different \n            object instances within an image from a\n            structural and global perspective.\n            Structure inference. Several interesting works [28, 34,\n            23, 39, 21, 9, 2, 22, 42] have been proposed to combine\n            deep networks with graphical models for structured prediction tasks that are solved by structure inference techniques.\n            A generic structured model is designed to leverage diverse\n            label relations including scene, object and attributes to improve image classification performance in [21]. Deng et al.\n            [9] propose structure inference machines for analyzing relations in group activity recognition. Structural-RNN [22]\n            combines the power of high-level spatio-temporal graphs\n            and sequence learning, and evaluates the model ranging\n            from motion to object interactions. In [42], a graph inference model is proposed to tackle the task of generating\n            structured scene graph from an image. While our work\n            shares similar spirit as [42] to formulate the object detection task as a graph structure inference problem, the two\n            works have essential differences in their technical sides,\n            such as the graph instantiation manners, inference mechanisms, message passing schemes, etc, which highly depend\n            on the specific task domains.\n\n            \n\n            <img src="https://d3i71xaburhd42.cloudfront.net/8633ebfe1bd7f6a95233025c872e4a2cf660f54c/3-Figure3-1.png" />\n            Figure 3. SIN: The Framework of Our Method. Firstly we get a fixed number of ROIs from an input image. Each ROI is pooled into a\n            fixed-size feature map and then mapped to a feature vector by a fully connected layer as node. We extract the whole image feature as scene\n            in the same way, and then we concatenate the descriptors of every two ROIs into edges. To iteratively update the node state, an elaborately\n            designed structure inference method is triggered, and the final state of each node is used to predict the category and refine the location of\n            the corresponding ROI. The whole framework is trained end-to-end with the original multi-task loss (this study exploits Faster R-CNN as\n            the base detection framework).\n            "\n\n            <a href="https://arxiv.org/pdf/1807.05857.pdf">Object Relation Detection Based on One-shot Learning</a>\n            Object relation is abstract representation of the visually\n            observable interactions between a pair of a subject and an\n            object, such as \u201cperson play piano\u201d. Detecting object relations in images is one \n            crucial task in image understanding. Each object relation involves a pair of \n            localized objects\n            (subject and object) which are connected via a predicate. A\n            predicate can be action (e.g. \u201cplay\u201d), a spatial preposition\n            (e.g. \u201con\u201d) or some comparative expression (e.g. \u201ctaller\n            than\u201d). While objects are the basic constituent elements of\n            an image, it is often the relations between objects that provide the holistic \n            interpretation of a scene.\n            <img src="https://www.researchgate.net/profile/Jian_Zhao68/publication/326437189/figure/fig1/AS:649228653191178@1531799581405/Here-are-two-examples-from-the-Visual-Relationship-Dataset-15-Object-relations-widely_Q320.jpg"/>\n            Meanwhile, each relation involves two parts, namely subject and object, resulting in a greater skew of rare relations especially when\n            co-occurrence of some pairs of objects is infrequent in the\n            dataset. Some types of relations contain very limited examples. \n            We call this phenomenon the <b>long-tail problem</b>. The\n            second reason is the large intra-class divergence. Relations that \n            have the same predicate but different subjects or\n            objects are essentially different. \n\n            The third reason is the semantic dependency that the predicate in a \n            relation is not only determined\n            by semantic information but also the categories of the subject and the object.\n\n            In essence, object relation detection can be regarded\n            as a classification task. Then the fundamental problem is\n            how to formulate the relation triplet (subject, predicate, object) to \n            define a reasonable classification task.\n\n            Without considering semantics dependencies, learning an object detection\n            model would be challenged by two difficulties. The first\n            difficulty is due to the intra-class divergence problem we\n            discuss above. A relation may contain different semantics\n            that would confuse the model. Second, the ignorance of\n            semantics dependencies consequently results in the lack of\n            attention of the model to visual feature regions at the high\n            level. However, in the human visual system, one can concentrate his attention volitionally due to top-down signals\n            guided by the current task (e.g., attempting to determine the\n            relation between objects), and automatically due to bottomup signals caused by salient or eye-catching stimuli.\n            \n            According to the above observations, we propose to learn\n            models for objects and predicates respectively and develop a\n            framework which combines both bottom-up and top-down\n            attention mechanisms. We refer to the mechanism related\n            to semantics dependencies as the top-down mechanism\n            and the one purely related to visual representations as the\n            bottom-up mechanism. The bottom-up mechanism generates a set of object proposals with category information,\n            then visual features are extracted from these proposals by\n            a Convolution Neural Network [19]. Practically, we implement bottom-up attention utilizing Faster R-CNN [16].\n            The top-down mechanism exploits semantics dependencies\n            (the categories of subjects and objects) to predict an attention distribution over the visual features from the bottom-up\n            mechanism.\n\n            Besides the dual attention mechanisms, the learning to\n            learn [5] module\u2014Semantics Induced Learner (SIL)\u2014is\n            the core component of our framework. SIL is able to\n            learn to fast adapt the predicate classification model conditioned on the semantics dependencies inferred from the\n            categories of subjects and objects, and therefore effectively\n            improves performance of the predicate classification model.\n            The proposed SIL incorporates semantics dependencies into\n            the predicate classification model in a novel and effective\n            way, then dynamically determines, in one-shot manner, the\n            weightings of visual features generated by the bottom-up\n            mechanism, which is challenging to accomplish for a purely\n            static predicate classification model.\n\n            \n\n            <a href=""></a>\n            <a href=""></a>\n            <a href=""></a>\n            <a href=""></a>\n            <a href=""></a>\n            <a href=""></a>\n            <a href=""></a>\n\n        </p>\n    </div>\n)'},{tag:"",name:"",description:"",content:""},{tag:"",name:"",description:"",content:""},{tag:"",name:"",description:"",content:""}];var d=t(10),p=t(11),m=t(14),u=t(12),f=t(15);o.a.Component;o.a.Component;var g=t(28),b=t.n(g),y=function(e){var n=e.location;return o.a.createElement("div",null,o.a.createElement("h1",null,n.state.name),o.a.createElement("h3",null,n.state.description),o.a.createElement("div",null,o.a.createElement("p",null,b()(n.state.content))))},v=function(e){var n=e.match;return o.a.createElement("div",null,o.a.createElement("div",{className:"jumbotron"},o.a.createElement("h1",null,"Topics")),o.a.createElement("div",null,h.map((function(e,t){return o.a.createElement("div",{key:t},o.a.createElement(l.b,{to:{pathname:"".concat(n.url,"/").concat(e.tag),state:e}},e.name))}))),o.a.createElement("hr",null),o.a.createElement(s.a,{path:"/zine-blog/topics/:tag",component:y}))},w=function(){return o.a.createElement("div",null,o.a.createElement("h1",null,"Home"),o.a.createElement("p",null,"This site is designed to produce helpful content for people trying to learn deep learning and for me to formalize my understanding. I try and take work from brilliant educators and practioners and condense and describe them in a simplistic way to enable new comers to learn without being intimidated."),o.a.createElement("div",null))};function k(){return o.a.createElement("div",null,o.a.createElement("h1",null,"About"))}var x=function(){return o.a.createElement("h1",null,"Oops! Page not found.")},j=function(e){function n(){return Object(d.a)(this,n),Object(m.a)(this,Object(u.a)(n).apply(this,arguments))}return Object(f.a)(n,e),Object(p.a)(n,[{key:"render",value:function(){return o.a.createElement("div",null,o.a.createElement("h2",null,"Courses"))}}]),n}(a.Component),E=t(13),T=t(30),N=function(){var e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:0,n=arguments.length>1?arguments[1]:void 0;switch(n.type){case"INCREMENT":return e+1;case"DECREMENT":return e-1;default:return e}},I=Object(E.b)({counter:N}),z=Object(E.c)(I);var S=function(){return o.a.createElement(T.a,{store:z},o.a.createElement("div",{className:"container-fluid"},o.a.createElement(c,null),o.a.createElement("div",{className:"App"},o.a.createElement("div",null,o.a.createElement(s.c,null,o.a.createElement(s.a,{exact:!0,path:"/zine-blog",component:w}),o.a.createElement(s.a,{exact:!0,path:"/zine-blog/about",component:k}),o.a.createElement(s.a,{path:"/zine-blog/topics",component:v}),o.a.createElement(s.a,{path:"/zine-blog/courses",component:j}),o.a.createElement(s.a,{component:x}))))))};Boolean("localhost"===window.location.hostname||"[::1]"===window.location.hostname||window.location.hostname.match(/^127(?:\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/));r.a.render(o.a.createElement(l.a,null,o.a.createElement(S,null)),document.getElementById("root")),"serviceWorker"in navigator&&navigator.serviceWorker.ready.then((function(e){e.unregister()}))}},[[31,1,2]]]);
//# sourceMappingURL=main.ddae9b3b.chunk.js.map