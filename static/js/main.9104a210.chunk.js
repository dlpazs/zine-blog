(this["webpackJsonpzine-blog"]=this["webpackJsonpzine-blog"]||[]).push([[0],{17:function(e,t,a){e.exports=a(29)},22:function(e,t,a){},23:function(e,t,a){},29:function(e,t,a){"use strict";a.r(t);var n=a(0),i=a.n(n),o=a(14),r=a.n(o),s=(a(22),a(23),a(4)),l=a(6),c=function(){return i.a.createElement("div",{className:"container"},i.a.createElement("div",{className:"row",style:{margin:"5vh"}},i.a.createElement("div",{className:"col"},i.a.createElement("h1",null,"Everybody loves ... convolutions (Work in progress)"),i.a.createElement("div",{style:{backgroundColor:"#f2f2f2",padding:"10px",margin:"10px",borderRadius:"5px"}},"In 3D, the following mental image may prove useful. Imagine two sheets of colored paper: one red and one blue. Put one on top of the other. Now crumple them together into a small ball. That crumpled paper ball is your input data, and each sheet of paper is a class of data in a classification problem. What a neural network (or any other machine-learning model) is meant to do is figure out a transformation of the paper ball that would uncrumple it, so as to make the two classes cleanly separable again. With deep learning, this would be implemented as a series of simple transformations of the 3D space, such as those you could apply on the paper ball with your fingers, one movement at a time.\xa0 Uncrumpling paper balls is what machine learning is about: finding neat representations for complex, highly folded data manifolds. At this point, you should have a pretty good intuition as to why deep learning excels at this: it takes the approach of incrementally decomposing a complicated geometric transformation into a long chain of elementary ones, which is pretty much the strategy a human would follow to uncrumple a paper ball. Each layer in a deep network applies a transformation that disentangles the data a little\u200a-\u200aand a deep stack of layers makes tractable an extremely complicated disentanglement process. Conv layers look at spatially local patterns by applying the same geometric transformation to different spatial locations (patches) in an input tensor. This results in a representation that are translation invariant (does not matter where in the image the object occurs). Convnets consist of stacks of convolution and max-pooling layers. The pooling layers let you spatially downsample the data, which is required to keep feature maps to a reasonable size as the number of features grows, and to allow subsequent convolution layers to see a greater spatial extent of the inputs. Convnets are often ended with a `Flatten` operation or global pooling layer, turning spatial feature maps into vectors, followed by `Dense` layers to achieve classification or regression.",i.a.createElement("br",null),i.a.createElement("br",null),i.a.createElement("b",null,"Deep Learning with Python page 44\u20135.")),i.a.createElement("img",{src:"https://i.stack.imgur.com/iY5n5.png",style:{width:"50%",height:"30%"}}),i.a.createElement("h2",null,"What is a convolution?"),i.a.createElement("p",null,"It is a sliding window applied to a matrix, whether that be an image or feature map. At each element we multiply the filter/kernel with the corresponding input pixel and then add them up. The same filter/kernel is slide across the input which is called weight tying. Filters of the convolutions are learned, so the model learns which filter is best to apply in each layer. An early layer of a convolution is likely, at best, to only detect very low level features such as edges. This is because the filters are limited at earlier layers, whereas as we go deeper, the amount of output channels (which we set) increases and later layers have several channels with which to predict collections of edges and orientations that make up a face or eye ball."),i.a.createElement("h2",null,"Properties"),i.a.createElement("p",null,"Location Invariance and Compositionality. Say you want to spot a cat in an image. Because of this sliding window approach you don't care where the cat occurs. The second aspect is (local) compositionality. Each filter composes a local patch of lower-level features into higher-level representation. You build edges from pixels, shapes from edges, eyes from circles and so on. You can derive more complex shapes and objects from lower level representations such as edges, pixels, gradients etc and from several channels of these features. Convolutions are translationally invariant because the filters slide over the image horizontally and vertically. But they are not rotationally invariant because the filters don't rotate. Thus, the net seems to need several similar filters in different orientations to detect objects and patterns that are differently oriented."),i.a.createElement("img",{src:"https://cdn-images-1.medium.com/max/1200/1*B41mvbzpZ7ythn5AlJWh-A.gif",style:{width:"90%"}}))))},m=function(){return i.a.createElement("div",null,i.a.createElement("h1",null,"Limitations (Work in progress)"),i.a.createElement("p",null,i.a.createElement("h2",null,i.a.createElement("a",{href:"http://faculty.neu.edu.cn/yury/AAI/Textbook/Deep%20Learning%20with%20Python.pdf"},"Heavily Inspired by this brilliant book")),'Deep learning works by doing a series of geometric transformations on an input. And then, given some error signal, improves the geometric transformations iteratively to map some input x to a correct output y. "In deep learning, everything is a vector: everything is a point in a geometric space. Model inputs and targets are first vectorized: turned into an initial input vector space and target vector space. Each layer operates one simple geometric transformation on the data that goes through it. The chain of layers forms one complex geometric trasnformation, broken down into a series of simpler ones. This complex transformation attempts to map the input space to the target space, one point at a time. This transformation is parameterized by weights of the layers, which are iteratively updated based on how well the model is currently performing. A key characteristic is that the geometric trasnformation must be differentiable, to be able to perform gradient descent. Thi means the geometric morphing from inputs to outputs must be smooth and continuous - a significant constraint." "The entire process of applying this complex geometric transformation to the input data can be visualized in 3D by imagining a person trying to uncrumple a paper ball: the crumpled paper ball is the manifold of the input data that the model starts with. Each movement operated by the person on the paper ball is similar to a simple geometric transformation operated by one layer. The full uncrumpling gesture sequence is the complex transformation of the entire model. Deep-learning models are mathematical machines for uncrumpling complicated manifolds of high-dimensional data. That\u2019s the magic of deep learning: turning meaning into vectors, into geometric spaces, and then incrementally learning complex geometric transformations that map one space to another. All you need are spaces of sufficiently high dimensionality in order to capture the full scope of the relationships found in the original data." "The whole thing hinges on a single core idea: that meaning is derived from the pairwise relationship between things (between words in a language, between pixels in an image) and that these relationships can be captured by a distance function. If the brain works in geometric spaces is a different question. Vector spaces are efficient to work with from a computational standpoint, but different data structures can be envisioned- like graphs. Neural networks initially emerged from using graphs (connectionism) but nowadays they have an incorrect meaning since they are neither neural nor networks. A more appropriate name is layered representations learning or hierarchical representations learning, or deep differentiable models or chained geometric transforms." "The space of applications is nearly infinite but many are completely out of reach for current deep learning techniques. You cannot train a model to read a product description and generate the appropriate codebase. Anything that requires reasoning, long-term planning, and algorithmic data manipulation is out of reach for deep-learning models no matter how much data is given. This is because a deep-learning model is just a chain of simple, continuous geometric transformation mapping one vector space into another. All it can do is map one data manifold X into another manifold Y, assuming the existence of a learnable continuous transform from X to Y." "One real risk is overestimating/ anthropomorphizing deep-learning models and their abilities. A fundamental feature of humans is our theory of mind: our tendency to project intentions, beliefs, and knowledge on the things around us. Drawing a smiley face on a rock suddenly makes it happy-in our minds. Applied to deep learning, this means that, for instance, when we\'re able to train a model to generate captions to describe pictures and the captions it generates. Then we\'re surprised when any slight departure from the sort of images present in the training data causes the model to generate completely absurd captions. This is highlighted by adversarial examples, which are samples ged to a deep-learning network that are designed to trick the model into misclassifying them. It\'s possible to do gradient ascent in input space to generate inputs that maximize the activation of some convnet filter-this is the basis of the filter-visualization technique. Similarly, through gradient ascent you can slightly modify an image to maximize the class prediction for a given class. By taking a picture of a panda and adding to it a gibbon gradient, we can get a neural network to classify the panda as a gibbon. This evidences both the brittleness of these models and the deep difference between their input-to-output mapping and our human perception."',i.a.createElement("img",{src:"https://blog.keras.io/img/limitations-of-dl/adversarial_example.png"}),'"In short, deepl-learning models don\'t have any understanding of their input-not in the human sense. Our understanding of images, sounds etc is grounded in our sensimotor experience as humans. ML models have no access to such experiences and thus can\'t understand their inputs in a human relatable way. By annotating large numbers of training examples to feed to our models, we get them to learn a geometric transform that maps data to human concepts on a specific set of examples, but this mapping is a simplistic sketch of the original model in our minds-the one developed from our experience as embodied agents. Never fall into the trap of believing that neural networks understand the task they perform. They were trained on a different, far narrower task than the one we wanted to teach them: that of mapping training inputs to training targets, point by point. Show them anything that deviates from their training data, and they will break." "Humans are capable of far more than mapping immediate stimuli to immediate responses, as a deep network, or maybe an insect, would. We maintain complex, abstract models of our current situation, of ourselves, and of other people, and can use these models to anticipate different possible futures and perform long-term planning. We can merge together known concepts to represent something we\u2019ve never experienced before-like picturing a horse wearing jeans, for instance. This ability to hypothecize, to expand our mental model beyond what we can experience directly-to perform abstraction and reasoning-is arguably the defining characterstic of human cognition. Extreme generalization: an ability to adapt to novel, never-before experienced situations using little data or no new data. This stands in sharp contrast with deep nets, local generalization. The mapping from inputs to outputs performed by deep net quickly stops making sense if new inputs deviate from what was seen during training. Consider, for instance, the problem of learning the appropriate launch parameters to get a rocket to land on the moon. If you used a deep net for this task and trained it using supervised learning or reinforcement learning, you\u2019d have to feed it thousands or even millions of launch trials: you\u2019d need to expose it to a dense sampling of the input space, in order for it to learn a reliable mapping from input space to output space. In contrast, as humans we can use our power of abstraction to come up with physical models\u2014rocket science\u2014and derive an exact solution that will land the rocket on the moon in one or a few trials. Similarly, if you developed a deep net controlling a human body, and you wanted it to learn to safely navigate a city without getting hit by cars, the net would have to die many thousands of times in various situations until it could infer that cars are dangerous, and develop appropriate avoidance behaviors. Dropped into a new city, the net would have to relearn most of what it knows. On the other hand, humans are able to learn safe behaviors without having to die even once\u2014again, thanks to our power of abstract modeling of hypothetical situations." "In short, despite our progress on machine perception, we\u2019re still far from humanlevel AI. Our models can only perform local generalization, adapting to new situations that must be similar to past data, whereas human cognition is capable of extreme generalization, quickly adapting to radically novel situations and planning for long-term future situations."'))},h=function(){return i.a.createElement("nav",{class:"navbar navbar-expand-lg navbar-light bg-light"},i.a.createElement("a",{class:"navbar-brand",href:"/zine-blog"},"DL Zine's"),i.a.createElement("button",{class:"navbar-toggler",type:"button","data-toggle":"collapse","data-target":"#navbarNav","aria-controls":"navbarNav","aria-expanded":"false","aria-label":"Toggle navigation"},i.a.createElement("span",{class:"navbar-toggler-icon"})),i.a.createElement("div",{class:"collapse navbar-collapse",id:"navbarNav"},i.a.createElement("ul",{class:"navbar-nav"},i.a.createElement("li",{class:"nav-item active"},i.a.createElement("a",{class:"nav-link",href:"/zine-blog"},"Home ",i.a.createElement("span",{class:"sr-only"},"(current)"))),i.a.createElement("li",{class:"nav-item"},i.a.createElement(s.b,{class:"nav-link",to:"/zine-blog/topics"},"Topics")))))},d=function(e){e.match;return i.a.createElement("div",{style:{margin:"5vh"}},i.a.createElement("h1",null,"Topics"),i.a.createElement("div",{class:"row",style:{margin:"5vh"}},i.a.createElement("div",{class:"col-sm-4"},i.a.createElement("div",{class:"card"},i.a.createElement("div",{class:"card-body"},i.a.createElement("h5",{class:"card-title"},"Everybody loves Convolutions..."),i.a.createElement("p",{class:"card-text"},"The focus here is on convolutional neural networks, the convolution operation and how it relates to neural nets, seeing convolution as just matrix multiplication and more!"),i.a.createElement(s.b,{to:"/cnns",class:"btn btn-primary"},"Read")))),i.a.createElement("div",{class:"col-sm-4"},i.a.createElement("div",{class:"card"},i.a.createElement("div",{class:"card-body"},i.a.createElement("h5",{class:"card-title"},"Limitations"),i.a.createElement("p",{class:"card-text"},"The aim of this post is to learn about the Limitations of neural networks before you embark on some new project so you can avoid wasting time."),i.a.createElement(s.b,{to:"/limitations",class:"btn btn-primary"},"Read")))),i.a.createElement("div",{class:"col-sm-4"},i.a.createElement("div",{class:"card"},i.a.createElement("div",{class:"card-body"},i.a.createElement("h5",{class:"card-title"},"SGD"),i.a.createElement("p",{class:"card-text"},"The aim of this post is to learn about the Limitations of neural networks before you embark on some new project so you can avoid wasting time."),i.a.createElement(s.b,{to:"/sgd",class:"btn btn-primary"},"Read"))))),i.a.createElement("div",{class:"row",style:{margin:"5vh"}},i.a.createElement("div",{class:"col-sm-4"},i.a.createElement("div",{class:"card"},i.a.createElement("div",{class:"card-body"},i.a.createElement("h5",{class:"card-title"},"Batch Norm"),i.a.createElement("p",{class:"card-text"},"The focus here is on convolutional neural networks, the convolution operation and how it relates to neural nets, seeing convolution as just matrix multiplication and more!"),i.a.createElement(s.b,{to:"/batch_norm",class:"btn btn-primary"},"Read")))),i.a.createElement("div",{class:"col-sm-4"},i.a.createElement("div",{class:"card"},i.a.createElement("div",{class:"card-body"},i.a.createElement("h5",{class:"card-title"},"Limitations"),i.a.createElement("p",{class:"card-text"},"The aim of this post is to learn about the Limitations of neural networks before you embark on some new project so you can avoid wasting time."),i.a.createElement(s.b,{to:"/limitations",class:"btn btn-primary"},"Read")))),i.a.createElement("div",{class:"col-sm-4"},i.a.createElement("div",{class:"card"},i.a.createElement("div",{class:"card-body"},i.a.createElement("h5",{class:"card-title"},"SGD"),i.a.createElement("p",{class:"card-text"},"The aim of this post is to learn about the Limitations of neural networks before you embark on some new project so you can avoid wasting time."),i.a.createElement(s.b,{to:"/sgd",class:"btn btn-primary"},"Read"))))))},p=function(){return i.a.createElement("div",null,i.a.createElement("h1",null,"Home"),i.a.createElement("p",null,"This site is designed to produce helpful content for people trying to learn deep learning and for me to formalize my understanding. I try and take work from brilliant educators and practioners and condense and describe them in a simplistic way to enable new comers to learn without being intimidated."))},u=function(){return i.a.createElement("div",{style:{margin:"5vh"}},i.a.createElement("h1",null,"Stochastic Gradient Descent"),i.a.createElement("h2",null,"How do you create a predictor function as a function of pixel values?"),i.a.createElement("p",null,"What does this mean? Well the broader question here is how does a simple series of matrix multiplications result in a good prediction based on its input.",i.a.createElement("br",null),i.a.createElement("img",{src:"https://latex.codecogs.com/gif.latex?%24y%3Dmx&plus;b%24"}),i.a.createElement("br",null),i.a.createElement("img",{src:"https://latex.codecogs.com/gif.latex?%24y%3Da_1x_1&plus;a_2*x_2%24"})))},g=function(){return i.a.createElement("div",null,i.a.createElement("h1",null,"Batch Norm"),i.a.createElement("h3",null,"What is Batch Normalization? It accelerates training by reducing internal covariate shift. ... Or does it?"),i.a.createElement("h3",null,"How does it work?"),i.a.createElement("p",null,"Batch norm removes the mean and and divides by the standard deviation of a channel of activations. Insert Channel Activation picture",i.a.createElement("br",null),i.a.createElement("img",{src:"https://miro.medium.com/max/810/1*Hiq-rLFGDpESpr8QNsJ1jg.png"}),i.a.createElement("br",null),'"How does batch normalization help optimization? Not by reducing internal covariate shift. What it does do, is allow you to increase your learning rate. The standard training without batch norm has a much more bumpier loss landscape but with batch norm there are less big bumps. So there is less risk of divergence. It is simple, it takes a mini-batch and it\u2019s activations. Firstly, we find the mean of the activativations, then the variance. Then we normalize, so element-wise activation values minus the mean divided by the standard deviation. The really important bit is: we take those normalized activation values and we add a vector of biases. Then we use something that\u2019s like a bias but instead we multiply by it. We have a gamma which we multiply by those normalized activations and add a bias beta. These two parameters are learnable parameters. The value of our predictions y_hat = f(w_1, w_2,...,w_n, X) is some function of our weights (there could be millions) and our input. This is our neural net function. Our loss: L = Sum(y_hat-y)^2. We want to predict outcomes between 1 and 5. Our activations and our final layer are between -1 and 1, and they\u2019re way off. The scale is off the mean is off. One thing we can do is try to come up with new weights to cause the spread and mean to increase. But this is hard since all these weights interact in specific ways. So that will take ages. But what if we amended that neural net function: y_hat = f(w_1, w_2, \u2026, w_n, X) * gamma + beta. These are two more parameter vectors, so this is really easy. To increase the scale or change the mean we have these two parameters and we have their gradient to increase the scale/mean. That is what batch norm does. It makes it easier to shift the outputs up and down etc. In practice, we don\u2019t use a different mean and standard deviation for each mini-batch since it would vary so greatly. So instead we take the exponentially weighted moving average of the mean and standard deviation. They use the same technique as they did for adam to calculate this. You can vary the amount of momentum in the batch norm layer. The smaller the momentum the less the standard deviation and mean will vary less from mini-batch to mini-batch and will have less of a regularising effect.""'))};var f=function(){return i.a.createElement("div",{className:"container"},i.a.createElement(s.a,null,i.a.createElement(h,null),i.a.createElement("div",{className:"App"},i.a.createElement("div",null,i.a.createElement(l.a,{exact:!0,path:"/zine-blog",component:p}),i.a.createElement(l.a,{path:"/zine-blog/topics",component:d}),i.a.createElement(l.a,{path:"/cnns",component:c}),i.a.createElement(l.a,{path:"/limitations",component:m}),i.a.createElement(l.a,{path:"/sgd",component:u}),i.a.createElement(l.a,{path:"/batch_norm",component:g})))))};Boolean("localhost"===window.location.hostname||"[::1]"===window.location.hostname||window.location.hostname.match(/^127(?:\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/));r.a.render(i.a.createElement(f,null),document.getElementById("root")),"serviceWorker"in navigator&&navigator.serviceWorker.ready.then((function(e){e.unregister()}))}},[[17,1,2]]]);
//# sourceMappingURL=main.9104a210.chunk.js.map