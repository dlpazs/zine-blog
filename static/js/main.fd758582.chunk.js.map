{"version":3,"sources":["components/cnn/cnn.zine.js","components/limitations/Limitations.js","components/layout/NavBar.js","components/topics/Topics.js","components/home/Home.js","components/sgd/SGD.js","components/batch_norm/BatchNorm.js","components/linear_algebra/LinearAlgebra.js","App.js","serviceWorker.js","index.js"],"names":["CNN","className","style","margin","backgroundColor","padding","borderRadius","src","width","height","Limitations","href","NavBar","class","type","data-toggle","data-target","aria-controls","aria-expanded","aria-label","id","to","Topics","match","Home","SGD","BatchNorm","LinearAlgebra","App","exact","path","component","Boolean","window","location","hostname","ReactDOM","render","document","getElementById","navigator","serviceWorker","ready","then","registration","unregister"],"mappings":"mQA4FeA,EA1FH,kBACA,yBAAKC,UAAU,aACX,yBAAKA,UAAU,MACfC,MAAO,CACHC,OAAO,QAGP,yBAAKF,UAAU,OACf,mFACA,yBACAC,MAAO,CACHE,gBAAgB,UAChBC,QAAQ,OACRF,OAAO,OACPG,aAAa,QALjB,sjEAoCJ,6BAAS,6BACT,yEAMA,yBAAKC,IAAI,sCACTL,MAAO,CACHM,MAAM,MACNC,OAAO,SAGX,sDACA,4wBAUA,0CACA,+3BAWA,yBAAKF,IAAI,wEACTL,MAAO,CACHM,MAAM,aCwCXE,EA3HK,kBAClB,6BACE,8DACA,2BACA,4BACA,uBAAGC,KAAK,mFAAR,4CAFA,krJAsEJ,yBAAKJ,IAAI,wEAtEL,48GCmBWK,EArBA,kBACX,yBAAKC,MAAM,iDACP,uBAAGA,MAAM,eAAeF,KAAK,cAA7B,aACA,4BAAQE,MAAM,iBAAiBC,KAAK,SAASC,cAAY,WAAWC,cAAY,aAAaC,gBAAc,YAAYC,gBAAc,QAAQC,aAAW,qBACpJ,0BAAMN,MAAM,yBAGhB,yBAAKA,MAAM,2BAA2BO,GAAG,aACrC,wBAAIP,MAAM,cACV,wBAAIA,MAAM,mBACN,uBAAGA,MAAM,WAAWF,KAAK,cAAzB,QAA2C,0BAAME,MAAM,WAAZ,eAE/C,wBAAIA,MAAM,YACN,kBAAC,IAAD,CAAMA,MAAM,WAAWQ,GAAG,qBAA1B,eCyEDC,EAtFA,SAAC,GAAD,EAAGC,MAAH,OACb,yBACArB,MAAO,CACLC,OAAO,QAKP,sCAIA,yBAAKU,MAAM,MAAKX,MAAO,CACPC,OAAO,QAErB,yBAAKU,MAAM,YACT,yBAAKA,MAAM,QACT,yBAAKA,MAAM,aACT,wBAAIA,MAAM,cAAV,mCACA,uBAAGA,MAAM,aAAT,+KAGA,kBAAC,IAAD,CAAMQ,GAAG,QAAQR,MAAM,mBAAvB,WAIN,yBAAKA,MAAM,YACT,yBAAKA,MAAM,QACT,yBAAKA,MAAM,aACT,wBAAIA,MAAM,cAAV,eACA,uBAAGA,MAAM,aAAT,kJAGA,kBAAC,IAAD,CAAMQ,GAAG,eAAeR,MAAM,mBAA9B,WAIN,yBAAKA,MAAM,YACT,yBAAKA,MAAM,QACT,yBAAKA,MAAM,aACT,wBAAIA,MAAM,cAAV,OACA,uBAAGA,MAAM,cACT,kBAAC,IAAD,CAAMQ,GAAG,OAAOR,MAAM,mBAAtB,YAOR,yBAAKA,MAAM,MAAKX,MAAO,CACPC,OAAO,QAErB,yBAAKU,MAAM,YACT,yBAAKA,MAAM,QACT,yBAAKA,MAAM,aACT,wBAAIA,MAAM,cAAV,cACA,uBAAGA,MAAM,cACT,kBAAC,IAAD,CAAMQ,GAAG,cAAcR,MAAM,mBAA7B,WAIN,yBAAKA,MAAM,YACT,yBAAKA,MAAM,QACT,yBAAKA,MAAM,aACT,wBAAIA,MAAM,cAAV,sCACA,uBAAGA,MAAM,cACT,kBAAC,IAAD,CAAMQ,GAAG,KAAKR,MAAM,mBAApB,WAIN,yBAAKA,MAAM,YACT,yBAAKA,MAAM,QACT,yBAAKA,MAAM,aACT,wBAAIA,MAAM,cAAV,yBACA,uBAAGA,MAAM,cACT,kBAAC,IAAD,CAAMQ,GAAG,kBAAkBR,MAAM,mBAAjC,cChEGW,EAZF,kBACX,6BACA,oCACA,6UCgBaC,EAnBH,kBACR,yBACAvB,MAAO,CACHC,OAAO,QAEP,2DACA,qGACA,uLAII,6BACA,yBAAKI,IAAI,6DACT,6BACA,yBAAKA,IAAI,0ECuFNmB,EArGG,kBACd,yBAAKxB,MAAO,CACRC,OAAO,QAEP,0CAEA,0IAGA,iDACA,gIAGA,6BAHA,oCAKA,6BACA,yBAAKI,IAAI,iEACT,6BAPA,0vEAmCA,8aAQR,yBACAA,IAAI,8GATI,kOAeR,yCAfQ,8SAuBR,yBAAKA,IAAI,4GAGT,0CACA,4BACI,4BAAI,uBAAGI,KAAK,4DAAR,cACJ,4BAAI,uBAAGA,KAAK,wCAAR,sDAOA,msCCsEOgB,EAvJO,kBAClB,6BACI,qDAEA,6BACI,2DACA,6rBAWI,6BAAS,6BACT,yBAAKpB,IAAI,sEACT,6BAAS,+BAlBrB,05jCC2CWqB,MAvBf,WACE,OACE,yBAAK3B,UAAU,aACb,kBAAC,IAAD,KACA,kBAAC,EAAD,MACA,yBAAKA,UAAU,OAEb,6BACE,kBAAC,IAAD,CAAO4B,OAAK,EAACC,KAAK,aAAaC,UAAWP,IAC1C,kBAAC,IAAD,CAAOM,KAAK,oBAAoBC,UAAWT,IAC3C,kBAAC,IAAD,CAAOQ,KAAK,QAAQC,UAAW/B,IAC/B,kBAAC,IAAD,CAAO8B,KAAK,eAAeC,UAAWrB,IACtC,kBAAC,IAAD,CAAOoB,KAAK,OAAOC,UAAWN,IAC9B,kBAAC,IAAD,CAAOK,KAAK,cAAcC,UAAWL,IACrC,kBAAC,IAAD,CAAOI,KAAK,kBAAkBC,UAAWJ,SCzB/BK,QACW,cAA7BC,OAAOC,SAASC,UAEe,UAA7BF,OAAOC,SAASC,UAEhBF,OAAOC,SAASC,SAASZ,MACvB,2DCZNa,IAASC,OAAO,kBAAC,EAAD,MAASC,SAASC,eAAe,SD2H3C,kBAAmBC,WACrBA,UAAUC,cAAcC,MAAMC,MAAK,SAAAC,GACjCA,EAAaC,kB","file":"static/js/main.fd758582.chunk.js","sourcesContent":["import React, { Component } from \"react\";\r\n\r\nconst CNN = () => (\r\n            <div className=\"container\">\r\n                <div className=\"row\"\r\n                style={{\r\n                    margin:\"5vh\"\r\n                }}\r\n                >\r\n                    <div className=\"col\">\r\n                    <h1>Everybody loves ... convolutions (Work in progress)</h1>\r\n                    <div\r\n                    style={{\r\n                        backgroundColor:\"#f2f2f2\",\r\n                        padding:\"10px\",\r\n                        margin:\"10px\",\r\n                        borderRadius:\"5px\"\r\n                    }}\r\n                    >\r\n                        In 3D, the following mental image may prove useful. Imagine two sheets of colored paper: \r\n                one red and one blue. Put one on top of the other. Now crumple them together into a small \r\n                ball. That crumpled paper ball is your input data, and each sheet of paper is a class of \r\n                data in a classification problem. What a neural network (or any other machine-learning model) \r\n                is meant to do is figure out a transformation of the paper ball that would uncrumple it, so \r\n                as to make the two classes cleanly separable again. With deep learning, this would be \r\n                implemented as a series of simple transformations of the 3D space, such as those you could \r\n                apply on the paper ball with your fingers, one movement at a time. \r\n\r\n                Uncrumpling paper balls is what machine learning is about: finding neat representations \r\n                for complex, highly folded data manifolds. At this point, you should have a pretty good \r\n                intuition as to why deep learning excels at this: it takes the approach of incrementally \r\n                decomposing a complicated geometric transformation into a long chain of elementary ones, \r\n                which is pretty much the strategy a human would follow to uncrumple a paper ball. \r\n                Each layer in a deep network applies a transformation that disentangles the data a \r\n                little - and a deep stack of layers makes tractable an extremely complicated \r\n                disentanglement process.\r\n\r\n                \r\n                \r\n                Conv layers look at spatially local patterns by applying the same geometric transformation to \r\n                different spatial locations (patches) in an input tensor. This results in a representation that \r\n                are translation invariant (does not matter where in the image the object occurs). Convnets consist \r\n                of stacks of convolution and max-pooling layers. The pooling layers let you spatially downsample the \r\n                data, which is required to keep feature maps to a reasonable size as the number of features grows, \r\n                and to allow subsequent convolution layers to see a greater spatial extent of the inputs. Convnets \r\n                are often ended with a `Flatten` operation or global pooling layer, turning spatial feature maps \r\n                into vectors, followed by `Dense` layers to achieve classification or regression.\r\n                <br></br><br></br>\r\n                <b>Deep Learning with Python page 44–5.</b>\r\n                    </div>\r\n                \r\n                \r\n                \r\n\r\n                <img src=\"https://i.stack.imgur.com/iY5n5.png\"\r\n                style={{\r\n                    width:\"50%\",\r\n                    height:\"30%\"\r\n                }}/>\r\n\r\n                <h2>What is a convolution?</h2> \r\n                <p>\r\n                It is a sliding window applied to a matrix, whether that be an image or feature map. At each element we multiply the \r\n                filter/kernel with the corresponding input pixel and then add them up. The same filter/kernel is slide across the input \r\n                which is called weight tying. Filters of the convolutions are learned, so the model learns which filter is best to apply \r\n                in each layer. An early layer of a convolution is likely, at best, to only detect very low level features such as edges. \r\n                This is because the filters are limited at earlier layers, whereas as we go deeper, the amount of output channels (which we set) \r\n                increases and later layers have several channels with which to predict collections of edges and orientations that make up a face or eye ball. \r\n                </p>\r\n                \r\n\r\n                <h2>Properties</h2> \r\n                <p>\r\n                Location Invariance and Compositionality. Say you want to spot a cat in an image. Because of this sliding window \r\n                approach you don't care where the cat occurs. The second aspect is (local) compositionality. Each filter composes \r\n                a local patch of lower-level features into higher-level representation. You build edges from pixels, shapes from edges, \r\n                eyes from circles and so on. You can derive more complex shapes and objects from lower level representations such as \r\n                edges, pixels, gradients etc and from several channels of these features.\r\n\r\n                Convolutions are translationally invariant because the filters slide over the image horizontally and vertically. \r\n                But they are not rotationally invariant because the filters don't rotate. Thus, the net seems to need several similar \r\n                filters in different orientations to detect objects and patterns that are differently oriented.\r\n                </p>\r\n                <img src=\"https://cdn-images-1.medium.com/max/1200/1*B41mvbzpZ7ythn5AlJWh-A.gif\" \r\n                style={{\r\n                    width:\"90%\"\r\n                }}/>\r\n                    </div>\r\n                </div>\r\n            </div>\r\n)\r\n\r\nexport default CNN\r\n\r\n","import React from 'react';\r\n\r\nconst Limitations = () => (\r\n  <div>\r\n    <h1>Limitations (Work in progress)</h1>\r\n    <p>\r\n    <h2>\r\n    <a href=\"http://faculty.neu.edu.cn/yury/AAI/Textbook/Deep%20Learning%20with%20Python.pdf\">\r\n        Heavily Inspired by this brilliant book</a>\r\n    </h2>\r\n\r\nDeep learning works by doing a series of geometric transformations on an input. And then, \r\ngiven some error signal, improves the geometric transformations iteratively to map some \r\ninput x to a correct output y. \r\n\r\n\"In deep learning, everything is a vector: everything is a point in a geometric space. \r\nModel inputs and targets are first vectorized: turned into an initial input vector space \r\nand target vector space. Each layer operates one simple geometric transformation on the \r\ndata that goes through it. The chain of layers forms one complex geometric trasnformation, \r\nbroken down into a series of simpler ones. This complex transformation attempts to map the \r\ninput space to the target space, one point at a time. This transformation is parameterized \r\nby weights of the layers, which are iteratively updated based on how well the model is \r\ncurrently performing. A key characteristic is that the geometric trasnformation must be \r\ndifferentiable, to be able to perform gradient descent. Thi means the geometric morphing \r\nfrom inputs to outputs must be smooth and continuous - a significant constraint.\"\r\n\r\n\"The entire process of applying this complex geometric transformation to the input data can be \r\nvisualized in 3D by imagining a person trying to uncrumple a paper ball: the crumpled paper ball \r\nis the manifold of the input data that the model starts with.\r\nEach movement operated by the person on the paper ball is similar to a simple geometric \r\ntransformation operated by one layer. The full uncrumpling gesture sequence is the complex \r\ntransformation of the entire model. Deep-learning models are mathematical machines for uncrumpling \r\ncomplicated manifolds of high-dimensional data.\r\n\r\nThat’s the magic of deep learning: turning meaning into vectors, into geometric spaces, and \r\nthen incrementally learning complex geometric transformations that map one space to another. \r\nAll you need are spaces of sufficiently high dimensionality in order to capture the full scope \r\nof the relationships found in the original data.\"\r\n\r\n\"The whole thing hinges on a single core idea: that meaning is derived from the pairwise \r\nrelationship between things (between words in a language, between pixels in an image) and \r\nthat these relationships can be captured by a distance function. If the brain works in \r\ngeometric spaces is a different question. Vector spaces are efficient to work with from a \r\ncomputational standpoint, but different data structures can be envisioned- like graphs. \r\nNeural networks initially emerged from using graphs (connectionism) but nowadays they have \r\nan incorrect meaning since they are neither neural nor networks. A more appropriate name is \r\nlayered representations learning or hierarchical representations learning, or deep differentiable \r\nmodels or chained geometric transforms.\"\r\n\r\n\"The space of applications is nearly infinite but many are completely out of reach for current \r\ndeep learning techniques. You cannot train a model to read a product description and generate \r\nthe appropriate codebase. Anything that requires reasoning, long-term planning, and algorithmic \r\ndata manipulation is out of reach for deep-learning models no matter how much data is given. \r\n\r\nThis is because a deep-learning model is just a chain of simple, continuous geometric \r\ntransformation mapping one vector space into another. All it can do is map one data manifold X \r\ninto another manifold Y, assuming the existence of a learnable continuous transform from X to Y.\"\r\n\r\n\"One real risk is overestimating/ anthropomorphizing deep-learning models and their abilities. \r\nA fundamental feature of humans is our theory of mind: our tendency to project intentions, beliefs, \r\nand knowledge on the things around us. Drawing a smiley face on a rock suddenly makes it happy-in \r\nour minds. Applied to deep learning, this means that, for instance, when we're able to train a \r\nmodel to generate captions to describe pictures and the captions it generates. Then we're \r\nsurprised when any slight departure from the sort of images present in the training data \r\ncauses the model to generate completely absurd captions. \r\n\r\nThis is highlighted by adversarial examples, which are samples ged to a deep-learning network \r\nthat are designed to trick the model into misclassifying them. It's possible to do gradient \r\nascent in input space to generate inputs that maximize the activation of some convnet filter-this \r\nis the basis of the filter-visualization technique. Similarly, through gradient ascent you can \r\nslightly modify an image to maximize the class prediction for a given class. By taking a \r\npicture of a panda and adding to it a gibbon gradient, we can get a neural network to classify \r\nthe panda as a gibbon. This evidences both the brittleness of these models and the deep \r\ndifference between their input-to-output mapping and our human perception.\"\r\n\r\n<img src=\"https://blog.keras.io/img/limitations-of-dl/adversarial_example.png\" />\r\n\"In short, deepl-learning models don't have any understanding of their input-not in the human \r\nsense. Our understanding of images, sounds etc is grounded in our sensimotor experience as humans. \r\nML models have no access to such experiences and thus can't understand their inputs in a human \r\nrelatable way. By annotating large numbers of training examples to feed to our models, we get \r\nthem to learn a geometric transform that maps data to human concepts on a specific set of examples, \r\nbut this mapping is a simplistic sketch of the original model in our minds-the one developed from \r\nour experience as embodied agents.\r\n\r\nNever fall into the trap of believing that neural networks understand the task they perform. \r\nThey were trained on a different, far narrower task than the one we wanted to teach them: that of \r\nmapping training inputs to training targets, point by point. Show them anything that deviates from \r\ntheir training data, and they will break.\"\r\n\r\n\"Humans are capable of far more than mapping immediate stimuli to immediate\r\nresponses, as a deep network, or maybe an insect, would. We maintain complex, abstract\r\nmodels of our current situation, of ourselves, and of other people, and can use these\r\nmodels to anticipate different possible futures and perform long-term planning. We\r\ncan merge together known concepts to represent something we’ve never experienced \r\nbefore-like picturing a horse wearing jeans, for instance. This ability to hypothecize, \r\nto expand our mental model beyond what we can experience directly-to perform abstraction \r\nand reasoning-is arguably the defining characterstic of human cognition. \r\nExtreme generalization: an ability to adapt to novel, never-before experienced \r\nsituations using little data or no new data. This stands in sharp contrast with deep nets, \r\nlocal generalization. The mapping from inputs to outputs performed by deep net quickly \r\nstops making sense if new inputs deviate from what was seen during training. Consider, \r\nfor instance, the problem of learning the appropriate launch parameters to get a rocket \r\nto land on the moon. If you used a deep net for this task and trained\r\nit using supervised learning or reinforcement learning, you’d have to feed it thousands\r\nor even millions of launch trials: you’d need to expose it to a dense sampling of the input\r\nspace, in order for it to learn a reliable mapping from input space to output space. In\r\ncontrast, as humans we can use our power of abstraction to come up with physical \r\nmodels—rocket science—and derive an exact solution that will land the rocket on the moon\r\nin one or a few trials. Similarly, if you developed a deep net controlling a human body,\r\nand you wanted it to learn to safely navigate a city without getting hit by cars, the net\r\nwould have to die many thousands of times in various situations until it could infer that\r\ncars are dangerous, and develop appropriate avoidance behaviors. Dropped into a new\r\ncity, the net would have to relearn most of what it knows. On the other hand, humans\r\nare able to learn safe behaviors without having to die even once—again, thanks to our\r\npower of abstract modeling of hypothetical situations.\"\r\n\r\n\"In short, despite our progress on machine perception, we’re still far from humanlevel AI. \r\nOur models can only perform local generalization, adapting to new situations that must be \r\nsimilar to past data, whereas human cognition is capable of extreme generalization, quickly \r\nadapting to radically novel situations and planning\r\nfor long-term future situations.\"\r\n</p>\r\n  </div>\r\n);\r\n\r\nexport default Limitations;","import React from 'react';\r\nimport {  Link } from 'react-router-dom'\r\n\r\nconst NavBar = () => (\r\n    <nav class=\"navbar navbar-expand-lg navbar-light bg-light\">\r\n        <a class=\"navbar-brand\" href=\"/zine-blog\">DL Zine's</a>\r\n        <button class=\"navbar-toggler\" type=\"button\" data-toggle=\"collapse\" data-target=\"#navbarNav\" aria-controls=\"navbarNav\" aria-expanded=\"false\" aria-label=\"Toggle navigation\">\r\n            <span class=\"navbar-toggler-icon\"></span>\r\n        </button>\r\n\r\n        <div class=\"collapse navbar-collapse\" id=\"navbarNav\">\r\n            <ul class=\"navbar-nav\">\r\n            <li class=\"nav-item active\">\r\n                <a class=\"nav-link\" href=\"/zine-blog\">Home <span class=\"sr-only\">(current)</span></a>\r\n            </li>\r\n            <li class=\"nav-item\">\r\n                <Link class=\"nav-link\" to=\"/zine-blog/topics\">Topics</Link>\r\n                \r\n            </li>\r\n            </ul>\r\n        </div>\r\n    </nav>\r\n);\r\n\r\nexport default NavBar;\r\n","import React from 'react';\r\nimport { Link } from 'react-router-dom';\r\n\r\nconst Topics = ({ match }) => (\r\n  <div\r\n  style={{\r\n    margin:\"5vh\"\r\n}}>\r\n\r\n\r\n\r\n    <h1>Topics</h1>\r\n\r\n\r\n\r\n    <div class=\"row\"style={{\r\n                    margin:\"5vh\"\r\n                }}>\r\n      <div class=\"col-sm-4\">\r\n        <div class=\"card\">\r\n          <div class=\"card-body\">\r\n            <h5 class=\"card-title\">Everybody loves Convolutions...</h5>\r\n            <p class=\"card-text\">The focus here is on convolutional neural networks,\r\n            the convolution operation and how it relates to neural nets, \r\n            seeing convolution as just matrix multiplication and more!</p>\r\n            <Link to=\"/cnns\" class=\"btn btn-primary\">Read</Link>\r\n          </div>\r\n        </div>\r\n      </div>\r\n      <div class=\"col-sm-4\">\r\n        <div class=\"card\">\r\n          <div class=\"card-body\">\r\n            <h5 class=\"card-title\">Limitations</h5>\r\n            <p class=\"card-text\">The aim of this post is to learn about the Limitations\r\n            of neural networks before you embark on some new project so you can avoid\r\n            wasting time.</p>\r\n            <Link to=\"/limitations\" class=\"btn btn-primary\">Read</Link>\r\n          </div>\r\n        </div>\r\n      </div>\r\n      <div class=\"col-sm-4\">\r\n        <div class=\"card\">\r\n          <div class=\"card-body\">\r\n            <h5 class=\"card-title\">SGD</h5>\r\n            <p class=\"card-text\"></p>\r\n            <Link to=\"/sgd\" class=\"btn btn-primary\">Read</Link>\r\n          </div>\r\n        </div>\r\n      </div>\r\n    </div>\r\n\r\n\r\n    <div class=\"row\"style={{\r\n                    margin:\"5vh\"\r\n                }}>\r\n      <div class=\"col-sm-4\">\r\n        <div class=\"card\">\r\n          <div class=\"card-body\">\r\n            <h5 class=\"card-title\">Batch Norm</h5>\r\n            <p class=\"card-text\"></p>\r\n            <Link to=\"/batch_norm\" class=\"btn btn-primary\">Read</Link>\r\n          </div>\r\n        </div>\r\n      </div>\r\n      <div class=\"col-sm-4\">\r\n        <div class=\"card\">\r\n          <div class=\"card-body\">\r\n            <h5 class=\"card-title\">Probability and Information Theory</h5>\r\n            <p class=\"card-text\"></p>\r\n            <Link to=\"/#\" class=\"btn btn-primary\">Read</Link>\r\n          </div>\r\n        </div>\r\n      </div>\r\n      <div class=\"col-sm-4\">\r\n        <div class=\"card\">\r\n          <div class=\"card-body\">\r\n            <h5 class=\"card-title\">Linear Algebra for ML</h5>\r\n            <p class=\"card-text\"></p>\r\n            <Link to=\"/linear_algebra\" class=\"btn btn-primary\">Read</Link>\r\n          </div>\r\n        </div>\r\n      </div>\r\n    </div>\r\n\r\n\r\n\r\n  </div>\r\n);\r\n\r\nexport default Topics;","import React from 'react';\r\n\r\nconst Home = () => (\r\n  <div>\r\n  <h1>Home</h1>\r\n  <p>\r\n    This site is designed to produce helpful content for people trying to learn deep learning and\r\n    for me to formalize my understanding. I try and take work from brilliant educators and \r\n    practioners and condense and describe them in a simplistic way to enable new comers to learn\r\n    without being intimidated.\r\n  </p>\r\n</div>\r\n);\r\n\r\nexport default Home;\r\n","import React from \"react\"\r\n\r\nconst SGD = () => (\r\n    <div\r\n    style={{\r\n        margin:\"5vh\"\r\n    }}>\r\n        <h1>Stochastic Gradient Descent</h1>\r\n        <h2>How do you create a predictor function as a function of pixel values?</h2>\r\n        <p>\r\n            What does this mean? Well the broader question here is how does a simple series\r\n            of matrix multiplications result in a good prediction based on its input. \r\n\r\n            <br></br>\r\n            <img src=\"https://latex.codecogs.com/gif.latex?%24y%3Dmx&plus;b%24\"></img>\r\n            <br></br>\r\n            <img src=\"https://latex.codecogs.com/gif.latex?%24y%3Da_1x_1&plus;a_2*x_2%24\"></img>\r\n        </p>\r\n    </div>\r\n)\r\n\r\nexport default SGD","import React from \"react\"\r\n\r\nconst BatchNorm = () => (\r\n    <div style={{\r\n        margin:\"5vh\"\r\n    }}>\r\n        <h1>Batch Norm</h1>\r\n\r\n        <h3>What is Batch Normalization? It accelerates training by reducing internal covariate shift. \r\n            ... Or does it?\r\n        </h3>\r\n        <h3>How does it work?</h3>\r\n        <p>\r\n        Batch norm removes the mean and and divides by the standard deviation of a \r\n        channel of activations. \r\n        <br></br>\r\n        Insert Channel Activation picture\r\n        <br></br>\r\n        <img src=\"https://miro.medium.com/max/810/1*Hiq-rLFGDpESpr8QNsJ1jg.png\"></img>\r\n        <br/>\r\n        \"How does batch normalization help optimization? Not by reducing internal covariate shift. \r\n        What it does do, is allow you to increase your learning rate. The standard training without \r\n        batch norm has a much more bumpier loss landscape but with batch norm there are less big bumps. \r\n        So there is less risk of divergence. It is simple, it takes a mini-batch and it’s activations. \r\n        Firstly, we find the mean of the activativations, then the variance. Then we normalize, \r\n        so element-wise activation values minus the mean divided by the standard deviation. The \r\n        really important bit is: we take those normalized activation values and we add a vector \r\n        of biases. Then we use something that’s like a bias but instead we multiply by it. We \r\n        have a gamma which we multiply by those normalized activations and add a bias beta. \r\n        These two parameters are learnable parameters. The value of our predictions \r\n        y_hat = f(w_1, w_2,...,w_n, X) is some function of our weights (there could be millions) \r\n        and our input. This is our neural net function. Our loss: L = Sum(y_hat-y)^2. We want to \r\n        predict outcomes between 1 and 5. Our activations and our final layer are between -1 and 1, \r\n        and they’re way off. The scale is off the mean is off. One thing we can do is try to come \r\n        up with new weights to cause the spread and mean to increase. But this is hard since all \r\n        these weights interact in specific ways. So that will take ages. But what if we amended \r\n        that neural net function: y_hat = f(w_1, w_2, …, w_n, X) * gamma + beta. These are two \r\n        more parameter vectors, so this is really easy. To increase the scale or change the mean \r\n        we have these two parameters and we have their gradient to increase the scale/mean. That \r\n        is what batch norm does. It makes it easier to shift the outputs up and down etc. \r\n        In practice, we don’t use a different mean and standard deviation for each mini-batch \r\n        since it would vary so greatly. So instead we take the exponentially weighted moving \r\n        average of the mean and standard deviation. They use the same technique as they did for \r\n        adam to calculate this. You can vary the amount of momentum in the batch norm layer. The \r\n        smaller the momentum the less the standard deviation and mean will vary less from mini-batch \r\n        to mini-batch and will have less of a regularising effect.\"\"\r\n        </p>\r\n        <p>\r\n        The beta is a learnable mean and gamma the standard deviation. \r\n        This reparameterises the network. Batch Norm stabilises optimsation \r\n        allowing much higher learning rates (by reducing the peaks of the loss landscape), \r\n        it injects noise through basic statistics to improve generalisation, it reduces \r\n        sensitivity to weight initialisation and it interacts with weight decay to control \r\n        learning rate dynamics.\r\n        \r\n<img \r\nsrc=\"https://296830-909578-raikfcquaxqncofqfm.stackpathdns.com/wp-content/uploads/2019/06/Artboard-1-7-1-1.svg\" /> \r\n\r\nIt is evident that the network with batch norm is stable over a much larger range of \r\nlearning rates (in log scale). This accelerates training as claimed in the original paper \r\nas we can safely use much higher learning rates. \r\n\r\n<h3>Drawbacks</h3> \r\n* Batch norm is slow, it's different at training and test time, \r\nit's ineffective for small batches and various layer types, \r\nit has multiple interacting effects which are hard to separate. \r\n\r\nMonitoring histograms of channel activations is rarely done but essential. \r\nEach channel has a different colour.\r\n\r\n<img src=\"https://296830-909578-raikfcquaxqncofqfm.stackpathdns.com/wp-content/uploads/2019/06/before_after_0.svg\" />\r\n\r\n\r\n<h3>Resources:</h3> \r\n<ul>\r\n    <li><a href=\"https://myrtle.ai/how-to-train-your-resnet-7-batch-norm/\">Myrtle.ai</a></li>\r\n    <li><a href=\"https://arxiv.org/pdf/1805.11604.pdf\">How Does Batch Normalization Help Optimization?</a></li>\r\n\r\n</ul>\r\n\r\n\r\n        </p>\r\n\r\n        <p>\r\n        Batch Normalization (Szegedy): Batch Norm doesn’t reduce Covariate Shift nor is it \r\n        why how it works? What it does is, it allows you to increase your learning rate and \r\n        the loss is less bumpy. The weights used to jump off an awful part of the weight space \r\n        it can’t recover from. The algorithm takes a mini-batch and takes in some activations \r\n        from a layer. Find the mean of those activations and variance. Then normalize, the \r\n        values minus mean divided by standard deviation. Important bit: take those values and \r\n        add a vector of bias’ (beta) and times a gamma (both learnable params). The value of \r\n        our predictions is a function of our weights and inputs to our layer i.e. our NN function.\r\n        Our loss is our actuals minus predicted squared. E.g. lets say our actuals are between 1-5 \r\n        but our predictinos are -1-1. But our weights are intertwined in this landscape. Why not: \r\n        add a *gamma + b to our NN function. We now have a direct gradient to change the scale and \r\n        the mean. Makes it easier to shift the outputs up and down and in and out. \r\n        So y_hat = F(w1, w2, w3, …. Wm, X) * gamma + b. We take the exponentially \r\n        weighted moving averages of the mean and standard deviation. \r\n            \r\n        </p>\r\n    </div>\r\n)\r\n\r\nexport default BatchNorm","import React from \"react\"\r\n\r\nconst LinearAlgebra = () => (\r\n    <div>\r\n        <h1>Linear Algebra for ML</h1>\r\n\r\n        <div>\r\n            <h3>Linear Systems of Equations</h3>\r\n            <p>\r\n                How do we solve for a system of linear equations? \r\n                If I saw this title or if I saw this in a maths book without \r\n                much clarification what would this mean to me?\r\n                When we have two or more linear equations, this is a system of linear equations. \r\n                Put simply, it's when we have two or more equations that define a line. If we \r\n                graph the relationship between those lines then we can define their relationship \r\n                to one another. Say those two lines intersect? Well then there is exactly one \r\n                solution to this system of linear equations. If they do no intersect then there \r\n                is no solutions to the system of equations. \r\n                If they lie on the same line there are an infinite amount of solutions.\r\n                <br></br><br></br>\r\n                <img src=\"https://www.mathsisfun.com/algebra/images/system-linear-types.svg\"></img>\r\n                <br></br><br></br>\r\n            </p>\r\n        </div>\r\n         We need to find the values of the variables x1, .., xn that satisfies all equations, that is to say the values taken by the unknowns.\r\nName the 3 possible cases a system of linear of equations Ax = b must be? No solution, One solution or infinite number of solutions.\r\nWhy cant there be more than one solution and less than infinite? Because if you visualize it in R2, a solution corresponds to the intersection of two lines so two lines will either never meet – no solution (parallel), they intersect i.e. 1 solution or are linearly dependent – infinite number of solutions (superimposed).\r\nWrite a proof for it? Say Ax = b and Ay = b and Az = b with z being the solution. So Az = A(alphax + ( 1 – alpha)y) which = Axalpha + Ay( 1 – alpha), And since Ax = Ay = b. This leads to Az = balpha + b(1-alpha) which = balpha + b – balpha which = b. So z is the solution. \r\nCan Ax = b be represented by a matrix A? Yes matrix A contains all the weights of each variable in the vector x which equals = b. This corresponds to a set of linear equations. \r\nThe number of columns of A is the number of dimensions of our vector space and the number n of directions we can travel in. The number of solutions of our linear system corresponds to the number of ways we can reach b by travelling through our n dimensions. \r\nWhat is an overdetermined system? It is when we have more equations m than unknowns n. E.g. say we have three lines that do not intersect all together. \r\nWhat is an undetermined system? If there is more unknowns than equations. E.g. there is only 1 equation i.e. 1 line and 2 dimensions, each point on that line is a solution therefore there is an infinite number of solutions. \r\nWhat is a linear combination? The linear combination of 2 vectors is their weighted sum. E.g. u = [1, 3] and v = [2, 1]. The linear combination of them is au + bu = a [1, 3] + b[2, 1] with a and b being the weights of the vectors. E.g. if a = 2 and b = 1 we get [4, 7]. \r\nWhat is Span? Well it is all the points that the linear combination of the 2 vectors above can reach by changing the combinations of a and b. \r\nWhat is a space? The space of a vector is all the values that can be taken by this vector denoted as R. Rn is the n amount of dimensions. \r\nWhat is a subspace? If you take a 2-d plane in R3, this plane is a subspace of your original R3 space. So a line in your R2 space, this line is a subspace of your original space. The linear combination of vectors gives vectors in the original space.\r\nWhat is the column form? You can represent a set of equations by considering the solution of b as corresponding to a linear combination of each column multiplied by their weights. So weights times the variables. We travel from the origin to the point of the coordinates of b. The columns of AA give us the directions we can travel by and their weights are the length of the way in that direction.\r\nWhat is linear dependence? Columns are linearly dependent if one of them is a linear combination of others. The direction of two linearly dependent vectors is the same. The row picture shows the lines are parallel, and the column picture shows them on the same line. \r\nWhat is a hyperplane? A hyper plane is n-1 dimensional. \r\nWhat is a Norm? Any function with the following properties: Non-negative (it is a length thus can’t be negative), Norms are 0 if and only if vectors are zero vectors, Norms respect the triangle inequality and the norm of a vector multiplied by a scalar is equal to the absolute value of this scalar multiplied by the norm of the vector. ||k . u|| = |k| . ||u||. It is usually written with two horizontal bars ||u||. Absolute value is a magnitude, can be thought of as the distance from the origin. \r\nWhat is the triangle inequality? The norm of the sum of some vectors is less than or equal to the sum of the norms of these vectors. ∥u+v∥≤∥u∥+∥v∥\r\nGeometrically this means the shortest path between two points is a line.\r\nHow do you get the p-norm of a vector? Calculate the absolute value of each element, take the power p of these absolute values, sum all these powered absolute value, take the power 1/p of this result. ∥x∥p=(∑i|xi|p)1/p\r\nSo the P in p norm is the power of e.g. L0, L1, L2. \r\nWhat is the L0 norm? All positive values give you a 1 if you calculate its power as 0 except 0 which gives you 0. This norm tells you the number of non-zero elements in the vector. Not really a norm as if you multiply a vector by alpha, you get the same number back. \r\nWhat is the L1 norm? P=1 so this norm is simply the sum of the absolute values. ∥x∥1=∑i|xi|\r\nWhat is the L2 norm? The Euclidean norm is p=2. ∥x∥2=(∑ix2i)1/2⇔√∑ix2i\r\nThe Euclidean norm corresponds to the length of the vector from origin to point. E.g. u=[3,4] So ||u||2 = sqrt |3|2 + |4|2 which = sqrt 25 which = 5 so the L2 norm is 5. \r\nWhat is the squared Euclidean norm? It is convenient since it removes negatives and removes the square root and we end up with the simple sum of every squared values of the vector. It is widely used because it can be calculated with xTx. \r\nWhat is the derivative of the squared L2 norm? Its partial derivatives are easy to calculate. The derivative of the L2 is complicated as it takes every element of the vector into account. However, the squared L2 norm barely discriminates between 0 and small values because the increase of the function is slow. \r\nWhat is the max norm? It is the Linfinity norm and corresponds to the absolute value of the greatest element of the vector. ∥x∥∞=maxi|xi|\r\nWhat is the frobenius norm? It is the same as the L2 norm of the matrix after flattening. It’s the square root of the L2 norm of a matrix. \r\nWhat are dot products with norms? I believe they give you the cosine angle of vectors. xTy=∥x∥2⋅∥y∥2cosθ\r\nE.g. x = [0,2] and y=[2,2]. We see that θ is equal to 45 degrees. Xty = [0, 2] . [2,\r\n2] = 0 x 2 + 2 x 2 = 4\r\nand \r\n∥x∥2=√02+22 = √4 = 2\r\n∥y∥2=√22+22 = 8\r\n2 × √ 8 × cos(45) = 4\r\n\r\nWhat are diagonal matrices? A matrix Ai,j is diagonal if its entries are all zeros except on the diagonal when I = j. A diagonal matrix is denoted diag(v) where v is the vector containing the diagonal values. The multiplication of a diagonal matrix and a vector is as you’d expect. The invert of a square diagonal matrix exists if all entries of the diagonal are non-zero. \r\nWhat is a symmetric matrix? A matrix A is symmetric if it is equal to its transpose i.e. A = AT . \r\nWhat is a unit vector? A unit vector is a vector of length 1 denoted u_hat. \r\nWhat is an orthogonal vector? Two orthogonal vectors are separated by a 90 degree angle. Their dot product is 0. \r\nWhat is orthonormal? When the norm of orthogonal vectors is the unit norm. It is impossible to have more than n vectors mutually orthogonal in Rn. They won’t be mutually orthogonal i.e. wont share the 90 degree angle. \r\nWhat is unit norm? The unit vector obtained by normalizing the normal vector (i.e., dividing a nonzero normal vector by its vector norm) is the unit normal vector, often known simply as the \"unit normal.\"\r\nWhat is an orthogonal matrix? A matrix is orthogonal if columns are mutually orthogonal and have a unit norm (orthonormal) and rows are mutually orthonormal and have unit norm. E.g. A1,1 , A2,1 and A1,2 , A2,2 are orthogonal vectors and the rows A1,1 A1,2 and A2,1 A2,2 are orthogonal vectors. The rows and columns are orthogonal vectors. \r\nName some properties of an orthogonal matrix? ATA = AAT = I. E.g. A = a,b,c,d and thus A transpose is a,c,b,d and their product is: \r\n\r\nThe norm of a vector [a,c] is a2 + c2. The rows of A have unit norm because A is orthogonal. This means that a2 + c2 = 1 and b2 + d2 = 1. So we have:\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nName another property? AT = A-1. We can show that if ATA = I then AT = A-1. If we multiply each side of the equation by A inverse we have. (ATA)A-1=IA-1. When a matrix or vector is multiplied by the identity matrix it doesn’t change. So we have (ATA)A-1=IA-1 and that matrix multiplication is associate so we can remove the paranthesis: ATAA-1=IA-1 , we also know AA-1 = I so we can replace ATI=A-1. This shows that AT=A-1. \r\n\r\nWhat is eigen decomposition? It is one form of matrix decomposition. Decomposing a matrix means we want to find the product of matrices that is equal to the initial matrix. We decompose the initial matrix into the product of its eigenvectors and eigenvalues. We can think of matrices as linear transformations. Some matrices will rotate your space, others rescale. So when we apply a matrix to a vector we end up with a transformed version of the vector. By apply we mean we calculate the dot product of the matrix with the vector. \r\n\r\nWhat are Eigenvectors and Eigenvalues? Imagine that the transformation of the initial vector gives a new vector that has the same direction. The scale can be different but in the same direction. Applying the matrix didn’t change the direction. This special vector is called an eigenvector of the matrix. Finding these eigenvectors can be useful. This means that v is an eigenvector of A if v and Av are in the same direction. That is to say that the vectors Av and v are parallel. The output vector is just a scaled version of the input vector. The scaling factor λ is called the eigenvalue of A. Av=λv. The eigenvector of matrix A is equivalent to the scaling factor of the initial vector v. E.g. A = [5,1][3,3] and we know that one eigenvector of A is v = [1,1] we can check that Av=λv: by A dot v = [6,6]. We can see that 6 x [1,1] = [6,6] which means v is an eigenvector of A and the eigenvalue is 6.  If v is an eigenvector of A then any rescaled vector sv is also an eigenvector of A. \r\nHow do we decompose a matrix using eigenvectors and eigenvalues? All eigenvectors of a matrix A can be concatenated in a matrix with each column corresponding to each eigenvector. The vector of eigenvalues can be created from this. The eigen decomposition is given by: A=V⋅diag(λ)⋅V−1. Diag(v) is a diagonal matrix containing all the eigenvalues. E.g. V = [1,1][1,-3] and the diagonal matrix is all zeros except the diagonal that being the vector of our eigenvalues: diag(v)=[6,0][0,2]. The inverse of V = [0.75, 0.25][0.25, -0.25]. Lets plug it into our equation on the right: \r\nA is the original matrix shown above. \r\n\r\nWhat is a real symmetric matrix? A=QΛQT where Q is the matrix with eigenvectors as columns and Λ is the diag(λ). \r\n Symmetric matrices are useful for this!\r\n\r\n\r\n\r\nHow can eigendecomposition be used for quadratic forms? They can be used to optimize quadratic functions. When x takes the values of an eigenvector, f(x) takes the values of its eigenvalue. \r\nWith x = x1, x2 and A = a,b,c,d we call them matrix forms. This form is used to do various things on the quadratic equation like constrained optimization. E.g a gives you the number x21, (b+c) the number x1x2 and d the number x22. This means that the same quadratic form can be obtained from infinite number of matrices A by changing b and c while preserving their sum.  See :\r\n\r\nWhat is linear substitution? A change of variable means we replace a variable with another. It can be used to remove cross terms (a quadratic form without a cross term is called diagonal form since it comes from a diagonal matrix) in our equation. Without the cross term its easier to characterize and optimize the function (finding max or minimum). \r\nThe change of variable will end up with a new equation but finding a specific substitute can simplify our statement (equation), and can be used to get rid of the cross term. The correct substitution is given by the eigenvectors of the matrix used to generate the quadratic form. E.g. x = x1, x2 and A = 6,2,2,3 and the eigenvectors of A are 0.89442719, 0.4472136, 0.4472136, 0.89442719, which can be replaced with 1/sqrt(5)[2,-1][1,2] (https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.7-Eigendecomposition/). The first eigen vector is 1/sqrt(5)[2,1] and second is 1/sqrt(5)[-1,2]. Thus see below:\r\n \r\nWhat is principal axes theorem? A simpler way to do the change of variable. Recall f(x)=xTAx and the linear substitution can be written in these terms. We want to replace the variables x by y that relates by: x=Py and we want to find P such as our new equation after changing the variable doesn’t contain the cross terms. Start by replacing this in the first equation xTAx=(Py)TA(Py)=yT(PTAP)y. We also know A is symmetric, so the diagonal matrix D contains the eigen vectors of A such that D = PTAP. We end up with xTAx = yTDy. This implies we can use D to simplify our quadratic and remove the cross terms. \r\nTo summarise, the principal axes form can be found with: xTAx=λ1y21+λ2y22 where λ1λ1 is the eigenvalue corresponding to the first eigenvector and λ2λ2 the eigenvalue corresponding to the second eigenvector (second column of x).\r\nHow do you find f(x) with eigendecomposition? You can with the eigenvectors/values when x is a unit vector: so f(x) = xTAx we know that if x is an eigenvector of A and λ the eigenvalue then Ax = λx and replace the term we get xTλx = xTx λ. Since x is a unit vector (||x||2 = 1 or xTx =1) we end up with f(x) = λ. If x is an eigenvector of A, f(x) = xTAx will take the value of the corresponding eigenvalue. Thus, this only works if the Euclidean norm of x is 1 a unit vector. \r\nWhat is quadratic form optimisation? Optimisation is finding the max or min of a function. Used to minimize the cost function. Eigendecomposition can be used to optimize quadratic functions and can be done without cross terms. The difficulty is we want a constrained optimization, that is to find the min or max of the function for f(x) being a unit vector. Thus, we want to optimize f(x) = xTAx subject to ||x||2 =1. \r\n The minimum of f(x) is the minimum eigenvalue of the corresponding matrix A. This value is obtained when x takes the value of the corresponding eigenvector. \r\n\r\n\r\n\r\nWe saw that the quadratic function can be represented by the symmetric matrix A. These functions can take one of three general shapes. \r\nWith the constraint that x is a unit vector, the minimum of the function f(x) corresponds to smallest eigenvalue and its eigenvector. The max is the biggest eigenvalue and its eigenvector. Eigendecomposition can’t be used for non-square matrices this is where Singular Value Decomposition comes in. \r\nPlease summarize? Linear transformations transform an vector with operations like rotation. Every transformations can be seen as applying a matrix on the input vector. \r\nWhat is Singular Value Decomposition? Eigendecomposition cant decompose square matrices so we need SVD for that. So we need to decompose matrix A into 3 matrices instead of 2 with ED. The matrices U, D and V have the following properties: U and V are orthogonal matrices, and D is a diagonal matrix but not necessarily square. The columns of U are the left-singular vectors of A while the columns of V are the right-singular vectors of A. The values along the diagonal of D are the singular values of A. Right is the dimensions of the factorization. A is a matrix that can be seen as a linear transformation, this can be decomposed in three sub-transformations: 1. Rotation, 2. Re-scaling, 3. Rotation. These three steps correspond to the three matrices U, D, and V. Every matrix is a linear transformation of a vector or another matrix. If we apply a matrix to a unit circle it will expand/contract/shift this circle. The general rule is that the transformation associated with diagonal matrices imply only a rescaling of each coordinate without rotation. Matrices that are not diagonal can produce a rotation. Vectors/matrices can be transformed by rotating or scaling matrices. The SVD can be seen as the decomposition of one complex transformation in 3 simpler transformations(a rotation, a scaling and another rotation). With non-square matrices we e.g. may take a 3 by 2 matrix will map a 2D space to 3D.   \r\nWhat are the three transformations? We use the three transformations of the SVD as equivalent to the transformation done with the original matrix. \r\nWhat are the singular values? In descending order, they are a new set of features (that are a linear combination of the original features) with the first feature explaining most of the variance. The major axis of the ellipse will be the first left singular value (u1) and its norm will be the first singular value (sigma 1). They are the major (sigma 1 u1) and minor (sigma 2 u2) axes of the ellipse and the major axis is associated with more variance. \r\nHow are the sub-transformations found? U, D and V can be found by transforming A in a square matrix and computing the eigenvectors of this square matrix. The square matrix can be obtained by multiplying the matrix A by its transpose. U=eigenvectors of AAT, V = eigenvectors of ATA and D = eigenvalues of AAT and ATA. E.g. A is non-square matrix = [7,2][3,4][5,3]. The left singular values of A = eigenvectors of AAT. The right singular values of A correspond to the eigenvectors of ATA. The nonzero singular values of A are the square roots of the eigenvalues of ATA and AAT. \r\nWhat is the Moore-Penrose Pseudoinverse? This is the direct application of the SVD. The case where the set of equations have 0 or many solutions the inverse cannot be found and the equation cannot be solved. The pseudoinverse is A+ such as AA+ approaches In minimizing the ||AA+ - In||2. The following formula is used to find the pseudoinverse: ||AA+ - In||2. A+ is the pseudoinverse of A and D+ the pseudoinverse of D. D is the diagonal matrix thus D+ can be calculated by taking the reciprocal (the quantity obtained by dividing the number one by a given quantity) of the non-zero values of D. E.g. A is non-square matrix = [7,2][3,4][5,3], lets calculate its SVD and then pseudoinverse. \r\nAnother way to compute the pseudo is to use this formula: (ATA)-1AT. \r\nHow can you use the pseudo to solve an overdetermined system of linear equations? In general, there is no solution to overdetermined systems, no point of intersection of the three lines. The pseudo solves the system in the least square error perspective: finds the solution that minimizes the error. This gives you the point at the intersection of the three symmedians of the triangle. This same method can be used to fit a line to a set of points. We have a set of x and y co-ordinates and we want to find a line y = mx+b that minimizes the error. The error is the sum of the differences between the fit and the actual data points. The matrix A are the values of the coefficients. \r\nWhat is the trace operator? The trace is the sum of all values in the diagonal of a square matrix. The trace can be used to specify the Frobenius norm of a matrix, that norm being equivalent to the L2 norm for matrices (take the square of all the elements then sum them and then square root the result).  Since the transpose of a matrix doesn’t change the diagonal, the trace of the matrix is equal to the trace of its transpose. \r\nWhat is the trace of a product? Tr(ABC) = Tr(CAB) = Tr(BCA)\r\nWhat is the determinant? The determinant of a matrix A is a number corresponding to the multiplicative change you get when you transform your space with this matrix. A negative determinant means there is a change in orientation. A change in orientation means for instance in 2D we take a plane out of these 2 dimensions, do some transformations and get back to the initial 2D space. The second transformation cant be obtained through rotation and rescaling. Thus the sign can tell you the nature of the transformation associated with the matrix. The determinant also gives you the amount of transformation. The absolute value of the determinant corresponds to the area of the transformed figure. We have two vectors 0,1 and 1,0 and apply matrix a 2,0 0,2 which is a diagonal matrix. This will rescale our space, without any rotation. It will rescale each dimension because the diagonal values are the same. The determinant of A is 4. The transformation has multiplied the area of the unit square by 4, the lengths of the newi and newj are 2 thus 2*2=4. \r\nWhat is Principal components analysis? Dimensions are crucial in data science, the dimensions are all the features (columns) of the dataset. Dimensions = columns, two dimensions easy to plot, >3 harder to visualize. With n amount of dimensions, some might be correlated. One way to reduce dimensionality is to simply keep only some of them, but you loose good information. It would be nice to reduce these dimensions while keeping all the information present in the dataset. The aim of PCA is reduce the number of dimensions of a dataset where the dimensions are not completely decorrelated. PCA provides a new set of dimensions the principal components. They are ordered: the PC is the dimension having the largest variance. Each PC is orthogonal to the preceding one. Remember that orthogonal vectors means that their dot product is equal to 0. This means each PC is decorrelated to the preceding one and is better than feature selection.  Unit vectors are an example of orthogonal vectors: \r\n\r\nThe problem can be expressed as finding a function that converts a set of data points from Rn to Rl. This means we change the number of dimensions of the dataset. We also need a function that can decode back from the transformed dataset to the initial one: \r\nThe encoding function f(x) transforms x into c and the decoding function transforms back c into an approximation of x. PCA will respect some constraints: 1. The decoding function has to be a simple matrix multiplication g(c) = Dc. By applying the matrix D to the dataset from the new coordinate system we should get back to the initial coordinate system. 2. The columns of D must be orthogonal. 3. Columns of D must have unit norm. We want a decoding function which is a simple matmul. We will then find the encoding function from the decoding function. We want to minimize the error between the decoded data point and actual data point i.e. reducing the distance between x and g(c) using squared L2 norm: ||x-g(c)||22. Lets call c* the optimal c. C* = argminc ||x-g(c)||22. L2 norm can be expressed as ||y||22 = yTy, we have named y to avoid confusion with x, here y=x-g(c). Thus the equation that we want to minimize becomes: (x-g(c))T(x-g(c)) and transpose respects addition we have (x T-g(c) T) (x-g(c)). The distributive property: xTx−xTg(c)−g(c)Tx+g(c)Tg(c), the commutative property tells us xTy=yTx. So the equation becomes: xTx−2xTg(c)+g(c)Tg(c). DON’T UNDERSTAND BEFORE MINIMIZING THE FUNCTION BIT (https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.12-Example-Principal-Components-Analysis/). But essentially we remove the xTx from the c*=argminc -2xTg(c)+g(c)Tg(c) and since g(c ) = Dc we replace g(c ) with Dc. With (Dc)T = cTDT we have c*=argminc - 2xTDc+cTDTDc. DTD=It because D is orthogonal and their columns have unit norm. We replace the equation: -2xTDc+cTIlc with -2xTDc+cTc. \r\nHow do we find the minimum of -2xTDc+cTc? Use gradient descent. The main idea of the derivative of the function at a specific value of x tells you if you need to increase or decrease x to reach the min. \r\nThe gradient is a vector containing the partial derivatives of all dimensions. Its notation is ∇xf(x). \r\nHow do we calculate the gradient of a function? Here we want to min through each dim of c. We want a slope of 0. ∇c(−2xTDc+cTc)=0. Lets take these terms separately to calculate the derivative according to c. d(−2xTDc)/dc = -2xTD. \r\n\r\nGreat we found the encoding function the dimensions are below: \r\n\r\n\r\n\r\nAnd to go back from c to x we use g(c ) = Dc: r(x)=g(f(x)=DDTx.  \r\n\r\n\r\n\r\n\r\n\r\nThe next step is to find matrix D. Recall the purpose of PCA is to change the coordinate system in order to maximize the variance along the first dimensions of the projected space. This is equivalent to minimizing the error between data points and their reconstruction. Maximizing the variance is equal to minimizing the error of the reconstruction. Since we have to take all points into account we use the Frobenius norm of the errors which is equal to L2 norm for matrices. It is like unrolling the matrix to a one dimensional vector and take the L2 norm of that vector. D* is the optimal D. D* = argminD sqrt sum over x(i)j – r(x(i)j)2. With the constraint DTD=It and columns of D orthogonal. We start to find only the first PC, we will have l =1. So matrix D will have shape (n x 1): it’s a simple column vector. We can thus remove the sum over j and the square root since we take the squared L2 norm instead of frobenius norm as it’s a vector not matrix. r(x) = DDTx and we are looking for the first PC r(x) = ddTx. We can plug r(x) into the equation: d*=argmind∑I ||x(i)−ddTx(i)||22. Because of the constrain the columns of D have unit norms we have ||d||2 = 1, d is one column of D. d*=argmind||X−XddT||2F. with the constraint ddT=1. We then use the trace operator to simplify the equation to minimize. Recall ||A||F= sqrt Tr(AAT) So here A = X – XddT. (DON’T UNDERSTAND ABOVE EIGENDECOMPOSITION).\r\nWe will see that we can find the maximum of the function by calculating the eigenvectors of XTX. \r\nWhat is the covariance matrix? The optimization problem of maximizing the variance of components and minimizing the error between reconstructed and actual data are equivalent. If we centre data around 0, XTX is the covariance matrix. The Covariance matrix is n by n matrix (n being dims).  Its diagonal is the variance of the dimensions and the other cells are the covariance between the two corresponding dimensions (amount of redundancy). The largest covariance between two dims the more redundancy exists between these dims. To max the variance and min covariance (to deccorelate) means the ideal covariance is a diagonal matrix. Thus, the diagonalization of the covariance matrix will give us the optimal solution. Highly correlated data means the dims are redundant, possible to predict one from the other w/out losing much info. The first processing we do is center the data around 0. PCA is a regression model without intercept so the first PC crosses the origin.  We can now look for PCs and they are the values taken by d that max d*, to find d we calculate the eigenvectors of XTX. These are the vectors maximizing our function, the vector associated with the larger eigenvalue tells us the direction associated with the larger variance in our data – the blue vector.  The blue vector direction corresponds to the oblique shape of the data. The idea is that if you project the data points on the line corresponding to the blue vector direction you will end up with the largest variance. This vector has the direction that max’s variance of the projected data. The second eigenvector is orthogonal to the orange (blue), representing the biggest eigenvalue. Now we have found d we will use the encoding function to rotate the data, the goal being to end up with a new coordinate system where data is uncorrelated and thus the basis axes gather all the variance. Thus only a few axes are kept. The rotation then transforms the dataset so we have more variance on one of the basis axis. \r\nWhat is a Hessian? We need to first understand what a gradient is.\r\nWhat is a gradient? The rate of change of some function in various directions. The derivative of some function (usually the loss) is basically how much the loss changes if you change the input by a very small amount. In 1-D, the gradient is simply the slope of the function at that point. The diagram to the right is for one variable (x-axis), but many functions have multiple inputs. Geometrically, each input can be seen as having a different direction. For such functions, there is a different rate of change for each direction. \r\nThe gradient is simply a collection (vector) of the derivatives of the function for each direction. Each element of the gradient is simply the slope of the function in each direction. \r\nNext we need to understand second-order derivatives. What is a second-order derivative? The second-order derivative is simply the derivative of the derivative. It is the rate of change of the slope. See the two lines to the left. The red arrows represent the derivative/slope at each point. In the left plot the slope changes a lot while the right one is more stable. The rate of change of the slope corresponds to how curved each loss function is. The sharper the curve the more rapidly the slope changes. In high-dimensional space, there are different rates of change = slope for each direction. What’s more, there is a different rate of change of the slope in each direction as well, meaning that the number of second-order derivatives in d2, where d is the dimensionality of the input. This is why second-order derivatives are expressed by the Hessian matrix is defined by:  \r\nEach row represents the change of the gradient in a certain direction. Each column can be thought of as the gradient of one element of the gradient. The hessian is represented by H. It is important to remember that regardless of what dimensional space we are in: the second-order derivative represents the curvature of a function. The curvature will differ according to direction. \r\nNN’s are too complex to analyse so we will build our intuition using the following, simple problem: minimizing a quadratic form. Quadratic forms: a 2-D quadratic form is simply any function that can be expressed like:  When we write this in matrix form, we get the following equation . Where A is symmetric(ith row and ith column are the same). For this function, the gradient is Ax-b and the Hessian is A. The optimal solution is where the gradient Ax-b becomes 0. What does a quadratic for look like? It depends on the Hessian: \r\nThe function curves in different ways according to the Hessian. But how is the curvature determined by the Hessian? The relationship between the shape of the quadratic form and the Hessian’s eigenvalues. The eigenvectors of matrix M do not change direction when multiplied with M and the eigenvalues represent the change in length of the eigenvector when multiplied with M.  where vi is an eigenvector and  is an eigenvalue. Each eigenvector has a single eigenvalue as a pair. Eigenvectors and values have important properties in a Hessian: 1. Each eigenvector represents a direction where the curvature is independent of other directions. 2. The curvature in the direction of the eigenvector is determined by the eigenvalue. If the eigenvalue is larger, there is a larger curvature, and if it is positive, the curvature will be positive and vice-versa. Lets have a look at the first element in second row of the H:  , . This is the rate of change in direction x2 of the gradient in direction x1. How confusing right? Wouldn’t it be great if the gradient only changed in the direction it pointed in? This is where the eigenvectors come to our rescue. For the eigenvectors, the gradient only changes in the direction it points in. In other words, all the non-diagonal elements of the Hessian are 0 for the eigenvectors. This is clearer when we plot the contours of the loss function:    A contour is a set of points where the loss function is equal. Here the eigenvectors are the axes of the ellipses. The length of the axis is determined by the eigenvalue in its direction. Larger eigenvalue=larger curvature=larger rate of change, so the axes in the direction of the large eigenvalues are actually shorter. Looking back up at the loss functions above, we can see (a) the loss is curving upwards in two directions so has two positive eigenvalues, whereas the eigenvalues are both negative for (b). (d) has one negative and one positive eigenvalue. (c) is special since one of its eigenvalues is 0 hence why the loss is flat across one axis. \r\nSo what’s the relationship between the Hessian and Optimisation? The key intuition is the relation between the eigenvalue, the eigenvector and the speed and direction of convergence when using gradient descent. The larger the eigenvalue, the faster the convergence from the direction of its corresponding eigenvector. Can be derived as follows: 1. A gradient descent step with step size  can be expressed as:  2. If  (the ith eigenvector),  where  is the initial vector. 3. Any vector  can be expressed as a weighted sum of eigenvectors. Thus, for each component, the convergence rate is . This is because the curve is steeper in directions with larger eigenvalues. The stronger the curve the quicker the ball can reach the minima where the gradient is zero. In the left diagram the gradient changes quickly so crosses 0 quickly as well. In the right, the gradient slowly changes so does not reach 0 within the diagram. This means you have to pick the right step size in case you diverge. What concerns us is whether the sign of the gradient is the same, if we take a step that is too large, we could end up actually increasing the loss. The ideal step size for each eigenvector component is when x converges to 0 in one step. Therefore, it is . If all the eigenvalues are the same, choosing the step size is trivial. The problem is when eigenvalues are very different. We have to be careful not to allow any of the components to diverge: this means the step-size is effectively bounded by the largest curvature = largest eigenvalue. The speed of convergence is determined by the smallest eigenvalue. This ratio between the largest and smallest eigenvalue in the Hessian is very important and is called the condition number. The problem of the condition number being very large is called ill-conditioning. NN’s are far more complex but we can look at small, local areas of the loss function, we can approximate the loss function as a quadratic form. This is done by using Taylor expansion: \r\nAll its saying is that we are approximating a function using a quadratic form. Large portion of problems in DL can be attributed to ill-conditioning, some parameters have huge curvature while some have smaller curvature. To mitigate we can normalize the inputs (mean of the inputs makes the largest eigenvalue even larger, thus normalizing reduces the largest curvature) and use batch norm. One other way is to instead of changing the Hessian itself, momentum is a way of reducing step size in directions with larger curvature and increasing it in directions of smaller curvature. When the parameters go back and forth, the curvature is high, so momentum automatically reduces the learning rate for that direction. \r\n\r\n    </div>\r\n)\r\n\r\nexport default LinearAlgebra","import React from 'react';\n// import logo from './logo.svg';\nimport './App.css';\nimport { Route, Link, BrowserRouter as Router } from 'react-router-dom'\nimport CNN from \"./components/cnn/cnn.zine\"\nimport Limitations from \"./components/limitations/Limitations\"\nimport NavBar from \"./components/layout/NavBar\"\nimport Topics from \"./components/topics/Topics\"\nimport Home from \"./components/home/Home\"\nimport SGD from \"./components/sgd/SGD\"\nimport BatchNorm from \"./components/batch_norm/BatchNorm\"\nimport LinearAlgebra from \"./components/linear_algebra/LinearAlgebra\"\n\nconst styles = {\n  background: '#000',\n  width: '2px',\n  cursor: 'col-resize',\n  margin: '0 5px',\n  height: '100%',\n  backgroundColor: \"#ccfff5\",\n  color: \"black\"\n};\n\nfunction App() {\n  return (\n    <div className=\"container\">\n      <Router>\n      <NavBar/>\n      <div className=\"App\">\n          \n        <div>\n          <Route exact path=\"/zine-blog\" component={Home}/>\n          <Route path=\"/zine-blog/topics\" component={Topics}/>\n          <Route path=\"/cnns\" component={CNN} />\n          <Route path=\"/limitations\" component={Limitations} />\n          <Route path=\"/sgd\" component={SGD} />\n          <Route path=\"/batch_norm\" component={BatchNorm} />\n          <Route path=\"/linear_algebra\" component={LinearAlgebra} />\n\n        </div>\n      </div>\n    </Router>\n    </div>\n  );\n}\n\nexport default App;\n","// This optional code is used to register a service worker.\n// register() is not called by default.\n\n// This lets the app load faster on subsequent visits in production, and gives\n// it offline capabilities. However, it also means that developers (and users)\n// will only see deployed updates on subsequent visits to a page, after all the\n// existing tabs open on the page have been closed, since previously cached\n// resources are updated in the background.\n\n// To learn more about the benefits of this model and instructions on how to\n// opt-in, read https://bit.ly/CRA-PWA\n\nconst isLocalhost = Boolean(\n  window.location.hostname === 'localhost' ||\n    // [::1] is the IPv6 localhost address.\n    window.location.hostname === '[::1]' ||\n    // 127.0.0.1/8 is considered localhost for IPv4.\n    window.location.hostname.match(\n      /^127(?:\\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/\n    )\n);\n\nexport function register(config) {\n  if (process.env.NODE_ENV === 'production' && 'serviceWorker' in navigator) {\n    // The URL constructor is available in all browsers that support SW.\n    const publicUrl = new URL(process.env.PUBLIC_URL, window.location.href);\n    if (publicUrl.origin !== window.location.origin) {\n      // Our service worker won't work if PUBLIC_URL is on a different origin\n      // from what our page is served on. This might happen if a CDN is used to\n      // serve assets; see https://github.com/facebook/create-react-app/issues/2374\n      return;\n    }\n\n    window.addEventListener('load', () => {\n      const swUrl = `${process.env.PUBLIC_URL}/service-worker.js`;\n\n      if (isLocalhost) {\n        // This is running on localhost. Let's check if a service worker still exists or not.\n        checkValidServiceWorker(swUrl, config);\n\n        // Add some additional logging to localhost, pointing developers to the\n        // service worker/PWA documentation.\n        navigator.serviceWorker.ready.then(() => {\n          console.log(\n            'This web app is being served cache-first by a service ' +\n              'worker. To learn more, visit https://bit.ly/CRA-PWA'\n          );\n        });\n      } else {\n        // Is not localhost. Just register service worker\n        registerValidSW(swUrl, config);\n      }\n    });\n  }\n}\n\nfunction registerValidSW(swUrl, config) {\n  navigator.serviceWorker\n    .register(swUrl)\n    .then(registration => {\n      registration.onupdatefound = () => {\n        const installingWorker = registration.installing;\n        if (installingWorker == null) {\n          return;\n        }\n        installingWorker.onstatechange = () => {\n          if (installingWorker.state === 'installed') {\n            if (navigator.serviceWorker.controller) {\n              // At this point, the updated precached content has been fetched,\n              // but the previous service worker will still serve the older\n              // content until all client tabs are closed.\n              console.log(\n                'New content is available and will be used when all ' +\n                  'tabs for this page are closed. See https://bit.ly/CRA-PWA.'\n              );\n\n              // Execute callback\n              if (config && config.onUpdate) {\n                config.onUpdate(registration);\n              }\n            } else {\n              // At this point, everything has been precached.\n              // It's the perfect time to display a\n              // \"Content is cached for offline use.\" message.\n              console.log('Content is cached for offline use.');\n\n              // Execute callback\n              if (config && config.onSuccess) {\n                config.onSuccess(registration);\n              }\n            }\n          }\n        };\n      };\n    })\n    .catch(error => {\n      console.error('Error during service worker registration:', error);\n    });\n}\n\nfunction checkValidServiceWorker(swUrl, config) {\n  // Check if the service worker can be found. If it can't reload the page.\n  fetch(swUrl)\n    .then(response => {\n      // Ensure service worker exists, and that we really are getting a JS file.\n      const contentType = response.headers.get('content-type');\n      if (\n        response.status === 404 ||\n        (contentType != null && contentType.indexOf('javascript') === -1)\n      ) {\n        // No service worker found. Probably a different app. Reload the page.\n        navigator.serviceWorker.ready.then(registration => {\n          registration.unregister().then(() => {\n            window.location.reload();\n          });\n        });\n      } else {\n        // Service worker found. Proceed as normal.\n        registerValidSW(swUrl, config);\n      }\n    })\n    .catch(() => {\n      console.log(\n        'No internet connection found. App is running in offline mode.'\n      );\n    });\n}\n\nexport function unregister() {\n  if ('serviceWorker' in navigator) {\n    navigator.serviceWorker.ready.then(registration => {\n      registration.unregister();\n    });\n  }\n}\n","import React from 'react';\nimport ReactDOM from 'react-dom';\nimport './index.css';\nimport App from './App';\nimport * as serviceWorker from './serviceWorker';\n\nReactDOM.render(<App />, document.getElementById('root'));\n\n// If you want your app to work offline and load faster, you can change\n// unregister() to register() below. Note this comes with some pitfalls.\n// Learn more about service workers: https://bit.ly/CRA-PWA\nserviceWorker.unregister();\n"],"sourceRoot":""}