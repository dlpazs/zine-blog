(this["webpackJsonpzine-blog"]=this["webpackJsonpzine-blog"]||[]).push([[0],{24:function(e,t,a){e.exports=a(38)},29:function(e,t,a){},30:function(e,t,a){e.exports=a.p+"static/media/logo.25bf045c.svg"},31:function(e,t,a){},38:function(e,t,a){"use strict";a.r(t);var n=a(0),o=a.n(n),i=a(15),l=a.n(i),r=(a(29),a(30),a(31),a(9)),s=a(6),c=a(16),h=a(17),p=a(21),m=a(18),u=a(23),d=function(e){function t(){return Object(c.a)(this,t),Object(p.a)(this,Object(m.a)(t).apply(this,arguments))}return Object(u.a)(t,e),Object(h.a)(t,[{key:"render",value:function(){return o.a.createElement("div",{className:"container"},o.a.createElement("h1",null,"Everybody loves ... convolutions"),o.a.createElement("blockquote",null,"In 3D, the following mental image may prove useful. Imagine two sheets of colored paper: one red and one blue. Put one on top of the other. Now crumple them together into a small ball. That crumpled paper ball is your input data, and each sheet of paper is a class of data in a classification problem. What a neural network (or any other machine-learning model) is meant to do is figure out a transformation of the paper ball that would uncrumple it, so as to make the two classes cleanly separable again. With deep learning, this would be implemented as a series of simple transformations of the 3D space, such as those you could apply on the paper ball with your fingers, one movement at a time.\xa0 Uncrumpling paper balls is what machine learning is about: finding neat representations for complex, highly folded data manifolds. At this point, you should have a pretty good intuition as to why deep learning excels at this: it takes the approach of incrementally decomposing a complicated geometric transformation into a long chain of elementary ones, which is pretty much the strategy a human would follow to uncrumple a paper ball. Each layer in a deep network applies a transformation that disentangles the data a little\u200a-\u200aand a deep stack of layers makes tractable an extremely complicated disentanglement process. Deep Learning with Python page 44\u20135."),o.a.createElement("blockquote",null,"Conv layers look at spatially local patterns by applying the same geometric transformation to different spatial locations (patches) in an input tensor. This results in a representation that are translation invariant (does not matter where in the image the object occurs). Convnets consist of stacks of convolution and max-pooling layers. The pooling layers let you spatially downsample the data, which is required to keep feature maps to a reasonable size as the number of features grows, and to allow subsequent convolution layers to see a greater spatial extent of the inputs. Convnets are often ended with a `Flatten` operation or global pooling layer, turning spatial feature maps into vectors, followed by `Dense` layers to achieve classification or regression."),o.a.createElement("img",{src:"https://i.stack.imgur.com/iY5n5.png",style:"width:100px;height:100px;"}),o.a.createElement("h2",null,"What is a convolution?"),"It is a sliding window applied to a matrix, whether that be an image or feature map. At each element we multiply the filter/kernel with the corresponding input pixel and then add them up. The same filter/kernel is slide across the input which is called weight tying. Filters of the convolutions are learned, so the model learns which filter is best to apply in each layer. An early layer of a convolution is likely, at best, to only detect very low level features such as edges. This is because the filters are limited at earlier layers, whereas as we go deeper, the amount of output channels (which we set) increases and later layers have several channels with which to predict collections of edges and orientations that make up a face or eye ball.",o.a.createElement("h2",null,"Properties"),"Location Invariance and Compositionality. Say you want to spot a cat in an image. Because of this sliding window approach you don't care where the cat occurs. The second aspect is (local) compositionality. Each filter composes a local patch of lower-level features into higher-level representation. You build edges from pixels, shapes from edges, eyes from circles and so on. You can derive more complex shapes and objects from lower level representations such as edges, pixels, gradients etc and from several channels of these features. Convolutions are translationally invariant because the filters slide over the image horizontally and vertically. But they are not rotationally invariant because the filters don't rotate. Thus, the net seems to need several similar filters in different orientations to detect objects and patterns that are differently oriented.",o.a.createElement("img",{src:"https://cdn-images-1.medium.com/max/1200/1*B41mvbzpZ7ythn5AlJWh-A.gif",style:"width:100px;height:100px;"}))}}]),t}(n.Component),f=a(22),g={background:"#000",width:"2px",cursor:"col-resize",margin:"0 5px",height:"100%"};var y=function e(){return o.a.createElement(r.a,null,o.a.createElement("div",{className:"App"},o.a.createElement(f.a,{split:"vertical",minSize:100,defaultSize:100,resizerStyle:g},o.a.createElement("menu",null,o.a.createElement("div",null,o.a.createElement(r.b,{to:"/zine-blog"},"Home")),o.a.createElement("div",null,o.a.createElement(r.b,{to:"/cnns"},"Convolutional Neural Nets"))),o.a.createElement("div",null,o.a.createElement(s.a,{exact:!0,path:"/zine-blog",component:e}),o.a.createElement(s.a,{path:"/cnns",component:d})))))};Boolean("localhost"===window.location.hostname||"[::1]"===window.location.hostname||window.location.hostname.match(/^127(?:\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/));l.a.render(o.a.createElement(y,null),document.getElementById("root")),"serviceWorker"in navigator&&navigator.serviceWorker.ready.then((function(e){e.unregister()}))}},[[24,1,2]]]);
//# sourceMappingURL=main.d1435625.chunk.js.map